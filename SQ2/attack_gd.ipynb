{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90cb8346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hskay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf004e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_PATH = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cc4b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.65s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    dtype=torch.float16,\n",
    "    device_map='auto' if torch.cuda.is_available() else None\n",
    ")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7266465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b40becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to capture activations\n",
    "def get_activation(name, activations_dict):\n",
    "    def hook(module, input, output):\n",
    "        # Handle different output types\n",
    "        if isinstance(output, tuple):\n",
    "            activations_dict[name] = output[0].detach().clone()\n",
    "        else:\n",
    "            activations_dict[name] = output.detach().clone()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d376c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample inputs for analysis\n",
    "def generate_sample_inputs(tokenizer, seq_length=32):    \n",
    "    sample_texts = [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Artificial intelligence is transforming the world of technology.\",\n",
    "        \"In a hole in the ground there lived a hobbit.\",\n",
    "        \"To be or not to be, that is the question Shakespeare posed.\",\n",
    "        \"Machine learning models require large datasets for training.\",\n",
    "        \"The mitochondria is the powerhouse of the cell in biology.\",\n",
    "        \"Climate change is causing unprecedented shifts in global weather patterns.\",\n",
    "        \"Mozart composed his first symphony at the age of eight years old.\",\n",
    "        \"The stock market experienced significant volatility during the pandemic crisis.\",\n",
    "        \"Quantum physics reveals the strange behavior of particles at subatomic levels.\",\n",
    "        \"Professional chefs recommend using fresh herbs to enhance flavor profiles.\",\n",
    "        \"Ancient Egyptian pyramids were built using sophisticated engineering techniques.\",\n",
    "        \"Regular exercise and proper nutrition are essential for maintaining good health.\",\n",
    "        \"The International Space Station orbits Earth approximately every ninety minutes.\",\n",
    "        \"Cryptocurrency markets operate twenty-four hours a day across global exchanges.\",\n",
    "        \"Vincent van Gogh painted Starry Night while staying at an asylum.\",\n",
    "        \"Professional athletes must maintain strict training regimens throughout their careers.\",\n",
    "        \"The Amazon rainforest produces twenty percent of the world's oxygen supply.\",\n",
    "        \"Modern architecture emphasizes clean lines and functional design principles.\",\n",
    "        \"Forensic scientists use DNA analysis to solve complex criminal investigations.\",\n",
    "        \"Traditional Japanese tea ceremonies follow centuries-old ritualistic practices.\",\n",
    "        \"Marine biologists study coral reef ecosystems threatened by ocean acidification.\",\n",
    "        \"The Renaissance period marked a cultural rebirth in European art and science.\",\n",
    "        \"Cybersecurity experts work tirelessly to protect digital infrastructure from threats.\",\n",
    "        \"Sustainable agriculture practices help preserve soil quality for future generations.\"\n",
    "    ]\n",
    "    \n",
    "    inputs = []\n",
    "    for i in range(len(sample_texts)):\n",
    "        # Cycle through sample texts and add variations\n",
    "        base_text = sample_texts[i % len(sample_texts)]\n",
    "\n",
    "        # Tokenize\n",
    "        tokenized = tokenizer(\n",
    "            base_text, \n",
    "            return_tensors=\"pt\", \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=seq_length\n",
    "        )\n",
    "        inputs.append(tokenized.input_ids.to(model.device))\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885df2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate activation differences for Llama-2 layers\n",
    "def generate_activation_differences_llama(model, X_data, n_samples=50, n_reconstructions=3):\n",
    "    results = []\n",
    "    \n",
    "    # Select specific layers to analyze (first few transformer layers)\n",
    "    # layer_names = [\n",
    "    #     'model.layers.0',  # First transformer layer\n",
    "    #     'model.layers.1',  # Second transformer layer  \n",
    "    #     'model.layers.2',  # Third transformer layer\n",
    "    # ]\n",
    "    layer_names = [f'model.layers.{i}' for i in range(len(model.model.layers))]\n",
    "    for sample_idx in tqdm(range(min(n_samples, len(X_data))), desc=\"Processing samples\"):\n",
    "        original_input = X_data[sample_idx]\n",
    "        \n",
    "        # Get original activations\n",
    "        original_activations = {}\n",
    "        hooks = []\n",
    "        \n",
    "       # Register hooks for all layers\n",
    "        for layer_name in layer_names:\n",
    "            layer_module = model\n",
    "            for attr in layer_name.split('.'):\n",
    "                layer_module = getattr(layer_module, attr)\n",
    "            hooks.append(layer_module.register_forward_hook(\n",
    "                get_activation(layer_name, original_activations)\n",
    "            ))\n",
    "        \n",
    "        # Get original output and activations\n",
    "        with torch.no_grad():\n",
    "            original_output = model(original_input).logits\n",
    "        \n",
    "        # Remove hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        \n",
    "        # Multiple reconstruction attempts\n",
    "        for recon_idx in range(n_reconstructions):\n",
    "            # Initialize random input embeddings for reconstruction\n",
    "            seq_length = original_input.shape[1]\n",
    "            embedding_dim = model.config.hidden_size\n",
    "            \n",
    "            # Use embeddings instead of token IDs for gradient-based optimization\n",
    "            reconstructed_embeddings = torch.randn(\n",
    "                1, seq_length, embedding_dim,\n",
    "                device=model.device,\n",
    "                dtype=torch.float32,\n",
    "                requires_grad=True\n",
    "            )\n",
    "            \n",
    "            optimizer = optim.Adam([reconstructed_embeddings], lr=0.001)\n",
    "            \n",
    "            # Reconstruction optimization\n",
    "            for iteration in range(10000): \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass with embeddings\n",
    "                embeddings_model_dtype = reconstructed_embeddings.to(model.dtype)\n",
    "                output = model(inputs_embeds=embeddings_model_dtype).logits\n",
    "                \n",
    "                # Loss: match original output\n",
    "                loss = nn.functional.mse_loss(output.float(), original_output.float())\n",
    "                \n",
    "                # Add regularization\n",
    "                reg_loss = 0.001 * torch.mean(reconstructed_embeddings ** 2)\n",
    "                total_loss = loss + reg_loss\n",
    "                \n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if total_loss.item() < 1e-3:\n",
    "                    break\n",
    "                if iteration % 1000 == 0:\n",
    "                    print(f\"Sample {sample_idx}, Recon {recon_idx}, Iter {iteration}, Loss: {total_loss.item():.6f}\",end=\"\")\n",
    "            \n",
    "            # Get reconstructed activations\n",
    "            reconstructed_activations = {}\n",
    "            hooks = []\n",
    "            \n",
    "            for layer_name in layer_names:\n",
    "                layer_module = model\n",
    "                for attr in layer_name.split('.'):\n",
    "                    layer_module = getattr(layer_module, attr)\n",
    "                hooks.append(layer_module.register_forward_hook(\n",
    "                    get_activation(layer_name, reconstructed_activations)\n",
    "                ))\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embeddings_model_dtype = reconstructed_embeddings.to(model.dtype)\n",
    "                _ = model(inputs_embeds=embeddings_model_dtype)\n",
    "            \n",
    "            # Remove hooks\n",
    "            for hook in hooks:\n",
    "                hook.remove()\n",
    "            \n",
    "            # Calculate differences for each layer\n",
    "            row = {'sample_idx': sample_idx, 'reconstruction_idx': recon_idx}\n",
    "            \n",
    "            # Store individual layer metrics\n",
    "            all_layer_max_diffs = []\n",
    "            \n",
    "            for layer_name in layer_names:\n",
    "                if layer_name in original_activations and layer_name in reconstructed_activations:\n",
    "                    orig_act = original_activations[layer_name].flatten().float()\n",
    "                    recon_act = reconstructed_activations[layer_name].flatten().float()\n",
    "                    \n",
    "                    abs_diff = torch.abs(orig_act - recon_act)\n",
    "                    \n",
    "                    layer_short = layer_name.split('.')[-1]  # Get layer number\n",
    "                    row[f'layer_{layer_short}_min_abs_diff'] = abs_diff.min().item()\n",
    "                    row[f'layer_{layer_short}_mean_abs_diff'] = abs_diff.mean().item()\n",
    "                    row[f'layer_{layer_short}_max_abs_diff'] = abs_diff.max().item()\n",
    "                    \n",
    "                    all_layer_max_diffs.append(abs_diff.max().item())\n",
    "            \n",
    "            # Store the maximum difference across ALL layers\n",
    "            if all_layer_max_diffs:\n",
    "                row['all_layers_max_diff'] = max(all_layer_max_diffs)\n",
    "                row['all_layers_min_of_max'] = min(all_layer_max_diffs)\n",
    "            \n",
    "            results.append(row)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5481000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sample inputs...\n",
      "Generated 20 samples\n"
     ]
    }
   ],
   "source": [
    "# Generate sample data\n",
    "print(\"Generating sample inputs...\")\n",
    "X_data = generate_sample_inputs(tokenizer, seq_length=15) \n",
    "print(f\"Generated {len(X_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe1a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating activation differences for Llama-2 layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0, Recon 0, Iter 0, Loss: 18.072140\n",
      "Sample 0, Recon 0, Iter 100, Loss: 6.001960\n",
      "Sample 0, Recon 0, Iter 200, Loss: 4.446840\n",
      "Sample 0, Recon 0, Iter 300, Loss: 3.604485\n",
      "Sample 0, Recon 0, Iter 400, Loss: 3.302956\n",
      "Sample 0, Recon 0, Iter 500, Loss: 3.122141\n",
      "Sample 0, Recon 0, Iter 600, Loss: 2.971247\n",
      "Sample 0, Recon 0, Iter 700, Loss: 2.826409\n",
      "Sample 0, Recon 0, Iter 800, Loss: 2.680085\n",
      "Sample 0, Recon 0, Iter 900, Loss: 2.532717\n",
      "Sample 0, Recon 0, Iter 1000, Loss: 2.393804\n",
      "Sample 0, Recon 0, Iter 1100, Loss: 2.269571\n",
      "Sample 0, Recon 0, Iter 1200, Loss: 2.159087\n",
      "Sample 0, Recon 0, Iter 1300, Loss: 2.060229\n",
      "Sample 0, Recon 0, Iter 1400, Loss: 1.970483\n",
      "Sample 0, Recon 0, Iter 1500, Loss: 1.888022\n",
      "Sample 0, Recon 0, Iter 1600, Loss: 1.811693\n",
      "Sample 0, Recon 0, Iter 1700, Loss: 1.740411\n",
      "Sample 0, Recon 0, Iter 1800, Loss: 1.674420\n",
      "Sample 0, Recon 0, Iter 1900, Loss: 1.613055\n",
      "Sample 0, Recon 0, Iter 2000, Loss: 1.555785\n",
      "Sample 0, Recon 0, Iter 2100, Loss: 1.502473\n",
      "Sample 0, Recon 0, Iter 2200, Loss: 1.452552\n",
      "Sample 0, Recon 0, Iter 2300, Loss: 1.405560\n",
      "Sample 0, Recon 0, Iter 2400, Loss: 1.361631\n",
      "Sample 0, Recon 0, Iter 2500, Loss: 1.320555\n",
      "Sample 0, Recon 0, Iter 2600, Loss: 1.281990\n",
      "Sample 0, Recon 0, Iter 2700, Loss: 1.245387\n",
      "Sample 0, Recon 0, Iter 2800, Loss: 1.211215\n",
      "Sample 0, Recon 0, Iter 2900, Loss: 1.179150\n",
      "Sample 0, Recon 0, Iter 3000, Loss: 1.149558\n",
      "Sample 0, Recon 0, Iter 3100, Loss: 1.121364\n",
      "Sample 0, Recon 0, Iter 3200, Loss: 1.094883\n",
      "Sample 0, Recon 0, Iter 3300, Loss: 1.069513\n",
      "Sample 0, Recon 0, Iter 3400, Loss: 1.045684\n",
      "Sample 0, Recon 0, Iter 3500, Loss: 1.023672\n",
      "Sample 0, Recon 0, Iter 3600, Loss: 1.003125\n",
      "Sample 0, Recon 0, Iter 3700, Loss: 0.980619\n",
      "Sample 0, Recon 0, Iter 3800, Loss: 0.960998\n",
      "Sample 0, Recon 0, Iter 3900, Loss: 0.942172\n",
      "Sample 0, Recon 0, Iter 4000, Loss: 0.924189\n",
      "Sample 0, Recon 0, Iter 4100, Loss: 0.906443\n",
      "Sample 0, Recon 0, Iter 4200, Loss: 0.889627\n",
      "Sample 0, Recon 0, Iter 4300, Loss: 0.874031\n",
      "Sample 0, Recon 0, Iter 4400, Loss: 0.858203\n",
      "Sample 0, Recon 0, Iter 4500, Loss: 0.843629\n",
      "Sample 0, Recon 0, Iter 4600, Loss: 0.828792\n",
      "Sample 0, Recon 0, Iter 4700, Loss: 0.814949\n",
      "Sample 0, Recon 0, Iter 4800, Loss: 0.802216\n",
      "Sample 0, Recon 0, Iter 4900, Loss: 0.789290\n",
      "Sample 0, Recon 0, Iter 5000, Loss: 0.776543\n",
      "Sample 0, Recon 0, Iter 5100, Loss: 0.765756\n",
      "Sample 0, Recon 0, Iter 5200, Loss: 0.752855\n",
      "Sample 0, Recon 0, Iter 5300, Loss: 0.741671\n",
      "Sample 0, Recon 0, Iter 5400, Loss: 0.731257\n",
      "Sample 0, Recon 0, Iter 5500, Loss: 0.720329\n",
      "Sample 0, Recon 0, Iter 5600, Loss: 0.709871\n",
      "Sample 0, Recon 0, Iter 5700, Loss: 0.700669\n",
      "Sample 0, Recon 0, Iter 5800, Loss: 0.690667\n",
      "Sample 0, Recon 0, Iter 5900, Loss: 0.680861\n",
      "Sample 0, Recon 0, Iter 6000, Loss: 0.671649\n",
      "Sample 0, Recon 0, Iter 6100, Loss: 0.663191\n",
      "Sample 0, Recon 0, Iter 6200, Loss: 0.655273\n",
      "Sample 0, Recon 0, Iter 6300, Loss: 0.645414\n",
      "Sample 0, Recon 0, Iter 6400, Loss: 0.638786\n",
      "Sample 0, Recon 0, Iter 6500, Loss: 0.629618\n",
      "Sample 0, Recon 0, Iter 6600, Loss: 0.621928\n",
      "Sample 0, Recon 0, Iter 6700, Loss: 0.614944\n",
      "Sample 0, Recon 0, Iter 6800, Loss: 0.607161\n",
      "Sample 0, Recon 0, Iter 6900, Loss: 0.600691\n",
      "Sample 0, Recon 0, Iter 7000, Loss: 0.593495\n",
      "Sample 0, Recon 0, Iter 7100, Loss: 0.588814\n",
      "Sample 0, Recon 0, Iter 7200, Loss: 0.580406\n",
      "Sample 0, Recon 0, Iter 7300, Loss: 0.574201\n",
      "Sample 0, Recon 0, Iter 7400, Loss: 0.568159\n",
      "Sample 0, Recon 0, Iter 7500, Loss: 0.562228\n",
      "Sample 0, Recon 0, Iter 7600, Loss: 0.556511\n",
      "Sample 0, Recon 0, Iter 7700, Loss: 0.551154\n",
      "Sample 0, Recon 0, Iter 7800, Loss: 0.545455\n",
      "Sample 0, Recon 0, Iter 7900, Loss: 0.540498\n",
      "Sample 0, Recon 0, Iter 8000, Loss: 0.535576\n",
      "Sample 0, Recon 0, Iter 8100, Loss: 0.530365\n",
      "Sample 0, Recon 0, Iter 8200, Loss: 0.526113\n",
      "Sample 0, Recon 0, Iter 8300, Loss: 0.521366\n",
      "Sample 0, Recon 0, Iter 8400, Loss: 0.519600\n",
      "Sample 0, Recon 0, Iter 8500, Loss: 0.513552\n",
      "Sample 0, Recon 0, Iter 8600, Loss: 0.508981\n",
      "Sample 0, Recon 0, Iter 8700, Loss: 0.505497\n",
      "Sample 0, Recon 0, Iter 8800, Loss: 0.505090\n",
      "Sample 0, Recon 0, Iter 8900, Loss: 0.497147\n",
      "Sample 0, Recon 0, Iter 9000, Loss: 0.497085\n",
      "Sample 0, Recon 0, Iter 9100, Loss: 0.490908\n",
      "Sample 0, Recon 0, Iter 9200, Loss: 0.486171\n",
      "Sample 0, Recon 0, Iter 9300, Loss: 0.483860\n",
      "Sample 0, Recon 0, Iter 9400, Loss: 0.479639\n",
      "Sample 0, Recon 0, Iter 9500, Loss: 0.476017\n",
      "Sample 0, Recon 0, Iter 9600, Loss: 0.474143\n",
      "Sample 0, Recon 0, Iter 9700, Loss: 0.469949\n",
      "Sample 0, Recon 0, Iter 9800, Loss: 0.466785\n",
      "Sample 0, Recon 0, Iter 9900, Loss: 0.463708\n",
      "Sample 0, Recon 1, Iter 0, Loss: 18.558796\n",
      "Sample 0, Recon 1, Iter 100, Loss: 6.400177\n",
      "Sample 0, Recon 1, Iter 200, Loss: 5.165540\n",
      "Sample 0, Recon 1, Iter 300, Loss: 4.122652\n",
      "Sample 0, Recon 1, Iter 400, Loss: 3.325861\n",
      "Sample 0, Recon 1, Iter 500, Loss: 3.052432\n",
      "Sample 0, Recon 1, Iter 600, Loss: 2.883837\n",
      "Sample 0, Recon 1, Iter 700, Loss: 2.734237\n",
      "Sample 0, Recon 1, Iter 800, Loss: 2.587856\n",
      "Sample 0, Recon 1, Iter 900, Loss: 2.446950\n",
      "Sample 0, Recon 1, Iter 1000, Loss: 2.317326\n",
      "Sample 0, Recon 1, Iter 1100, Loss: 2.201121\n",
      "Sample 0, Recon 1, Iter 1200, Loss: 2.096479\n",
      "Sample 0, Recon 1, Iter 1300, Loss: 2.001555\n",
      "Sample 0, Recon 1, Iter 1400, Loss: 1.915190\n",
      "Sample 0, Recon 1, Iter 1500, Loss: 1.835792\n",
      "Sample 0, Recon 1, Iter 1600, Loss: 1.762593\n",
      "Sample 0, Recon 1, Iter 1700, Loss: 1.695232\n",
      "Sample 0, Recon 1, Iter 1800, Loss: 1.633048\n",
      "Sample 0, Recon 1, Iter 1900, Loss: 1.575351\n",
      "Sample 0, Recon 1, Iter 2000, Loss: 1.521285\n",
      "Sample 0, Recon 1, Iter 2100, Loss: 1.471955\n",
      "Sample 0, Recon 1, Iter 2200, Loss: 1.426157\n",
      "Sample 0, Recon 1, Iter 2300, Loss: 1.383412\n",
      "Sample 0, Recon 1, Iter 2400, Loss: 1.342719\n",
      "Sample 0, Recon 1, Iter 2500, Loss: 1.303919\n",
      "Sample 0, Recon 1, Iter 2600, Loss: 1.266434\n",
      "Sample 0, Recon 1, Iter 2700, Loss: 1.230435\n",
      "Sample 0, Recon 1, Iter 2800, Loss: 1.195795\n",
      "Sample 0, Recon 1, Iter 2900, Loss: 1.162257\n",
      "Sample 0, Recon 1, Iter 3000, Loss: 1.130217\n",
      "Sample 0, Recon 1, Iter 3100, Loss: 1.099735\n",
      "Sample 0, Recon 1, Iter 3200, Loss: 1.070132\n",
      "Sample 0, Recon 1, Iter 3300, Loss: 1.042095\n",
      "Sample 0, Recon 1, Iter 3400, Loss: 1.015462\n",
      "Sample 0, Recon 1, Iter 3500, Loss: 0.990244\n",
      "Sample 0, Recon 1, Iter 3600, Loss: 0.965955\n",
      "Sample 0, Recon 1, Iter 3700, Loss: 0.942474\n",
      "Sample 0, Recon 1, Iter 3800, Loss: 0.920232\n",
      "Sample 0, Recon 1, Iter 3900, Loss: 0.898923\n",
      "Sample 0, Recon 1, Iter 4000, Loss: 0.878654\n",
      "Sample 0, Recon 1, Iter 4100, Loss: 0.859901\n",
      "Sample 0, Recon 1, Iter 4200, Loss: 0.841531\n",
      "Sample 0, Recon 1, Iter 4300, Loss: 0.824752\n",
      "Sample 0, Recon 1, Iter 4400, Loss: 0.808448\n",
      "Sample 0, Recon 1, Iter 4500, Loss: 0.792913\n",
      "Sample 0, Recon 1, Iter 4600, Loss: 0.778368\n",
      "Sample 0, Recon 1, Iter 4700, Loss: 0.764903\n",
      "Sample 0, Recon 1, Iter 4800, Loss: 0.751528\n",
      "Sample 0, Recon 1, Iter 4900, Loss: 0.738851\n",
      "Sample 0, Recon 1, Iter 5000, Loss: 0.726969\n",
      "Sample 0, Recon 1, Iter 5100, Loss: 0.715751\n",
      "Sample 0, Recon 1, Iter 5200, Loss: 0.705726\n",
      "Sample 0, Recon 1, Iter 5300, Loss: 0.695492\n",
      "Sample 0, Recon 1, Iter 5400, Loss: 0.685605\n",
      "Sample 0, Recon 1, Iter 5500, Loss: 0.675521\n",
      "Sample 0, Recon 1, Iter 5600, Loss: 0.666321\n",
      "Sample 0, Recon 1, Iter 5700, Loss: 0.662350\n",
      "Sample 0, Recon 1, Iter 5800, Loss: 0.649362\n",
      "Sample 0, Recon 1, Iter 5900, Loss: 0.641584\n",
      "Sample 0, Recon 1, Iter 6000, Loss: 0.634565\n",
      "Sample 0, Recon 1, Iter 6100, Loss: 0.626633\n",
      "Sample 0, Recon 1, Iter 6200, Loss: 0.620370\n",
      "Sample 0, Recon 1, Iter 6300, Loss: 0.614734\n",
      "Sample 0, Recon 1, Iter 6400, Loss: 0.607152\n",
      "Sample 0, Recon 1, Iter 6500, Loss: 0.601243\n",
      "Sample 0, Recon 1, Iter 6600, Loss: 0.595261\n",
      "Sample 0, Recon 1, Iter 6700, Loss: 0.589366\n",
      "Sample 0, Recon 1, Iter 6800, Loss: 0.584119\n",
      "Sample 0, Recon 1, Iter 6900, Loss: 0.578824\n",
      "Sample 0, Recon 1, Iter 7000, Loss: 0.572843\n",
      "Sample 0, Recon 1, Iter 7100, Loss: 0.567686\n",
      "Sample 0, Recon 1, Iter 7200, Loss: 0.562775\n",
      "Sample 0, Recon 1, Iter 7300, Loss: 0.560349\n",
      "Sample 0, Recon 1, Iter 7400, Loss: 0.553673\n",
      "Sample 0, Recon 1, Iter 7500, Loss: 0.551478\n",
      "Sample 0, Recon 1, Iter 7600, Loss: 0.546152\n",
      "Sample 0, Recon 1, Iter 7700, Loss: 0.542997\n",
      "Sample 0, Recon 1, Iter 7800, Loss: 0.538348\n",
      "Sample 0, Recon 1, Iter 7900, Loss: 0.533537\n",
      "Sample 0, Recon 1, Iter 8000, Loss: 0.530322\n",
      "Sample 0, Recon 1, Iter 8100, Loss: 0.526899\n",
      "Sample 0, Recon 1, Iter 8200, Loss: 0.522860\n",
      "Sample 0, Recon 1, Iter 8300, Loss: 0.520506\n",
      "Sample 0, Recon 1, Iter 8400, Loss: 0.516425\n",
      "Sample 0, Recon 1, Iter 8500, Loss: 0.513092\n",
      "Sample 0, Recon 1, Iter 8600, Loss: 0.510564\n",
      "Sample 0, Recon 1, Iter 8700, Loss: 0.506965\n",
      "Sample 0, Recon 1, Iter 8800, Loss: 0.506100\n",
      "Sample 0, Recon 1, Iter 8900, Loss: 0.501207\n",
      "Sample 0, Recon 1, Iter 9000, Loss: 0.500466\n",
      "Sample 0, Recon 1, Iter 9100, Loss: 0.496162\n",
      "Sample 0, Recon 1, Iter 9200, Loss: 0.493560\n",
      "Sample 0, Recon 1, Iter 9300, Loss: 0.491651\n",
      "Sample 0, Recon 1, Iter 9400, Loss: 0.490904\n",
      "Sample 0, Recon 1, Iter 9500, Loss: 0.485326\n",
      "Sample 0, Recon 1, Iter 9600, Loss: 0.485737\n",
      "Sample 0, Recon 1, Iter 9700, Loss: 0.481182\n",
      "Sample 0, Recon 1, Iter 9800, Loss: 0.478500\n",
      "Sample 0, Recon 1, Iter 9900, Loss: 0.476420\n",
      "Sample 0, Recon 2, Iter 0, Loss: 18.700155\n",
      "Sample 0, Recon 2, Iter 100, Loss: 5.794522\n",
      "Sample 0, Recon 2, Iter 200, Loss: 4.678231\n",
      "Sample 0, Recon 2, Iter 300, Loss: 3.824054\n",
      "Sample 0, Recon 2, Iter 400, Loss: 3.353722\n",
      "Sample 0, Recon 2, Iter 500, Loss: 3.120764\n",
      "Sample 0, Recon 2, Iter 600, Loss: 2.949448\n",
      "Sample 0, Recon 2, Iter 700, Loss: 2.816503\n",
      "Sample 0, Recon 2, Iter 800, Loss: 2.703885\n",
      "Sample 0, Recon 2, Iter 900, Loss: 2.603410\n",
      "Sample 0, Recon 2, Iter 1000, Loss: 2.508883\n",
      "Sample 0, Recon 2, Iter 1100, Loss: 2.413533\n",
      "Sample 0, Recon 2, Iter 1200, Loss: 2.312732\n",
      "Sample 0, Recon 2, Iter 1300, Loss: 2.207844\n",
      "Sample 0, Recon 2, Iter 1400, Loss: 2.107706\n",
      "Sample 0, Recon 2, Iter 1500, Loss: 2.018024\n",
      "Sample 0, Recon 2, Iter 1600, Loss: 1.938922\n",
      "Sample 0, Recon 2, Iter 1700, Loss: 1.867840\n",
      "Sample 0, Recon 2, Iter 1800, Loss: 1.802337\n",
      "Sample 0, Recon 2, Iter 1900, Loss: 1.741057\n",
      "Sample 0, Recon 2, Iter 2000, Loss: 1.683170\n",
      "Sample 0, Recon 2, Iter 2100, Loss: 1.628228\n",
      "Sample 0, Recon 2, Iter 2200, Loss: 1.575491\n",
      "Sample 0, Recon 2, Iter 2300, Loss: 1.525299\n",
      "Sample 0, Recon 2, Iter 2400, Loss: 1.477086\n",
      "Sample 0, Recon 2, Iter 2500, Loss: 1.430870\n",
      "Sample 0, Recon 2, Iter 2600, Loss: 1.386364\n",
      "Sample 0, Recon 2, Iter 2700, Loss: 1.343529\n",
      "Sample 0, Recon 2, Iter 2800, Loss: 1.302556\n",
      "Sample 0, Recon 2, Iter 2900, Loss: 1.262913\n",
      "Sample 0, Recon 2, Iter 3000, Loss: 1.225442\n",
      "Sample 0, Recon 2, Iter 3100, Loss: 1.189921\n",
      "Sample 0, Recon 2, Iter 3200, Loss: 1.156475\n",
      "Sample 0, Recon 2, Iter 3300, Loss: 1.124424\n",
      "Sample 0, Recon 2, Iter 3400, Loss: 1.095307\n",
      "Sample 0, Recon 2, Iter 3500, Loss: 1.067766\n",
      "Sample 0, Recon 2, Iter 3600, Loss: 1.042203\n",
      "Sample 0, Recon 2, Iter 3700, Loss: 1.017700\n",
      "Sample 0, Recon 2, Iter 3800, Loss: 0.995363\n",
      "Sample 0, Recon 2, Iter 3900, Loss: 0.975540\n",
      "Sample 0, Recon 2, Iter 4000, Loss: 0.955845\n",
      "Sample 0, Recon 2, Iter 4100, Loss: 0.937153\n",
      "Sample 0, Recon 2, Iter 4200, Loss: 0.920344\n",
      "Sample 0, Recon 2, Iter 4300, Loss: 0.906008\n",
      "Sample 0, Recon 2, Iter 4400, Loss: 0.890717\n",
      "Sample 0, Recon 2, Iter 4500, Loss: 0.876312\n",
      "Sample 0, Recon 2, Iter 4600, Loss: 0.864302\n",
      "Sample 0, Recon 2, Iter 4700, Loss: 0.852190\n",
      "Sample 0, Recon 2, Iter 4800, Loss: 0.839213\n",
      "Sample 0, Recon 2, Iter 4900, Loss: 0.827768\n",
      "Sample 0, Recon 2, Iter 5000, Loss: 0.818248\n",
      "Sample 0, Recon 2, Iter 5100, Loss: 0.809453\n",
      "Sample 0, Recon 2, Iter 5200, Loss: 0.798124\n",
      "Sample 0, Recon 2, Iter 5300, Loss: 0.789280\n",
      "Sample 0, Recon 2, Iter 5400, Loss: 0.780182\n",
      "Sample 0, Recon 2, Iter 5500, Loss: 0.774880\n",
      "Sample 0, Recon 2, Iter 5600, Loss: 0.763994\n",
      "Sample 0, Recon 2, Iter 5700, Loss: 0.756776\n",
      "Sample 0, Recon 2, Iter 5800, Loss: 0.749954\n",
      "Sample 0, Recon 2, Iter 5900, Loss: 0.742322\n",
      "Sample 0, Recon 2, Iter 6000, Loss: 0.738257\n",
      "Sample 0, Recon 2, Iter 6100, Loss: 0.728958\n",
      "Sample 0, Recon 2, Iter 6200, Loss: 0.723157\n",
      "Sample 0, Recon 2, Iter 6300, Loss: 0.716759\n",
      "Sample 0, Recon 2, Iter 6400, Loss: 0.713122\n",
      "Sample 0, Recon 2, Iter 6500, Loss: 0.705582\n",
      "Sample 0, Recon 2, Iter 6600, Loss: 0.703006\n",
      "Sample 0, Recon 2, Iter 6700, Loss: 0.696066\n",
      "Sample 0, Recon 2, Iter 6800, Loss: 0.689309\n",
      "Sample 0, Recon 2, Iter 6900, Loss: 0.684213\n",
      "Sample 0, Recon 2, Iter 7000, Loss: 0.679242\n",
      "Sample 0, Recon 2, Iter 7100, Loss: 0.675998\n",
      "Sample 0, Recon 2, Iter 7200, Loss: 0.675266\n",
      "Sample 0, Recon 2, Iter 7300, Loss: 0.665487\n",
      "Sample 0, Recon 2, Iter 7400, Loss: 0.661744\n",
      "Sample 0, Recon 2, Iter 7500, Loss: 0.657362\n",
      "Sample 0, Recon 2, Iter 7600, Loss: 0.654625\n",
      "Sample 0, Recon 2, Iter 7700, Loss: 0.649492\n",
      "Sample 0, Recon 2, Iter 7800, Loss: 0.645652\n",
      "Sample 0, Recon 2, Iter 7900, Loss: 0.641961\n",
      "Sample 0, Recon 2, Iter 8000, Loss: 0.640847\n",
      "Sample 0, Recon 2, Iter 8100, Loss: 0.636154\n",
      "Sample 0, Recon 2, Iter 8200, Loss: 0.632241\n",
      "Sample 0, Recon 2, Iter 8300, Loss: 0.628514\n",
      "Sample 0, Recon 2, Iter 8400, Loss: 0.624533\n",
      "Sample 0, Recon 2, Iter 8500, Loss: 0.623291\n",
      "Sample 0, Recon 2, Iter 8600, Loss: 0.619898\n",
      "Sample 0, Recon 2, Iter 8700, Loss: 0.617716\n",
      "Sample 0, Recon 2, Iter 8800, Loss: 0.612993\n",
      "Sample 0, Recon 2, Iter 8900, Loss: 0.609418\n",
      "Sample 0, Recon 2, Iter 9000, Loss: 0.606392\n",
      "Sample 0, Recon 2, Iter 9100, Loss: 0.603702\n",
      "Sample 0, Recon 2, Iter 9200, Loss: 0.601048\n",
      "Sample 0, Recon 2, Iter 9300, Loss: 0.600360\n",
      "Sample 0, Recon 2, Iter 9400, Loss: 0.595893\n",
      "Sample 0, Recon 2, Iter 9500, Loss: 0.593161\n",
      "Sample 0, Recon 2, Iter 9600, Loss: 0.591006\n",
      "Sample 0, Recon 2, Iter 9700, Loss: 0.587607\n",
      "Sample 0, Recon 2, Iter 9800, Loss: 0.585792\n",
      "Sample 0, Recon 2, Iter 9900, Loss: 0.582516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:  10%|█         | 1/10 [40:42<6:06:23, 2442.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1, Recon 0, Iter 0, Loss: 18.817217\n",
      "Sample 1, Recon 0, Iter 100, Loss: 6.056330\n",
      "Sample 1, Recon 0, Iter 200, Loss: 4.589106\n",
      "Sample 1, Recon 0, Iter 300, Loss: 3.701640\n",
      "Sample 1, Recon 0, Iter 400, Loss: 3.357178\n",
      "Sample 1, Recon 0, Iter 500, Loss: 3.164711\n",
      "Sample 1, Recon 0, Iter 600, Loss: 3.012650\n",
      "Sample 1, Recon 0, Iter 700, Loss: 2.877337\n",
      "Sample 1, Recon 0, Iter 800, Loss: 2.747234\n",
      "Sample 1, Recon 0, Iter 900, Loss: 2.616783\n",
      "Sample 1, Recon 0, Iter 1000, Loss: 2.485771\n",
      "Sample 1, Recon 0, Iter 1100, Loss: 2.362695\n",
      "Sample 1, Recon 0, Iter 1200, Loss: 2.257669\n",
      "Sample 1, Recon 0, Iter 1300, Loss: 2.167194\n",
      "Sample 1, Recon 0, Iter 1400, Loss: 2.085522\n",
      "Sample 1, Recon 0, Iter 1500, Loss: 2.009644\n",
      "Sample 1, Recon 0, Iter 1600, Loss: 1.937713\n",
      "Sample 1, Recon 0, Iter 1700, Loss: 1.869321\n",
      "Sample 1, Recon 0, Iter 1800, Loss: 1.804370\n",
      "Sample 1, Recon 0, Iter 1900, Loss: 1.742591\n",
      "Sample 1, Recon 0, Iter 2000, Loss: 1.684048\n",
      "Sample 1, Recon 0, Iter 2100, Loss: 1.629177\n",
      "Sample 1, Recon 0, Iter 2200, Loss: 1.576511\n",
      "Sample 1, Recon 0, Iter 2300, Loss: 1.529383\n",
      "Sample 1, Recon 0, Iter 2400, Loss: 1.482075\n",
      "Sample 1, Recon 0, Iter 2500, Loss: 1.439035\n",
      "Sample 1, Recon 0, Iter 2600, Loss: 1.398811\n",
      "Sample 1, Recon 0, Iter 2700, Loss: 1.360381\n",
      "Sample 1, Recon 0, Iter 2800, Loss: 1.324283\n",
      "Sample 1, Recon 0, Iter 2900, Loss: 1.290995\n",
      "Sample 1, Recon 0, Iter 3000, Loss: 1.259478\n",
      "Sample 1, Recon 0, Iter 3100, Loss: 1.229598\n",
      "Sample 1, Recon 0, Iter 3200, Loss: 1.200832\n",
      "Sample 1, Recon 0, Iter 3300, Loss: 1.174109\n",
      "Sample 1, Recon 0, Iter 3400, Loss: 1.148387\n",
      "Sample 1, Recon 0, Iter 3500, Loss: 1.125084\n",
      "Sample 1, Recon 0, Iter 3600, Loss: 1.101836\n",
      "Sample 1, Recon 0, Iter 3700, Loss: 1.081688\n",
      "Sample 1, Recon 0, Iter 3800, Loss: 1.061912\n",
      "Sample 1, Recon 0, Iter 3900, Loss: 1.043741\n",
      "Sample 1, Recon 0, Iter 4000, Loss: 1.026048\n",
      "Sample 1, Recon 0, Iter 4100, Loss: 1.008984\n",
      "Sample 1, Recon 0, Iter 4200, Loss: 0.994559\n",
      "Sample 1, Recon 0, Iter 4300, Loss: 0.978426\n",
      "Sample 1, Recon 0, Iter 4400, Loss: 0.964248\n",
      "Sample 1, Recon 0, Iter 4500, Loss: 0.948994\n",
      "Sample 1, Recon 0, Iter 4600, Loss: 0.937224\n",
      "Sample 1, Recon 0, Iter 4700, Loss: 0.924150\n",
      "Sample 1, Recon 0, Iter 4800, Loss: 0.912423\n",
      "Sample 1, Recon 0, Iter 4900, Loss: 0.899737\n",
      "Sample 1, Recon 0, Iter 5000, Loss: 0.888574\n",
      "Sample 1, Recon 0, Iter 5100, Loss: 0.878078\n",
      "Sample 1, Recon 0, Iter 5200, Loss: 0.868040\n",
      "Sample 1, Recon 0, Iter 5300, Loss: 0.858370\n",
      "Sample 1, Recon 0, Iter 5400, Loss: 0.850550\n",
      "Sample 1, Recon 0, Iter 5500, Loss: 0.840715\n",
      "Sample 1, Recon 0, Iter 5600, Loss: 0.834884\n",
      "Sample 1, Recon 0, Iter 5700, Loss: 0.821674\n",
      "Sample 1, Recon 0, Iter 5800, Loss: 0.812505\n",
      "Sample 1, Recon 0, Iter 5900, Loss: 0.804328\n",
      "Sample 1, Recon 0, Iter 6000, Loss: 0.796538\n",
      "Sample 1, Recon 0, Iter 6100, Loss: 0.789251\n",
      "Sample 1, Recon 0, Iter 6200, Loss: 0.782730\n",
      "Sample 1, Recon 0, Iter 6300, Loss: 0.774515\n",
      "Sample 1, Recon 0, Iter 6400, Loss: 0.770319\n",
      "Sample 1, Recon 0, Iter 6500, Loss: 0.761245\n",
      "Sample 1, Recon 0, Iter 6600, Loss: 0.755558\n",
      "Sample 1, Recon 0, Iter 6700, Loss: 0.748564\n",
      "Sample 1, Recon 0, Iter 6800, Loss: 0.742293\n",
      "Sample 1, Recon 0, Iter 6900, Loss: 0.736451\n",
      "Sample 1, Recon 0, Iter 7000, Loss: 0.732480\n",
      "Sample 1, Recon 0, Iter 7100, Loss: 0.724604\n",
      "Sample 1, Recon 0, Iter 7200, Loss: 0.722555\n",
      "Sample 1, Recon 0, Iter 7300, Loss: 0.718687\n",
      "Sample 1, Recon 0, Iter 7400, Loss: 0.709101\n",
      "Sample 1, Recon 0, Iter 7500, Loss: 0.704553\n",
      "Sample 1, Recon 0, Iter 7600, Loss: 0.698807\n",
      "Sample 1, Recon 0, Iter 7700, Loss: 0.695867\n",
      "Sample 1, Recon 0, Iter 7800, Loss: 0.689899\n",
      "Sample 1, Recon 0, Iter 7900, Loss: 0.686156\n",
      "Sample 1, Recon 0, Iter 8000, Loss: 0.680870\n",
      "Sample 1, Recon 0, Iter 8100, Loss: 0.676504\n",
      "Sample 1, Recon 0, Iter 8200, Loss: 0.673576\n",
      "Sample 1, Recon 0, Iter 8300, Loss: 0.668915\n",
      "Sample 1, Recon 0, Iter 8400, Loss: 0.664570\n",
      "Sample 1, Recon 0, Iter 8500, Loss: 0.660265\n",
      "Sample 1, Recon 0, Iter 8600, Loss: 0.656496\n",
      "Sample 1, Recon 0, Iter 8700, Loss: 0.652765\n",
      "Sample 1, Recon 0, Iter 8800, Loss: 0.649051\n",
      "Sample 1, Recon 0, Iter 8900, Loss: 0.645658\n",
      "Sample 1, Recon 0, Iter 9000, Loss: 0.642197\n",
      "Sample 1, Recon 0, Iter 9100, Loss: 0.639798\n",
      "Sample 1, Recon 0, Iter 9200, Loss: 0.635534\n",
      "Sample 1, Recon 0, Iter 9300, Loss: 0.632624\n",
      "Sample 1, Recon 0, Iter 9400, Loss: 0.629078\n",
      "Sample 1, Recon 0, Iter 9500, Loss: 0.626543\n",
      "Sample 1, Recon 0, Iter 9600, Loss: 0.627469\n",
      "Sample 1, Recon 0, Iter 9700, Loss: 0.620820\n",
      "Sample 1, Recon 0, Iter 9800, Loss: 0.617238\n",
      "Sample 1, Recon 0, Iter 9900, Loss: 0.614071\n",
      "Sample 1, Recon 1, Iter 0, Loss: 16.319498\n",
      "Sample 1, Recon 1, Iter 100, Loss: 5.167575\n",
      "Sample 1, Recon 1, Iter 200, Loss: 4.064913\n",
      "Sample 1, Recon 1, Iter 300, Loss: 3.567583\n",
      "Sample 1, Recon 1, Iter 400, Loss: 3.268304\n",
      "Sample 1, Recon 1, Iter 500, Loss: 3.058147\n",
      "Sample 1, Recon 1, Iter 600, Loss: 2.892247\n",
      "Sample 1, Recon 1, Iter 700, Loss: 2.750459\n",
      "Sample 1, Recon 1, Iter 800, Loss: 2.617866\n",
      "Sample 1, Recon 1, Iter 900, Loss: 2.490456\n",
      "Sample 1, Recon 1, Iter 1000, Loss: 2.370879\n",
      "Sample 1, Recon 1, Iter 1100, Loss: 2.263416\n",
      "Sample 1, Recon 1, Iter 1200, Loss: 2.168060\n",
      "Sample 1, Recon 1, Iter 1300, Loss: 2.082240\n",
      "Sample 1, Recon 1, Iter 1400, Loss: 2.002882\n",
      "Sample 1, Recon 1, Iter 1500, Loss: 1.928240\n",
      "Sample 1, Recon 1, Iter 1600, Loss: 1.856975\n",
      "Sample 1, Recon 1, Iter 1700, Loss: 1.788851\n",
      "Sample 1, Recon 1, Iter 1800, Loss: 1.723553\n",
      "Sample 1, Recon 1, Iter 1900, Loss: 1.660820\n",
      "Sample 1, Recon 1, Iter 2000, Loss: 1.600701\n",
      "Sample 1, Recon 1, Iter 2100, Loss: 1.543093\n",
      "Sample 1, Recon 1, Iter 2200, Loss: 1.487859\n",
      "Sample 1, Recon 1, Iter 2300, Loss: 1.435215\n",
      "Sample 1, Recon 1, Iter 2400, Loss: 1.385276\n",
      "Sample 1, Recon 1, Iter 2500, Loss: 1.338512\n",
      "Sample 1, Recon 1, Iter 2600, Loss: 1.294873\n",
      "Sample 1, Recon 1, Iter 2700, Loss: 1.253418\n",
      "Sample 1, Recon 1, Iter 2800, Loss: 1.214139\n",
      "Sample 1, Recon 1, Iter 2900, Loss: 1.177304\n",
      "Sample 1, Recon 1, Iter 3000, Loss: 1.142575\n",
      "Sample 1, Recon 1, Iter 3100, Loss: 1.110148\n",
      "Sample 1, Recon 1, Iter 3200, Loss: 1.079450\n",
      "Sample 1, Recon 1, Iter 3300, Loss: 1.051583\n",
      "Sample 1, Recon 1, Iter 3400, Loss: 1.024955\n",
      "Sample 1, Recon 1, Iter 3500, Loss: 1.000495\n",
      "Sample 1, Recon 1, Iter 3600, Loss: 0.977325\n",
      "Sample 1, Recon 1, Iter 3700, Loss: 0.955299\n",
      "Sample 1, Recon 1, Iter 3800, Loss: 0.934458\n",
      "Sample 1, Recon 1, Iter 3900, Loss: 0.915027\n",
      "Sample 1, Recon 1, Iter 4000, Loss: 0.895769\n",
      "Sample 1, Recon 1, Iter 4100, Loss: 0.879915\n",
      "Sample 1, Recon 1, Iter 4200, Loss: 0.861407\n",
      "Sample 1, Recon 1, Iter 4300, Loss: 0.846517\n",
      "Sample 1, Recon 1, Iter 4400, Loss: 0.832073\n",
      "Sample 1, Recon 1, Iter 4500, Loss: 0.818545\n",
      "Sample 1, Recon 1, Iter 4600, Loss: 0.806981\n",
      "Sample 1, Recon 1, Iter 4700, Loss: 0.793948\n",
      "Sample 1, Recon 1, Iter 4800, Loss: 0.782707\n",
      "Sample 1, Recon 1, Iter 4900, Loss: 0.772156\n",
      "Sample 1, Recon 1, Iter 5000, Loss: 0.760711\n",
      "Sample 1, Recon 1, Iter 5100, Loss: 0.751321\n",
      "Sample 1, Recon 1, Iter 5200, Loss: 0.741918\n",
      "Sample 1, Recon 1, Iter 5300, Loss: 0.732958\n",
      "Sample 1, Recon 1, Iter 5400, Loss: 0.724718\n",
      "Sample 1, Recon 1, Iter 5500, Loss: 0.716981\n",
      "Sample 1, Recon 1, Iter 5600, Loss: 0.709973\n",
      "Sample 1, Recon 1, Iter 5700, Loss: 0.701952\n",
      "Sample 1, Recon 1, Iter 5800, Loss: 0.696580\n",
      "Sample 1, Recon 1, Iter 5900, Loss: 0.688782\n",
      "Sample 1, Recon 1, Iter 6000, Loss: 0.682435\n",
      "Sample 1, Recon 1, Iter 6100, Loss: 0.677284\n",
      "Sample 1, Recon 1, Iter 6200, Loss: 0.671330\n",
      "Sample 1, Recon 1, Iter 6300, Loss: 0.666613\n",
      "Sample 1, Recon 1, Iter 6400, Loss: 0.661653\n",
      "Sample 1, Recon 1, Iter 6500, Loss: 0.655782\n",
      "Sample 1, Recon 1, Iter 6600, Loss: 0.651704\n",
      "Sample 1, Recon 1, Iter 6700, Loss: 0.646690\n",
      "Sample 1, Recon 1, Iter 6800, Loss: 0.641598\n",
      "Sample 1, Recon 1, Iter 6900, Loss: 0.637419\n",
      "Sample 1, Recon 1, Iter 7000, Loss: 0.634005\n",
      "Sample 1, Recon 1, Iter 7100, Loss: 0.629359\n",
      "Sample 1, Recon 1, Iter 7200, Loss: 0.625730\n",
      "Sample 1, Recon 1, Iter 7300, Loss: 0.621518\n",
      "Sample 1, Recon 1, Iter 7400, Loss: 0.616985\n",
      "Sample 1, Recon 1, Iter 7500, Loss: 0.613282\n",
      "Sample 1, Recon 1, Iter 7600, Loss: 0.612075\n",
      "Sample 1, Recon 1, Iter 7700, Loss: 0.606699\n",
      "Sample 1, Recon 1, Iter 7800, Loss: 0.602489\n",
      "Sample 1, Recon 1, Iter 7900, Loss: 0.599918\n",
      "Sample 1, Recon 1, Iter 8000, Loss: 0.596103\n",
      "Sample 1, Recon 1, Iter 8100, Loss: 0.592422\n",
      "Sample 1, Recon 1, Iter 8200, Loss: 0.592084\n",
      "Sample 1, Recon 1, Iter 8300, Loss: 0.588425\n",
      "Sample 1, Recon 1, Iter 8400, Loss: 0.583132\n",
      "Sample 1, Recon 1, Iter 8500, Loss: 0.582442\n",
      "Sample 1, Recon 1, Iter 8600, Loss: 0.582328\n",
      "Sample 1, Recon 1, Iter 8700, Loss: 0.577318\n",
      "Sample 1, Recon 1, Iter 8800, Loss: 0.572495\n",
      "Sample 1, Recon 1, Iter 8900, Loss: 0.569075\n",
      "Sample 1, Recon 1, Iter 9000, Loss: 0.566430\n",
      "Sample 1, Recon 1, Iter 9100, Loss: 0.563859\n",
      "Sample 1, Recon 1, Iter 9200, Loss: 0.561583\n",
      "Sample 1, Recon 1, Iter 9300, Loss: 0.558764\n",
      "Sample 1, Recon 1, Iter 9400, Loss: 0.557216\n",
      "Sample 1, Recon 1, Iter 9500, Loss: 0.554297\n",
      "Sample 1, Recon 1, Iter 9600, Loss: 0.551844\n",
      "Sample 1, Recon 1, Iter 9700, Loss: 0.550159\n",
      "Sample 1, Recon 1, Iter 9800, Loss: 0.547573\n",
      "Sample 1, Recon 1, Iter 9900, Loss: 0.544876\n",
      "Sample 1, Recon 2, Iter 0, Loss: 14.372749\n",
      "Sample 1, Recon 2, Iter 100, Loss: 4.920686\n",
      "Sample 1, Recon 2, Iter 200, Loss: 4.039754\n",
      "Sample 1, Recon 2, Iter 300, Loss: 3.566104\n",
      "Sample 1, Recon 2, Iter 400, Loss: 3.319644\n",
      "Sample 1, Recon 2, Iter 500, Loss: 3.151929\n",
      "Sample 1, Recon 2, Iter 600, Loss: 3.020962\n",
      "Sample 1, Recon 2, Iter 700, Loss: 2.909619\n",
      "Sample 1, Recon 2, Iter 800, Loss: 2.805847\n",
      "Sample 1, Recon 2, Iter 900, Loss: 2.694904\n",
      "Sample 1, Recon 2, Iter 1000, Loss: 2.574120\n",
      "Sample 1, Recon 2, Iter 1100, Loss: 2.460066\n",
      "Sample 1, Recon 2, Iter 1200, Loss: 2.363596\n",
      "Sample 1, Recon 2, Iter 1300, Loss: 2.281267\n",
      "Sample 1, Recon 2, Iter 1400, Loss: 2.206638\n",
      "Sample 1, Recon 2, Iter 1500, Loss: 2.136124\n",
      "Sample 1, Recon 2, Iter 1600, Loss: 2.068472\n",
      "Sample 1, Recon 2, Iter 1700, Loss: 2.003340\n",
      "Sample 1, Recon 2, Iter 1800, Loss: 1.940082\n",
      "Sample 1, Recon 2, Iter 1900, Loss: 1.880067\n",
      "Sample 1, Recon 2, Iter 2000, Loss: 1.823517\n",
      "Sample 1, Recon 2, Iter 2100, Loss: 1.771711\n",
      "Sample 1, Recon 2, Iter 2200, Loss: 1.723094\n",
      "Sample 1, Recon 2, Iter 2300, Loss: 1.677890\n",
      "Sample 1, Recon 2, Iter 2400, Loss: 1.634816\n",
      "Sample 1, Recon 2, Iter 2500, Loss: 1.594279\n",
      "Sample 1, Recon 2, Iter 2600, Loss: 1.554716\n",
      "Sample 1, Recon 2, Iter 2700, Loss: 1.516837\n",
      "Sample 1, Recon 2, Iter 2800, Loss: 1.480905\n",
      "Sample 1, Recon 2, Iter 2900, Loss: 1.445302\n",
      "Sample 1, Recon 2, Iter 3000, Loss: 1.411190\n",
      "Sample 1, Recon 2, Iter 3100, Loss: 1.377797\n",
      "Sample 1, Recon 2, Iter 3200, Loss: 1.346521\n",
      "Sample 1, Recon 2, Iter 3300, Loss: 1.314892\n",
      "Sample 1, Recon 2, Iter 3400, Loss: 1.284365\n",
      "Sample 1, Recon 2, Iter 3500, Loss: 1.254686\n",
      "Sample 1, Recon 2, Iter 3600, Loss: 1.225657\n",
      "Sample 1, Recon 2, Iter 3700, Loss: 1.197654\n",
      "Sample 1, Recon 2, Iter 3800, Loss: 1.169904\n",
      "Sample 1, Recon 2, Iter 3900, Loss: 1.143428\n",
      "Sample 1, Recon 2, Iter 4000, Loss: 1.119554\n",
      "Sample 1, Recon 2, Iter 4100, Loss: 1.094847\n",
      "Sample 1, Recon 2, Iter 4200, Loss: 1.072039\n",
      "Sample 1, Recon 2, Iter 4300, Loss: 1.050607\n",
      "Sample 1, Recon 2, Iter 4400, Loss: 1.030428\n",
      "Sample 1, Recon 2, Iter 4500, Loss: 1.010393\n",
      "Sample 1, Recon 2, Iter 4600, Loss: 0.992137\n",
      "Sample 1, Recon 2, Iter 4700, Loss: 0.974531\n",
      "Sample 1, Recon 2, Iter 4800, Loss: 0.957759\n",
      "Sample 1, Recon 2, Iter 4900, Loss: 0.942198\n",
      "Sample 1, Recon 2, Iter 5000, Loss: 0.927227\n",
      "Sample 1, Recon 2, Iter 5100, Loss: 0.912768\n",
      "Sample 1, Recon 2, Iter 5200, Loss: 0.899057\n",
      "Sample 1, Recon 2, Iter 5300, Loss: 0.886257\n",
      "Sample 1, Recon 2, Iter 5400, Loss: 0.874029\n",
      "Sample 1, Recon 2, Iter 5500, Loss: 0.862208\n",
      "Sample 1, Recon 2, Iter 5600, Loss: 0.851948\n",
      "Sample 1, Recon 2, Iter 5700, Loss: 0.840423\n",
      "Sample 1, Recon 2, Iter 5800, Loss: 0.829642\n",
      "Sample 1, Recon 2, Iter 5900, Loss: 0.820316\n",
      "Sample 1, Recon 2, Iter 6000, Loss: 0.810417\n",
      "Sample 1, Recon 2, Iter 6100, Loss: 0.800901\n",
      "Sample 1, Recon 2, Iter 6200, Loss: 0.794017\n",
      "Sample 1, Recon 2, Iter 6300, Loss: 0.784877\n",
      "Sample 1, Recon 2, Iter 6400, Loss: 0.777742\n",
      "Sample 1, Recon 2, Iter 6500, Loss: 0.769731\n",
      "Sample 1, Recon 2, Iter 6600, Loss: 0.762320\n",
      "Sample 1, Recon 2, Iter 6700, Loss: 0.755014\n",
      "Sample 1, Recon 2, Iter 6800, Loss: 0.747632\n",
      "Sample 1, Recon 2, Iter 6900, Loss: 0.740443\n",
      "Sample 1, Recon 2, Iter 7000, Loss: 0.733620\n",
      "Sample 1, Recon 2, Iter 7100, Loss: 0.724216\n",
      "Sample 1, Recon 2, Iter 7200, Loss: 0.703430\n",
      "Sample 1, Recon 2, Iter 7300, Loss: 0.685382\n",
      "Sample 1, Recon 2, Iter 7400, Loss: 0.673069\n",
      "Sample 1, Recon 2, Iter 7500, Loss: 0.662595\n",
      "Sample 1, Recon 2, Iter 7600, Loss: 0.653256\n",
      "Sample 1, Recon 2, Iter 7700, Loss: 0.645249\n",
      "Sample 1, Recon 2, Iter 7800, Loss: 0.637607\n",
      "Sample 1, Recon 2, Iter 7900, Loss: 0.631071\n",
      "Sample 1, Recon 2, Iter 8000, Loss: 0.624906\n",
      "Sample 1, Recon 2, Iter 8100, Loss: 0.617598\n",
      "Sample 1, Recon 2, Iter 8200, Loss: 0.611808\n",
      "Sample 1, Recon 2, Iter 8300, Loss: 0.606992\n",
      "Sample 1, Recon 2, Iter 8400, Loss: 0.600371\n",
      "Sample 1, Recon 2, Iter 8500, Loss: 0.595394\n",
      "Sample 1, Recon 2, Iter 8600, Loss: 0.592035\n",
      "Sample 1, Recon 2, Iter 8700, Loss: 0.585239\n",
      "Sample 1, Recon 2, Iter 8800, Loss: 0.580531\n",
      "Sample 1, Recon 2, Iter 8900, Loss: 0.575898\n",
      "Sample 1, Recon 2, Iter 9000, Loss: 0.571729\n",
      "Sample 1, Recon 2, Iter 9100, Loss: 0.567715\n",
      "Sample 1, Recon 2, Iter 9200, Loss: 0.562990\n",
      "Sample 1, Recon 2, Iter 9300, Loss: 0.558829\n",
      "Sample 1, Recon 2, Iter 9400, Loss: 0.554968\n",
      "Sample 1, Recon 2, Iter 9500, Loss: 0.552239\n",
      "Sample 1, Recon 2, Iter 9600, Loss: 0.547069\n",
      "Sample 1, Recon 2, Iter 9700, Loss: 0.544116\n",
      "Sample 1, Recon 2, Iter 9800, Loss: 0.540455\n",
      "Sample 1, Recon 2, Iter 9900, Loss: 0.538555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:  20%|██        | 2/10 [1:21:06<5:24:11, 2431.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 2, Recon 0, Iter 0, Loss: 17.252518\n",
      "Sample 2, Recon 0, Iter 100, Loss: 4.607908\n",
      "Sample 2, Recon 0, Iter 200, Loss: 3.768399\n",
      "Sample 2, Recon 0, Iter 300, Loss: 3.402176\n",
      "Sample 2, Recon 0, Iter 400, Loss: 3.168390\n",
      "Sample 2, Recon 0, Iter 500, Loss: 2.995996\n",
      "Sample 2, Recon 0, Iter 600, Loss: 2.857948\n",
      "Sample 2, Recon 0, Iter 700, Loss: 2.740199\n",
      "Sample 2, Recon 0, Iter 800, Loss: 2.627674\n",
      "Sample 2, Recon 0, Iter 900, Loss: 2.512667\n",
      "Sample 2, Recon 0, Iter 1000, Loss: 2.399300\n",
      "Sample 2, Recon 0, Iter 1100, Loss: 2.288285\n",
      "Sample 2, Recon 0, Iter 1200, Loss: 2.183577\n",
      "Sample 2, Recon 0, Iter 1300, Loss: 2.087837\n",
      "Sample 2, Recon 0, Iter 1400, Loss: 2.000232\n",
      "Sample 2, Recon 0, Iter 1500, Loss: 1.918835\n",
      "Sample 2, Recon 0, Iter 1600, Loss: 1.842659\n",
      "Sample 2, Recon 0, Iter 1700, Loss: 1.772247\n",
      "Sample 2, Recon 0, Iter 1800, Loss: 1.706420\n",
      "Sample 2, Recon 0, Iter 1900, Loss: 1.643850\n",
      "Sample 2, Recon 0, Iter 2000, Loss: 1.584657\n",
      "Sample 2, Recon 0, Iter 2100, Loss: 1.528299\n",
      "Sample 2, Recon 0, Iter 2200, Loss: 1.474822\n",
      "Sample 2, Recon 0, Iter 2300, Loss: 1.424227\n",
      "Sample 2, Recon 0, Iter 2400, Loss: 1.376398\n",
      "Sample 2, Recon 0, Iter 2500, Loss: 1.330977\n",
      "Sample 2, Recon 0, Iter 2600, Loss: 1.288104\n",
      "Sample 2, Recon 0, Iter 2700, Loss: 1.247597\n",
      "Sample 2, Recon 0, Iter 2800, Loss: 1.209599\n",
      "Sample 2, Recon 0, Iter 2900, Loss: 1.173791\n",
      "Sample 2, Recon 0, Iter 3000, Loss: 1.139964\n",
      "Sample 2, Recon 0, Iter 3100, Loss: 1.108181\n",
      "Sample 2, Recon 0, Iter 3200, Loss: 1.078630\n",
      "Sample 2, Recon 0, Iter 3300, Loss: 1.050602\n",
      "Sample 2, Recon 0, Iter 3400, Loss: 1.024313\n",
      "Sample 2, Recon 0, Iter 3500, Loss: 0.999095\n",
      "Sample 2, Recon 0, Iter 3600, Loss: 0.975821\n",
      "Sample 2, Recon 0, Iter 3700, Loss: 0.952967\n",
      "Sample 2, Recon 0, Iter 3800, Loss: 0.932068\n",
      "Sample 2, Recon 0, Iter 3900, Loss: 0.912090\n",
      "Sample 2, Recon 0, Iter 4000, Loss: 0.893305\n",
      "Sample 2, Recon 0, Iter 4100, Loss: 0.875691\n",
      "Sample 2, Recon 0, Iter 4200, Loss: 0.859813\n",
      "Sample 2, Recon 0, Iter 4300, Loss: 0.842972\n",
      "Sample 2, Recon 0, Iter 4400, Loss: 0.827616\n",
      "Sample 2, Recon 0, Iter 4500, Loss: 0.813173\n",
      "Sample 2, Recon 0, Iter 4600, Loss: 0.799328\n",
      "Sample 2, Recon 0, Iter 4700, Loss: 0.785494\n",
      "Sample 2, Recon 0, Iter 4800, Loss: 0.772568\n",
      "Sample 2, Recon 0, Iter 4900, Loss: 0.760742\n",
      "Sample 2, Recon 0, Iter 5000, Loss: 0.749787\n",
      "Sample 2, Recon 0, Iter 5100, Loss: 0.737723\n",
      "Sample 2, Recon 0, Iter 5200, Loss: 0.726847\n",
      "Sample 2, Recon 0, Iter 5300, Loss: 0.716382\n",
      "Sample 2, Recon 0, Iter 5400, Loss: 0.708093\n",
      "Sample 2, Recon 0, Iter 5500, Loss: 0.697070\n",
      "Sample 2, Recon 0, Iter 5600, Loss: 0.687998\n",
      "Sample 2, Recon 0, Iter 5700, Loss: 0.678701\n",
      "Sample 2, Recon 0, Iter 5800, Loss: 0.669947\n",
      "Sample 2, Recon 0, Iter 5900, Loss: 0.662266\n",
      "Sample 2, Recon 0, Iter 6000, Loss: 0.654042\n",
      "Sample 2, Recon 0, Iter 6100, Loss: 0.646434\n",
      "Sample 2, Recon 0, Iter 6200, Loss: 0.639407\n",
      "Sample 2, Recon 0, Iter 6300, Loss: 0.632333\n",
      "Sample 2, Recon 0, Iter 6400, Loss: 0.625377\n",
      "Sample 2, Recon 0, Iter 6500, Loss: 0.619052\n",
      "Sample 2, Recon 0, Iter 6600, Loss: 0.612744\n",
      "Sample 2, Recon 0, Iter 6700, Loss: 0.606581\n",
      "Sample 2, Recon 0, Iter 6800, Loss: 0.600986\n",
      "Sample 2, Recon 0, Iter 6900, Loss: 0.594842\n",
      "Sample 2, Recon 0, Iter 7000, Loss: 0.589705\n",
      "Sample 2, Recon 0, Iter 7100, Loss: 0.584806\n",
      "Sample 2, Recon 0, Iter 7200, Loss: 0.579091\n",
      "Sample 2, Recon 0, Iter 7300, Loss: 0.574462\n",
      "Sample 2, Recon 0, Iter 7400, Loss: 0.570453\n",
      "Sample 2, Recon 0, Iter 7500, Loss: 0.564668\n",
      "Sample 2, Recon 0, Iter 7600, Loss: 0.560614\n",
      "Sample 2, Recon 0, Iter 7700, Loss: 0.557006\n",
      "Sample 2, Recon 0, Iter 7800, Loss: 0.551806\n",
      "Sample 2, Recon 0, Iter 7900, Loss: 0.547524\n",
      "Sample 2, Recon 0, Iter 8000, Loss: 0.543293\n",
      "Sample 2, Recon 0, Iter 8100, Loss: 0.539737\n",
      "Sample 2, Recon 0, Iter 8200, Loss: 0.535659\n",
      "Sample 2, Recon 0, Iter 8300, Loss: 0.532011\n",
      "Sample 2, Recon 0, Iter 8400, Loss: 0.528506\n",
      "Sample 2, Recon 0, Iter 8500, Loss: 0.526008\n",
      "Sample 2, Recon 0, Iter 8600, Loss: 0.521795\n",
      "Sample 2, Recon 0, Iter 8700, Loss: 0.519115\n",
      "Sample 2, Recon 0, Iter 8800, Loss: 0.515911\n",
      "Sample 2, Recon 0, Iter 8900, Loss: 0.512589\n",
      "Sample 2, Recon 0, Iter 9000, Loss: 0.509430\n",
      "Sample 2, Recon 0, Iter 9100, Loss: 0.506447\n",
      "Sample 2, Recon 0, Iter 9200, Loss: 0.504855\n",
      "Sample 2, Recon 0, Iter 9300, Loss: 0.500803\n",
      "Sample 2, Recon 0, Iter 9400, Loss: 0.498221\n",
      "Sample 2, Recon 0, Iter 9500, Loss: 0.496303\n",
      "Sample 2, Recon 0, Iter 9600, Loss: 0.492888\n",
      "Sample 2, Recon 0, Iter 9700, Loss: 0.490953\n",
      "Sample 2, Recon 0, Iter 9800, Loss: 0.489589\n",
      "Sample 2, Recon 0, Iter 9900, Loss: 0.485802\n",
      "Sample 2, Recon 1, Iter 0, Loss: 20.612154\n",
      "Sample 2, Recon 1, Iter 100, Loss: 5.359956\n",
      "Sample 2, Recon 1, Iter 200, Loss: 3.923552\n",
      "Sample 2, Recon 1, Iter 300, Loss: 3.302189\n",
      "Sample 2, Recon 1, Iter 400, Loss: 3.072376\n",
      "Sample 2, Recon 1, Iter 500, Loss: 2.906246\n",
      "Sample 2, Recon 1, Iter 600, Loss: 2.781480\n",
      "Sample 2, Recon 1, Iter 700, Loss: 2.678798\n",
      "Sample 2, Recon 1, Iter 800, Loss: 2.589798\n",
      "Sample 2, Recon 1, Iter 900, Loss: 2.508891\n",
      "Sample 2, Recon 1, Iter 1000, Loss: 2.430151\n",
      "Sample 2, Recon 1, Iter 1100, Loss: 2.349278\n",
      "Sample 2, Recon 1, Iter 1200, Loss: 2.265224\n",
      "Sample 2, Recon 1, Iter 1300, Loss: 2.179524\n",
      "Sample 2, Recon 1, Iter 1400, Loss: 2.094573\n",
      "Sample 2, Recon 1, Iter 1500, Loss: 2.013026\n",
      "Sample 2, Recon 1, Iter 1600, Loss: 1.936201\n",
      "Sample 2, Recon 1, Iter 1700, Loss: 1.864919\n",
      "Sample 2, Recon 1, Iter 1800, Loss: 1.798305\n",
      "Sample 2, Recon 1, Iter 1900, Loss: 1.735743\n",
      "Sample 2, Recon 1, Iter 2000, Loss: 1.676586\n",
      "Sample 2, Recon 1, Iter 2100, Loss: 1.620205\n",
      "Sample 2, Recon 1, Iter 2200, Loss: 1.566317\n",
      "Sample 2, Recon 1, Iter 2300, Loss: 1.514599\n",
      "Sample 2, Recon 1, Iter 2400, Loss: 1.465313\n",
      "Sample 2, Recon 1, Iter 2500, Loss: 1.418236\n",
      "Sample 2, Recon 1, Iter 2600, Loss: 1.373280\n",
      "Sample 2, Recon 1, Iter 2700, Loss: 1.330790\n",
      "Sample 2, Recon 1, Iter 2800, Loss: 1.290124\n",
      "Sample 2, Recon 1, Iter 2900, Loss: 1.251474\n",
      "Sample 2, Recon 1, Iter 3000, Loss: 1.214807\n",
      "Sample 2, Recon 1, Iter 3100, Loss: 1.179774\n",
      "Sample 2, Recon 1, Iter 3200, Loss: 1.146027\n",
      "Sample 2, Recon 1, Iter 3300, Loss: 1.114229\n",
      "Sample 2, Recon 1, Iter 3400, Loss: 1.084081\n",
      "Sample 2, Recon 1, Iter 3500, Loss: 1.055866\n",
      "Sample 2, Recon 1, Iter 3600, Loss: 1.028988\n",
      "Sample 2, Recon 1, Iter 3700, Loss: 1.004073\n",
      "Sample 2, Recon 1, Iter 3800, Loss: 0.980789\n",
      "Sample 2, Recon 1, Iter 3900, Loss: 0.960456\n",
      "Sample 2, Recon 1, Iter 4000, Loss: 0.940214\n",
      "Sample 2, Recon 1, Iter 4100, Loss: 0.921536\n",
      "Sample 2, Recon 1, Iter 4200, Loss: 0.904540\n",
      "Sample 2, Recon 1, Iter 4300, Loss: 0.888274\n",
      "Sample 2, Recon 1, Iter 4400, Loss: 0.872432\n",
      "Sample 2, Recon 1, Iter 4500, Loss: 0.858382\n",
      "Sample 2, Recon 1, Iter 4600, Loss: 0.844798\n",
      "Sample 2, Recon 1, Iter 4700, Loss: 0.832445\n",
      "Sample 2, Recon 1, Iter 4800, Loss: 0.820440\n",
      "Sample 2, Recon 1, Iter 4900, Loss: 0.808842\n",
      "Sample 2, Recon 1, Iter 5000, Loss: 0.797641\n",
      "Sample 2, Recon 1, Iter 5100, Loss: 0.787099\n",
      "Sample 2, Recon 1, Iter 5200, Loss: 0.776646\n",
      "Sample 2, Recon 1, Iter 5300, Loss: 0.766861\n",
      "Sample 2, Recon 1, Iter 5400, Loss: 0.757184\n",
      "Sample 2, Recon 1, Iter 5500, Loss: 0.748346\n",
      "Sample 2, Recon 1, Iter 5600, Loss: 0.739555\n",
      "Sample 2, Recon 1, Iter 5700, Loss: 0.731868\n",
      "Sample 2, Recon 1, Iter 5800, Loss: 0.724107\n",
      "Sample 2, Recon 1, Iter 5900, Loss: 0.715686\n",
      "Sample 2, Recon 1, Iter 6000, Loss: 0.708916\n",
      "Sample 2, Recon 1, Iter 6100, Loss: 0.701563\n",
      "Sample 2, Recon 1, Iter 6200, Loss: 0.694640\n",
      "Sample 2, Recon 1, Iter 6300, Loss: 0.687867\n",
      "Sample 2, Recon 1, Iter 6400, Loss: 0.681518\n",
      "Sample 2, Recon 1, Iter 6500, Loss: 0.675321\n",
      "Sample 2, Recon 1, Iter 6600, Loss: 0.669755\n",
      "Sample 2, Recon 1, Iter 6700, Loss: 0.663608\n",
      "Sample 2, Recon 1, Iter 6800, Loss: 0.658871\n",
      "Sample 2, Recon 1, Iter 6900, Loss: 0.652592\n",
      "Sample 2, Recon 1, Iter 7000, Loss: 0.647998\n",
      "Sample 2, Recon 1, Iter 7100, Loss: 0.641993\n",
      "Sample 2, Recon 1, Iter 7200, Loss: 0.637063\n",
      "Sample 2, Recon 1, Iter 7300, Loss: 0.632621\n",
      "Sample 2, Recon 1, Iter 7400, Loss: 0.627358\n",
      "Sample 2, Recon 1, Iter 7500, Loss: 0.622961\n",
      "Sample 2, Recon 1, Iter 7600, Loss: 0.619124\n",
      "Sample 2, Recon 1, Iter 7700, Loss: 0.614191\n",
      "Sample 2, Recon 1, Iter 7800, Loss: 0.609994\n",
      "Sample 2, Recon 1, Iter 7900, Loss: 0.606620\n",
      "Sample 2, Recon 1, Iter 8000, Loss: 0.604172\n",
      "Sample 2, Recon 1, Iter 8100, Loss: 0.599552\n",
      "Sample 2, Recon 1, Iter 8200, Loss: 0.594633\n",
      "Sample 2, Recon 1, Iter 8300, Loss: 0.590914\n",
      "Sample 2, Recon 1, Iter 8400, Loss: 0.588621\n",
      "Sample 2, Recon 1, Iter 8500, Loss: 0.583502\n",
      "Sample 2, Recon 1, Iter 8600, Loss: 0.580235\n",
      "Sample 2, Recon 1, Iter 8700, Loss: 0.577017\n",
      "Sample 2, Recon 1, Iter 8800, Loss: 0.574214\n",
      "Sample 2, Recon 1, Iter 8900, Loss: 0.572517\n",
      "Sample 2, Recon 1, Iter 9000, Loss: 0.568075\n",
      "Sample 2, Recon 1, Iter 9100, Loss: 0.564469\n",
      "Sample 2, Recon 1, Iter 9200, Loss: 0.561989\n",
      "Sample 2, Recon 1, Iter 9300, Loss: 0.558569\n",
      "Sample 2, Recon 1, Iter 9400, Loss: 0.556093\n",
      "Sample 2, Recon 1, Iter 9500, Loss: 0.554808\n",
      "Sample 2, Recon 1, Iter 9600, Loss: 0.550621\n",
      "Sample 2, Recon 1, Iter 9700, Loss: 0.548112\n",
      "Sample 2, Recon 1, Iter 9800, Loss: 0.545088\n",
      "Sample 2, Recon 1, Iter 9900, Loss: 0.542572\n",
      "Sample 2, Recon 2, Iter 0, Loss: 22.512566\n",
      "Sample 2, Recon 2, Iter 100, Loss: 4.756055\n",
      "Sample 2, Recon 2, Iter 200, Loss: 3.789238\n",
      "Sample 2, Recon 2, Iter 300, Loss: 3.403713\n",
      "Sample 2, Recon 2, Iter 400, Loss: 3.122737\n",
      "Sample 2, Recon 2, Iter 500, Loss: 2.943550\n",
      "Sample 2, Recon 2, Iter 600, Loss: 2.818507\n",
      "Sample 2, Recon 2, Iter 700, Loss: 2.714729\n",
      "Sample 2, Recon 2, Iter 800, Loss: 2.619310\n",
      "Sample 2, Recon 2, Iter 900, Loss: 2.526348\n",
      "Sample 2, Recon 2, Iter 1000, Loss: 2.432518\n",
      "Sample 2, Recon 2, Iter 1100, Loss: 2.338636\n",
      "Sample 2, Recon 2, Iter 1200, Loss: 2.247216\n",
      "Sample 2, Recon 2, Iter 1300, Loss: 2.161458\n",
      "Sample 2, Recon 2, Iter 1400, Loss: 2.083262\n",
      "Sample 2, Recon 2, Iter 1500, Loss: 2.012015\n",
      "Sample 2, Recon 2, Iter 1600, Loss: 1.946337\n",
      "Sample 2, Recon 2, Iter 1700, Loss: 1.884640\n",
      "Sample 2, Recon 2, Iter 1800, Loss: 1.826348\n",
      "Sample 2, Recon 2, Iter 1900, Loss: 1.770760\n",
      "Sample 2, Recon 2, Iter 2000, Loss: 1.717273\n",
      "Sample 2, Recon 2, Iter 2100, Loss: 1.666030\n",
      "Sample 2, Recon 2, Iter 2200, Loss: 1.616674\n",
      "Sample 2, Recon 2, Iter 2300, Loss: 1.569340\n",
      "Sample 2, Recon 2, Iter 2400, Loss: 1.523613\n",
      "Sample 2, Recon 2, Iter 2500, Loss: 1.479431\n",
      "Sample 2, Recon 2, Iter 2600, Loss: 1.436788\n",
      "Sample 2, Recon 2, Iter 2700, Loss: 1.395365\n",
      "Sample 2, Recon 2, Iter 2800, Loss: 1.355361\n",
      "Sample 2, Recon 2, Iter 2900, Loss: 1.316600\n",
      "Sample 2, Recon 2, Iter 3000, Loss: 1.279190\n",
      "Sample 2, Recon 2, Iter 3100, Loss: 1.243057\n",
      "Sample 2, Recon 2, Iter 3200, Loss: 1.208356\n",
      "Sample 2, Recon 2, Iter 3300, Loss: 1.174839\n",
      "Sample 2, Recon 2, Iter 3400, Loss: 1.142730\n",
      "Sample 2, Recon 2, Iter 3500, Loss: 1.112243\n",
      "Sample 2, Recon 2, Iter 3600, Loss: 1.083664\n",
      "Sample 2, Recon 2, Iter 3700, Loss: 1.056691\n",
      "Sample 2, Recon 2, Iter 3800, Loss: 1.031266\n",
      "Sample 2, Recon 2, Iter 3900, Loss: 1.007053\n",
      "Sample 2, Recon 2, Iter 4000, Loss: 0.984297\n",
      "Sample 2, Recon 2, Iter 4100, Loss: 0.962943\n",
      "Sample 2, Recon 2, Iter 4200, Loss: 0.942720\n",
      "Sample 2, Recon 2, Iter 4300, Loss: 0.923852\n",
      "Sample 2, Recon 2, Iter 4400, Loss: 0.905826\n",
      "Sample 2, Recon 2, Iter 4500, Loss: 0.888744\n",
      "Sample 2, Recon 2, Iter 4600, Loss: 0.872223\n",
      "Sample 2, Recon 2, Iter 4700, Loss: 0.856663\n",
      "Sample 2, Recon 2, Iter 4800, Loss: 0.841915\n",
      "Sample 2, Recon 2, Iter 4900, Loss: 0.828681\n",
      "Sample 2, Recon 2, Iter 5000, Loss: 0.814945\n",
      "Sample 2, Recon 2, Iter 5100, Loss: 0.802470\n",
      "Sample 2, Recon 2, Iter 5200, Loss: 0.791741\n",
      "Sample 2, Recon 2, Iter 5300, Loss: 0.779845\n",
      "Sample 2, Recon 2, Iter 5400, Loss: 0.768727\n",
      "Sample 2, Recon 2, Iter 5500, Loss: 0.759134\n",
      "Sample 2, Recon 2, Iter 5600, Loss: 0.748347\n",
      "Sample 2, Recon 2, Iter 5700, Loss: 0.738784\n",
      "Sample 2, Recon 2, Iter 5800, Loss: 0.729757\n",
      "Sample 2, Recon 2, Iter 5900, Loss: 0.722283\n",
      "Sample 2, Recon 2, Iter 6000, Loss: 0.714016\n",
      "Sample 2, Recon 2, Iter 6100, Loss: 0.705520\n",
      "Sample 2, Recon 2, Iter 6200, Loss: 0.697943\n",
      "Sample 2, Recon 2, Iter 6300, Loss: 0.690822\n",
      "Sample 2, Recon 2, Iter 6400, Loss: 0.684093\n",
      "Sample 2, Recon 2, Iter 6500, Loss: 0.677376\n",
      "Sample 2, Recon 2, Iter 6600, Loss: 0.670703\n",
      "Sample 2, Recon 2, Iter 6700, Loss: 0.664342\n",
      "Sample 2, Recon 2, Iter 6800, Loss: 0.658409\n",
      "Sample 2, Recon 2, Iter 6900, Loss: 0.652974\n",
      "Sample 2, Recon 2, Iter 7000, Loss: 0.647176\n",
      "Sample 2, Recon 2, Iter 7100, Loss: 0.641581\n",
      "Sample 2, Recon 2, Iter 7200, Loss: 0.636318\n",
      "Sample 2, Recon 2, Iter 7300, Loss: 0.631655\n",
      "Sample 2, Recon 2, Iter 7400, Loss: 0.627137\n",
      "Sample 2, Recon 2, Iter 7500, Loss: 0.621581\n",
      "Sample 2, Recon 2, Iter 7600, Loss: 0.616816\n",
      "Sample 2, Recon 2, Iter 7700, Loss: 0.613135\n",
      "Sample 2, Recon 2, Iter 7800, Loss: 0.609293\n",
      "Sample 2, Recon 2, Iter 7900, Loss: 0.603422\n",
      "Sample 2, Recon 2, Iter 8000, Loss: 0.599608\n",
      "Sample 2, Recon 2, Iter 8100, Loss: 0.595061\n",
      "Sample 2, Recon 2, Iter 8200, Loss: 0.591103\n",
      "Sample 2, Recon 2, Iter 8300, Loss: 0.587533\n",
      "Sample 2, Recon 2, Iter 8400, Loss: 0.584315\n",
      "Sample 2, Recon 2, Iter 8500, Loss: 0.580706\n",
      "Sample 2, Recon 2, Iter 8600, Loss: 0.576466\n",
      "Sample 2, Recon 2, Iter 8700, Loss: 0.573172\n",
      "Sample 2, Recon 2, Iter 8800, Loss: 0.570584\n",
      "Sample 2, Recon 2, Iter 8900, Loss: 0.567989\n",
      "Sample 2, Recon 2, Iter 9000, Loss: 0.563272\n",
      "Sample 2, Recon 2, Iter 9100, Loss: 0.560217\n",
      "Sample 2, Recon 2, Iter 9200, Loss: 0.557189\n",
      "Sample 2, Recon 2, Iter 9300, Loss: 0.556509\n",
      "Sample 2, Recon 2, Iter 9400, Loss: 0.551558\n",
      "Sample 2, Recon 2, Iter 9500, Loss: 0.548550\n",
      "Sample 2, Recon 2, Iter 9600, Loss: 0.545933\n",
      "Sample 2, Recon 2, Iter 9700, Loss: 0.543272\n",
      "Sample 2, Recon 2, Iter 9800, Loss: 0.542815\n",
      "Sample 2, Recon 2, Iter 9900, Loss: 0.538955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:  30%|███       | 3/10 [2:01:28<4:43:10, 2427.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 3, Recon 0, Iter 0, Loss: 20.815178\n",
      "Sample 3, Recon 0, Iter 100, Loss: 4.727788\n",
      "Sample 3, Recon 0, Iter 200, Loss: 4.329004\n",
      "Sample 3, Recon 0, Iter 300, Loss: 4.148098\n",
      "Sample 3, Recon 0, Iter 400, Loss: 3.951496\n",
      "Sample 3, Recon 0, Iter 500, Loss: 3.790228\n",
      "Sample 3, Recon 0, Iter 600, Loss: 3.649293\n",
      "Sample 3, Recon 0, Iter 700, Loss: 3.532684\n",
      "Sample 3, Recon 0, Iter 800, Loss: 3.422399\n",
      "Sample 3, Recon 0, Iter 900, Loss: 3.309604\n",
      "Sample 3, Recon 0, Iter 1000, Loss: 3.169080\n",
      "Sample 3, Recon 0, Iter 1100, Loss: 3.023308\n",
      "Sample 3, Recon 0, Iter 1200, Loss: 2.901199\n",
      "Sample 3, Recon 0, Iter 1300, Loss: 2.777738\n",
      "Sample 3, Recon 0, Iter 1400, Loss: 2.532162\n",
      "Sample 3, Recon 0, Iter 1500, Loss: 2.416222\n",
      "Sample 3, Recon 0, Iter 1600, Loss: 2.333764\n",
      "Sample 3, Recon 0, Iter 1700, Loss: 2.259716\n",
      "Sample 3, Recon 0, Iter 1800, Loss: 2.190849\n",
      "Sample 3, Recon 0, Iter 1900, Loss: 2.125586\n",
      "Sample 3, Recon 0, Iter 2000, Loss: 2.063579\n",
      "Sample 3, Recon 0, Iter 2100, Loss: 2.005655\n",
      "Sample 3, Recon 0, Iter 2200, Loss: 1.951013\n",
      "Sample 3, Recon 0, Iter 2300, Loss: 1.900041\n",
      "Sample 3, Recon 0, Iter 2400, Loss: 1.854890\n",
      "Sample 3, Recon 0, Iter 2500, Loss: 1.814041\n",
      "Sample 3, Recon 0, Iter 2600, Loss: 1.772810\n",
      "Sample 3, Recon 0, Iter 2700, Loss: 1.736857\n",
      "Sample 3, Recon 0, Iter 2800, Loss: 1.703184\n",
      "Sample 3, Recon 0, Iter 2900, Loss: 1.672023\n",
      "Sample 3, Recon 0, Iter 3000, Loss: 1.642291\n",
      "Sample 3, Recon 0, Iter 3100, Loss: 1.612582\n",
      "Sample 3, Recon 0, Iter 3200, Loss: 1.583901\n",
      "Sample 3, Recon 0, Iter 3300, Loss: 1.558637\n",
      "Sample 3, Recon 0, Iter 3400, Loss: 1.531195\n",
      "Sample 3, Recon 0, Iter 3500, Loss: 1.508460\n",
      "Sample 3, Recon 0, Iter 3600, Loss: 1.483302\n",
      "Sample 3, Recon 0, Iter 3700, Loss: 1.462910\n",
      "Sample 3, Recon 0, Iter 3800, Loss: 1.440636\n",
      "Sample 3, Recon 0, Iter 3900, Loss: 1.418949\n",
      "Sample 3, Recon 0, Iter 4000, Loss: 1.397316\n",
      "Sample 3, Recon 0, Iter 4100, Loss: 1.377608\n",
      "Sample 3, Recon 0, Iter 4200, Loss: 1.357840\n",
      "Sample 3, Recon 0, Iter 4300, Loss: 1.339950\n",
      "Sample 3, Recon 0, Iter 4400, Loss: 1.318780\n",
      "Sample 3, Recon 0, Iter 4500, Loss: 1.299268\n",
      "Sample 3, Recon 0, Iter 4600, Loss: 1.278035\n",
      "Sample 3, Recon 0, Iter 4700, Loss: 1.257326\n",
      "Sample 3, Recon 0, Iter 4800, Loss: 1.227364\n",
      "Sample 3, Recon 0, Iter 4900, Loss: 1.181867\n",
      "Sample 3, Recon 0, Iter 5000, Loss: 1.163130\n",
      "Sample 3, Recon 0, Iter 5100, Loss: 1.147386\n",
      "Sample 3, Recon 0, Iter 5200, Loss: 1.134782\n",
      "Sample 3, Recon 0, Iter 5300, Loss: 1.122716\n",
      "Sample 3, Recon 0, Iter 5400, Loss: 1.111501\n",
      "Sample 3, Recon 0, Iter 5500, Loss: 1.101655\n",
      "Sample 3, Recon 0, Iter 5600, Loss: 1.091394\n",
      "Sample 3, Recon 0, Iter 5700, Loss: 1.082239\n",
      "Sample 3, Recon 0, Iter 5800, Loss: 1.072406\n",
      "Sample 3, Recon 0, Iter 5900, Loss: 1.063367\n",
      "Sample 3, Recon 0, Iter 6000, Loss: 1.056852\n",
      "Sample 3, Recon 0, Iter 6100, Loss: 1.046879\n",
      "Sample 3, Recon 0, Iter 6200, Loss: 1.039770\n",
      "Sample 3, Recon 0, Iter 6300, Loss: 1.030747\n",
      "Sample 3, Recon 0, Iter 6400, Loss: 1.025109\n",
      "Sample 3, Recon 0, Iter 6500, Loss: 1.019206\n",
      "Sample 3, Recon 0, Iter 6600, Loss: 1.013407\n",
      "Sample 3, Recon 0, Iter 6700, Loss: 1.004086\n",
      "Sample 3, Recon 0, Iter 6800, Loss: 0.999926\n",
      "Sample 3, Recon 0, Iter 6900, Loss: 0.992513\n",
      "Sample 3, Recon 0, Iter 7000, Loss: 0.985371\n",
      "Sample 3, Recon 0, Iter 7100, Loss: 0.982291\n",
      "Sample 3, Recon 0, Iter 7200, Loss: 0.977125\n",
      "Sample 3, Recon 0, Iter 7300, Loss: 0.969814\n",
      "Sample 3, Recon 0, Iter 7400, Loss: 0.963508\n",
      "Sample 3, Recon 0, Iter 7500, Loss: 0.958432\n",
      "Sample 3, Recon 0, Iter 7600, Loss: 0.953052\n",
      "Sample 3, Recon 0, Iter 7700, Loss: 0.948629\n",
      "Sample 3, Recon 0, Iter 7800, Loss: 0.943016\n",
      "Sample 3, Recon 0, Iter 7900, Loss: 0.938514\n",
      "Sample 3, Recon 0, Iter 8000, Loss: 0.937100\n",
      "Sample 3, Recon 0, Iter 8100, Loss: 0.934663\n",
      "Sample 3, Recon 0, Iter 8200, Loss: 0.927207\n",
      "Sample 3, Recon 0, Iter 8300, Loss: 0.925026\n",
      "Sample 3, Recon 0, Iter 8400, Loss: 0.921455\n",
      "Sample 3, Recon 0, Iter 8500, Loss: 0.914484\n",
      "Sample 3, Recon 0, Iter 8600, Loss: 0.918453\n",
      "Sample 3, Recon 0, Iter 8700, Loss: 0.905991\n",
      "Sample 3, Recon 0, Iter 8800, Loss: 0.904611\n",
      "Sample 3, Recon 0, Iter 8900, Loss: 0.898170\n",
      "Sample 3, Recon 0, Iter 9000, Loss: 0.898201\n",
      "Sample 3, Recon 0, Iter 9100, Loss: 0.891165\n",
      "Sample 3, Recon 0, Iter 9200, Loss: 0.887387\n",
      "Sample 3, Recon 0, Iter 9300, Loss: 0.884493\n",
      "Sample 3, Recon 0, Iter 9400, Loss: 0.880631\n",
      "Sample 3, Recon 0, Iter 9500, Loss: 0.878357\n",
      "Sample 3, Recon 0, Iter 9600, Loss: 0.873922\n",
      "Sample 3, Recon 0, Iter 9700, Loss: 0.871533\n",
      "Sample 3, Recon 0, Iter 9800, Loss: 0.867415\n",
      "Sample 3, Recon 0, Iter 9900, Loss: 0.862293\n",
      "Sample 3, Recon 1, Iter 0, Loss: 18.368433\n",
      "Sample 3, Recon 1, Iter 100, Loss: 4.803389\n",
      "Sample 3, Recon 1, Iter 200, Loss: 4.026853\n",
      "Sample 3, Recon 1, Iter 300, Loss: 3.630507\n",
      "Sample 3, Recon 1, Iter 400, Loss: 3.399830\n",
      "Sample 3, Recon 1, Iter 500, Loss: 3.246711\n",
      "Sample 3, Recon 1, Iter 600, Loss: 3.124180\n",
      "Sample 3, Recon 1, Iter 700, Loss: 3.010574\n",
      "Sample 3, Recon 1, Iter 800, Loss: 2.909711\n",
      "Sample 3, Recon 1, Iter 900, Loss: 2.813688\n",
      "Sample 3, Recon 1, Iter 1000, Loss: 2.718628\n",
      "Sample 3, Recon 1, Iter 1100, Loss: 2.623236\n",
      "Sample 3, Recon 1, Iter 1200, Loss: 2.526317\n",
      "Sample 3, Recon 1, Iter 1300, Loss: 2.395221\n",
      "Sample 3, Recon 1, Iter 1400, Loss: 2.308061\n",
      "Sample 3, Recon 1, Iter 1500, Loss: 2.227682\n",
      "Sample 3, Recon 1, Iter 1600, Loss: 2.154112\n",
      "Sample 3, Recon 1, Iter 1700, Loss: 2.086926\n",
      "Sample 3, Recon 1, Iter 1800, Loss: 2.025011\n",
      "Sample 3, Recon 1, Iter 1900, Loss: 1.967796\n",
      "Sample 3, Recon 1, Iter 2000, Loss: 1.913685\n",
      "Sample 3, Recon 1, Iter 2100, Loss: 1.863149\n",
      "Sample 3, Recon 1, Iter 2200, Loss: 1.816233\n",
      "Sample 3, Recon 1, Iter 2300, Loss: 1.769981\n",
      "Sample 3, Recon 1, Iter 2400, Loss: 1.726645\n",
      "Sample 3, Recon 1, Iter 2500, Loss: 1.686006\n",
      "Sample 3, Recon 1, Iter 2600, Loss: 1.646618\n",
      "Sample 3, Recon 1, Iter 2700, Loss: 1.609326\n",
      "Sample 3, Recon 1, Iter 2800, Loss: 1.573437\n",
      "Sample 3, Recon 1, Iter 2900, Loss: 1.539726\n",
      "Sample 3, Recon 1, Iter 3000, Loss: 1.506175\n",
      "Sample 3, Recon 1, Iter 3100, Loss: 1.475585\n",
      "Sample 3, Recon 1, Iter 3200, Loss: 1.448755\n",
      "Sample 3, Recon 1, Iter 3300, Loss: 1.419344\n",
      "Sample 3, Recon 1, Iter 3400, Loss: 1.394681\n",
      "Sample 3, Recon 1, Iter 3500, Loss: 1.371427\n",
      "Sample 3, Recon 1, Iter 3600, Loss: 1.349362\n",
      "Sample 3, Recon 1, Iter 3700, Loss: 1.327336\n",
      "Sample 3, Recon 1, Iter 3800, Loss: 1.308807\n",
      "Sample 3, Recon 1, Iter 3900, Loss: 1.287659\n",
      "Sample 3, Recon 1, Iter 4000, Loss: 1.268723\n",
      "Sample 3, Recon 1, Iter 4100, Loss: 1.251366\n",
      "Sample 3, Recon 1, Iter 4200, Loss: 1.235034\n",
      "Sample 3, Recon 1, Iter 4300, Loss: 1.219007\n",
      "Sample 3, Recon 1, Iter 4400, Loss: 1.205290\n",
      "Sample 3, Recon 1, Iter 4500, Loss: 1.189493\n",
      "Sample 3, Recon 1, Iter 4600, Loss: 1.174421\n",
      "Sample 3, Recon 1, Iter 4700, Loss: 1.161246\n",
      "Sample 3, Recon 1, Iter 4800, Loss: 1.147083\n",
      "Sample 3, Recon 1, Iter 4900, Loss: 1.134435\n",
      "Sample 3, Recon 1, Iter 5000, Loss: 1.121822\n",
      "Sample 3, Recon 1, Iter 5100, Loss: 1.112048\n",
      "Sample 3, Recon 1, Iter 5200, Loss: 1.099268\n",
      "Sample 3, Recon 1, Iter 5300, Loss: 1.087877\n",
      "Sample 3, Recon 1, Iter 5400, Loss: 1.077475\n",
      "Sample 3, Recon 1, Iter 5500, Loss: 1.065419\n",
      "Sample 3, Recon 1, Iter 5600, Loss: 1.055522\n",
      "Sample 3, Recon 1, Iter 5700, Loss: 1.044216\n",
      "Sample 3, Recon 1, Iter 5800, Loss: 1.034626\n",
      "Sample 3, Recon 1, Iter 5900, Loss: 1.024092\n",
      "Sample 3, Recon 1, Iter 6000, Loss: 1.013492\n",
      "Sample 3, Recon 1, Iter 6100, Loss: 1.003226\n",
      "Sample 3, Recon 1, Iter 6200, Loss: 0.993272\n",
      "Sample 3, Recon 1, Iter 6300, Loss: 0.984311\n",
      "Sample 3, Recon 1, Iter 6400, Loss: 0.975402\n",
      "Sample 3, Recon 1, Iter 6500, Loss: 0.965036\n",
      "Sample 3, Recon 1, Iter 6600, Loss: 0.955681\n",
      "Sample 3, Recon 1, Iter 6700, Loss: 0.947008\n",
      "Sample 3, Recon 1, Iter 6800, Loss: 0.939006\n",
      "Sample 3, Recon 1, Iter 6900, Loss: 0.929032\n",
      "Sample 3, Recon 1, Iter 7000, Loss: 0.920009\n",
      "Sample 3, Recon 1, Iter 7100, Loss: 0.911559\n",
      "Sample 3, Recon 1, Iter 7200, Loss: 0.905279\n",
      "Sample 3, Recon 1, Iter 7300, Loss: 0.898256\n",
      "Sample 3, Recon 1, Iter 7400, Loss: 0.889181\n",
      "Sample 3, Recon 1, Iter 7500, Loss: 0.882001\n",
      "Sample 3, Recon 1, Iter 7600, Loss: 0.873959\n",
      "Sample 3, Recon 1, Iter 7700, Loss: 0.866941\n",
      "Sample 3, Recon 1, Iter 7800, Loss: 0.859813\n",
      "Sample 3, Recon 1, Iter 7900, Loss: 0.854159\n",
      "Sample 3, Recon 1, Iter 8000, Loss: 0.847064\n",
      "Sample 3, Recon 1, Iter 8100, Loss: 0.840952\n",
      "Sample 3, Recon 1, Iter 8200, Loss: 0.834526\n",
      "Sample 3, Recon 1, Iter 8300, Loss: 0.829009\n",
      "Sample 3, Recon 1, Iter 8400, Loss: 0.822687\n",
      "Sample 3, Recon 1, Iter 8500, Loss: 0.817054\n",
      "Sample 3, Recon 1, Iter 8600, Loss: 0.812183\n",
      "Sample 3, Recon 1, Iter 8700, Loss: 0.805993\n",
      "Sample 3, Recon 1, Iter 8800, Loss: 0.801512\n",
      "Sample 3, Recon 1, Iter 8900, Loss: 0.798409\n",
      "Sample 3, Recon 1, Iter 9000, Loss: 0.792676\n",
      "Sample 3, Recon 1, Iter 9100, Loss: 0.788381\n",
      "Sample 3, Recon 1, Iter 9200, Loss: 0.784407\n",
      "Sample 3, Recon 1, Iter 9300, Loss: 0.780048\n",
      "Sample 3, Recon 1, Iter 9400, Loss: 0.775915\n",
      "Sample 3, Recon 1, Iter 9500, Loss: 0.771386\n",
      "Sample 3, Recon 1, Iter 9600, Loss: 0.768418\n",
      "Sample 3, Recon 1, Iter 9700, Loss: 0.764311\n",
      "Sample 3, Recon 1, Iter 9800, Loss: 0.797465\n",
      "Sample 3, Recon 1, Iter 9900, Loss: 0.774698\n",
      "Sample 3, Recon 2, Iter 0, Loss: 18.969645\n",
      "Sample 3, Recon 2, Iter 100, Loss: 5.132897\n",
      "Sample 3, Recon 2, Iter 200, Loss: 4.480721\n",
      "Sample 3, Recon 2, Iter 300, Loss: 4.123048\n",
      "Sample 3, Recon 2, Iter 400, Loss: 3.904748\n",
      "Sample 3, Recon 2, Iter 500, Loss: 3.630014\n",
      "Sample 3, Recon 2, Iter 600, Loss: 3.453624\n",
      "Sample 3, Recon 2, Iter 700, Loss: 3.313581\n",
      "Sample 3, Recon 2, Iter 800, Loss: 3.188052\n",
      "Sample 3, Recon 2, Iter 900, Loss: 3.067496\n",
      "Sample 3, Recon 2, Iter 1000, Loss: 2.949711\n",
      "Sample 3, Recon 2, Iter 1100, Loss: 2.835884\n",
      "Sample 3, Recon 2, Iter 1200, Loss: 2.722649\n",
      "Sample 3, Recon 2, Iter 1300, Loss: 2.606227\n",
      "Sample 3, Recon 2, Iter 1400, Loss: 2.387612\n",
      "Sample 3, Recon 2, Iter 1500, Loss: 2.263192\n",
      "Sample 3, Recon 2, Iter 1600, Loss: 2.170058\n",
      "Sample 3, Recon 2, Iter 1700, Loss: 2.093100\n",
      "Sample 3, Recon 2, Iter 1800, Loss: 2.024092\n",
      "Sample 3, Recon 2, Iter 1900, Loss: 1.960999\n",
      "Sample 3, Recon 2, Iter 2000, Loss: 1.902909\n",
      "Sample 3, Recon 2, Iter 2100, Loss: 1.849010\n",
      "Sample 3, Recon 2, Iter 2200, Loss: 1.798874\n",
      "Sample 3, Recon 2, Iter 2300, Loss: 1.751338\n",
      "Sample 3, Recon 2, Iter 2400, Loss: 1.706625\n",
      "Sample 3, Recon 2, Iter 2500, Loss: 1.663859\n",
      "Sample 3, Recon 2, Iter 2600, Loss: 1.623272\n",
      "Sample 3, Recon 2, Iter 2700, Loss: 1.584327\n",
      "Sample 3, Recon 2, Iter 2800, Loss: 1.546552\n",
      "Sample 3, Recon 2, Iter 2900, Loss: 1.511225\n",
      "Sample 3, Recon 2, Iter 3000, Loss: 1.476781\n",
      "Sample 3, Recon 2, Iter 3100, Loss: 1.444268\n",
      "Sample 3, Recon 2, Iter 3200, Loss: 1.413528\n",
      "Sample 3, Recon 2, Iter 3300, Loss: 1.384765\n",
      "Sample 3, Recon 2, Iter 3400, Loss: 1.356192\n",
      "Sample 3, Recon 2, Iter 3500, Loss: 1.330495\n",
      "Sample 3, Recon 2, Iter 3600, Loss: 1.304902\n",
      "Sample 3, Recon 2, Iter 3700, Loss: 1.281208\n",
      "Sample 3, Recon 2, Iter 3800, Loss: 1.258213\n",
      "Sample 3, Recon 2, Iter 3900, Loss: 1.236709\n",
      "Sample 3, Recon 2, Iter 4000, Loss: 1.215552\n",
      "Sample 3, Recon 2, Iter 4100, Loss: 1.195321\n",
      "Sample 3, Recon 2, Iter 4200, Loss: 1.176246\n",
      "Sample 3, Recon 2, Iter 4300, Loss: 1.157680\n",
      "Sample 3, Recon 2, Iter 4400, Loss: 1.140171\n",
      "Sample 3, Recon 2, Iter 4500, Loss: 1.122734\n",
      "Sample 3, Recon 2, Iter 4600, Loss: 1.106445\n",
      "Sample 3, Recon 2, Iter 4700, Loss: 1.090853\n",
      "Sample 3, Recon 2, Iter 4800, Loss: 1.075917\n",
      "Sample 3, Recon 2, Iter 4900, Loss: 1.061182\n",
      "Sample 3, Recon 2, Iter 5000, Loss: 1.046390\n",
      "Sample 3, Recon 2, Iter 5100, Loss: 1.032904\n",
      "Sample 3, Recon 2, Iter 5200, Loss: 1.021144\n",
      "Sample 3, Recon 2, Iter 5300, Loss: 1.006069\n",
      "Sample 3, Recon 2, Iter 5400, Loss: 0.994556\n",
      "Sample 3, Recon 2, Iter 5500, Loss: 0.981860\n",
      "Sample 3, Recon 2, Iter 5600, Loss: 0.969935\n",
      "Sample 3, Recon 2, Iter 5700, Loss: 0.958881\n",
      "Sample 3, Recon 2, Iter 5800, Loss: 0.947991\n",
      "Sample 3, Recon 2, Iter 5900, Loss: 0.939561\n",
      "Sample 3, Recon 2, Iter 6000, Loss: 0.928287\n",
      "Sample 3, Recon 2, Iter 6100, Loss: 0.918528\n",
      "Sample 3, Recon 2, Iter 6200, Loss: 0.909256\n",
      "Sample 3, Recon 2, Iter 6300, Loss: 0.900226\n",
      "Sample 3, Recon 2, Iter 6400, Loss: 0.891508\n",
      "Sample 3, Recon 2, Iter 6500, Loss: 0.882820\n",
      "Sample 3, Recon 2, Iter 6600, Loss: 0.874485\n",
      "Sample 3, Recon 2, Iter 6700, Loss: 0.867213\n",
      "Sample 3, Recon 2, Iter 6800, Loss: 0.861618\n",
      "Sample 3, Recon 2, Iter 6900, Loss: 0.851498\n",
      "Sample 3, Recon 2, Iter 7000, Loss: 0.844438\n",
      "Sample 3, Recon 2, Iter 7100, Loss: 0.838385\n",
      "Sample 3, Recon 2, Iter 7200, Loss: 0.831837\n",
      "Sample 3, Recon 2, Iter 7300, Loss: 0.824291\n",
      "Sample 3, Recon 2, Iter 7400, Loss: 0.818206\n",
      "Sample 3, Recon 2, Iter 7500, Loss: 0.812106\n",
      "Sample 3, Recon 2, Iter 7600, Loss: 0.806094\n",
      "Sample 3, Recon 2, Iter 7700, Loss: 0.800044\n",
      "Sample 3, Recon 2, Iter 7800, Loss: 0.795313\n",
      "Sample 3, Recon 2, Iter 7900, Loss: 0.789037\n",
      "Sample 3, Recon 2, Iter 8000, Loss: 0.783371\n",
      "Sample 3, Recon 2, Iter 8100, Loss: 0.777670\n",
      "Sample 3, Recon 2, Iter 8200, Loss: 0.772610\n",
      "Sample 3, Recon 2, Iter 8300, Loss: 0.768972\n",
      "Sample 3, Recon 2, Iter 8400, Loss: 0.763415\n",
      "Sample 3, Recon 2, Iter 8500, Loss: 0.758602\n",
      "Sample 3, Recon 2, Iter 8600, Loss: 0.753760\n",
      "Sample 3, Recon 2, Iter 8700, Loss: 0.750236\n",
      "Sample 3, Recon 2, Iter 8800, Loss: 0.745530\n",
      "Sample 3, Recon 2, Iter 8900, Loss: 0.741693\n",
      "Sample 3, Recon 2, Iter 9000, Loss: 0.736954\n",
      "Sample 3, Recon 2, Iter 9100, Loss: 0.733397\n",
      "Sample 3, Recon 2, Iter 9200, Loss: 0.729948\n",
      "Sample 3, Recon 2, Iter 9300, Loss: 0.724960\n",
      "Sample 3, Recon 2, Iter 9400, Loss: 0.721483\n",
      "Sample 3, Recon 2, Iter 9500, Loss: 0.716364\n",
      "Sample 3, Recon 2, Iter 9600, Loss: 0.716490\n",
      "Sample 3, Recon 2, Iter 9700, Loss: 0.709640\n",
      "Sample 3, Recon 2, Iter 9800, Loss: 0.705260\n",
      "Sample 3, Recon 2, Iter 9900, Loss: 0.701801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:  40%|████      | 4/10 [2:41:52<4:02:36, 2426.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 4, Recon 0, Iter 0, Loss: 22.247620\n",
      "Sample 4, Recon 0, Iter 100, Loss: 4.539945\n",
      "Sample 4, Recon 0, Iter 200, Loss: 3.781294\n",
      "Sample 4, Recon 0, Iter 300, Loss: 3.455182\n",
      "Sample 4, Recon 0, Iter 400, Loss: 3.229456\n",
      "Sample 4, Recon 0, Iter 500, Loss: 3.075818\n",
      "Sample 4, Recon 0, Iter 600, Loss: 2.948146\n",
      "Sample 4, Recon 0, Iter 700, Loss: 2.839006\n",
      "Sample 4, Recon 0, Iter 800, Loss: 2.743369\n",
      "Sample 4, Recon 0, Iter 900, Loss: 2.655700\n",
      "Sample 4, Recon 0, Iter 1000, Loss: 2.571412\n",
      "Sample 4, Recon 0, Iter 1100, Loss: 2.487682\n",
      "Sample 4, Recon 0, Iter 1200, Loss: 2.403078\n",
      "Sample 4, Recon 0, Iter 1300, Loss: 2.318426\n",
      "Sample 4, Recon 0, Iter 1400, Loss: 2.235150\n",
      "Sample 4, Recon 0, Iter 1500, Loss: 2.153696\n",
      "Sample 4, Recon 0, Iter 1600, Loss: 2.074239\n",
      "Sample 4, Recon 0, Iter 1700, Loss: 1.997745\n",
      "Sample 4, Recon 0, Iter 1800, Loss: 1.925679\n",
      "Sample 4, Recon 0, Iter 1900, Loss: 1.858283\n",
      "Sample 4, Recon 0, Iter 2000, Loss: 1.795222\n",
      "Sample 4, Recon 0, Iter 2100, Loss: 1.735891\n",
      "Sample 4, Recon 0, Iter 2200, Loss: 1.679931\n",
      "Sample 4, Recon 0, Iter 2300, Loss: 1.626678\n",
      "Sample 4, Recon 0, Iter 2400, Loss: 1.575620\n",
      "Sample 4, Recon 0, Iter 2500, Loss: 1.526711\n",
      "Sample 4, Recon 0, Iter 2600, Loss: 1.479417\n",
      "Sample 4, Recon 0, Iter 2700, Loss: 1.434001\n",
      "Sample 4, Recon 0, Iter 2800, Loss: 1.390301\n",
      "Sample 4, Recon 0, Iter 2900, Loss: 1.348918\n",
      "Sample 4, Recon 0, Iter 3000, Loss: 1.309070\n",
      "Sample 4, Recon 0, Iter 3100, Loss: 1.270805\n",
      "Sample 4, Recon 0, Iter 3200, Loss: 1.234159\n",
      "Sample 4, Recon 0, Iter 3300, Loss: 1.198681\n",
      "Sample 4, Recon 0, Iter 3400, Loss: 1.165246\n",
      "Sample 4, Recon 0, Iter 3500, Loss: 1.134696\n",
      "Sample 4, Recon 0, Iter 3600, Loss: 1.104582\n",
      "Sample 4, Recon 0, Iter 3700, Loss: 1.077653\n",
      "Sample 4, Recon 0, Iter 3800, Loss: 1.050210\n",
      "Sample 4, Recon 0, Iter 3900, Loss: 1.025206\n",
      "Sample 4, Recon 0, Iter 4000, Loss: 1.001443\n",
      "Sample 4, Recon 0, Iter 4100, Loss: 0.978679\n",
      "Sample 4, Recon 0, Iter 4200, Loss: 0.956823\n",
      "Sample 4, Recon 0, Iter 4300, Loss: 0.937173\n",
      "Sample 4, Recon 0, Iter 4400, Loss: 0.916889\n",
      "Sample 4, Recon 0, Iter 4500, Loss: 0.898264\n",
      "Sample 4, Recon 0, Iter 4600, Loss: 0.880450\n",
      "Sample 4, Recon 0, Iter 4700, Loss: 0.863309\n",
      "Sample 4, Recon 0, Iter 4800, Loss: 0.847008\n",
      "Sample 4, Recon 0, Iter 4900, Loss: 0.832290\n",
      "Sample 4, Recon 0, Iter 5000, Loss: 0.817281\n",
      "Sample 4, Recon 0, Iter 5100, Loss: 0.803737\n",
      "Sample 4, Recon 0, Iter 5200, Loss: 0.790161\n",
      "Sample 4, Recon 0, Iter 5300, Loss: 0.777914\n",
      "Sample 4, Recon 0, Iter 5400, Loss: 0.765114\n",
      "Sample 4, Recon 0, Iter 5500, Loss: 0.753085\n",
      "Sample 4, Recon 0, Iter 5600, Loss: 0.742255\n",
      "Sample 4, Recon 0, Iter 5700, Loss: 0.730847\n",
      "Sample 4, Recon 0, Iter 5800, Loss: 0.720360\n",
      "Sample 4, Recon 0, Iter 5900, Loss: 0.710625\n",
      "Sample 4, Recon 0, Iter 6000, Loss: 0.701563\n",
      "Sample 4, Recon 0, Iter 6100, Loss: 0.692715\n",
      "Sample 4, Recon 0, Iter 6200, Loss: 0.684699\n",
      "Sample 4, Recon 0, Iter 6300, Loss: 0.675901\n",
      "Sample 4, Recon 0, Iter 6400, Loss: 0.667713\n",
      "Sample 4, Recon 0, Iter 6500, Loss: 0.659798\n",
      "Sample 4, Recon 0, Iter 6600, Loss: 0.652025\n",
      "Sample 4, Recon 0, Iter 6700, Loss: 0.645270\n",
      "Sample 4, Recon 0, Iter 6800, Loss: 0.639248\n",
      "Sample 4, Recon 0, Iter 6900, Loss: 0.635615\n",
      "Sample 4, Recon 0, Iter 7000, Loss: 0.624286\n",
      "Sample 4, Recon 0, Iter 7100, Loss: 0.618070\n",
      "Sample 4, Recon 0, Iter 7200, Loss: 0.614638\n",
      "Sample 4, Recon 0, Iter 7300, Loss: 0.607803\n",
      "Sample 4, Recon 0, Iter 7400, Loss: 0.601348\n",
      "Sample 4, Recon 0, Iter 7500, Loss: 0.596106\n",
      "Sample 4, Recon 0, Iter 7600, Loss: 0.589695\n",
      "Sample 4, Recon 0, Iter 7700, Loss: 0.584398\n",
      "Sample 4, Recon 0, Iter 7800, Loss: 0.579312\n",
      "Sample 4, Recon 0, Iter 7900, Loss: 0.575479\n",
      "Sample 4, Recon 0, Iter 8000, Loss: 0.569438\n",
      "Sample 4, Recon 0, Iter 8100, Loss: 0.565083\n",
      "Sample 4, Recon 0, Iter 8200, Loss: 0.561798\n",
      "Sample 4, Recon 0, Iter 8300, Loss: 0.555808\n",
      "Sample 4, Recon 0, Iter 8400, Loss: 0.551494\n",
      "Sample 4, Recon 0, Iter 8500, Loss: 0.547170\n",
      "Sample 4, Recon 0, Iter 8600, Loss: 0.543797\n",
      "Sample 4, Recon 0, Iter 8700, Loss: 0.539814\n",
      "Sample 4, Recon 0, Iter 8800, Loss: 0.538561\n",
      "Sample 4, Recon 0, Iter 8900, Loss: 0.532541\n",
      "Sample 4, Recon 0, Iter 9000, Loss: 0.528691\n",
      "Sample 4, Recon 0, Iter 9100, Loss: 0.527253\n",
      "Sample 4, Recon 0, Iter 9200, Loss: 0.522244\n",
      "Sample 4, Recon 0, Iter 9300, Loss: 0.518651\n",
      "Sample 4, Recon 0, Iter 9400, Loss: 0.515948\n",
      "Sample 4, Recon 0, Iter 9500, Loss: 0.512152\n",
      "Sample 4, Recon 0, Iter 9600, Loss: 0.508693\n",
      "Sample 4, Recon 0, Iter 9700, Loss: 0.505681\n",
      "Sample 4, Recon 0, Iter 9800, Loss: 0.502477\n",
      "Sample 4, Recon 0, Iter 9900, Loss: 0.501196\n",
      "Sample 4, Recon 1, Iter 0, Loss: 19.628941\n",
      "Sample 4, Recon 1, Iter 100, Loss: 4.157316\n",
      "Sample 4, Recon 1, Iter 200, Loss: 3.654552\n",
      "Sample 4, Recon 1, Iter 300, Loss: 3.338815\n",
      "Sample 4, Recon 1, Iter 400, Loss: 3.149525\n",
      "Sample 4, Recon 1, Iter 500, Loss: 3.017077\n",
      "Sample 4, Recon 1, Iter 600, Loss: 2.904424\n",
      "Sample 4, Recon 1, Iter 700, Loss: 2.799248\n",
      "Sample 4, Recon 1, Iter 800, Loss: 2.697771\n",
      "Sample 4, Recon 1, Iter 900, Loss: 2.597825\n",
      "Sample 4, Recon 1, Iter 1000, Loss: 2.497986\n",
      "Sample 4, Recon 1, Iter 1100, Loss: 2.398120\n",
      "Sample 4, Recon 1, Iter 1200, Loss: 2.299819\n",
      "Sample 4, Recon 1, Iter 1300, Loss: 2.205162\n",
      "Sample 4, Recon 1, Iter 1400, Loss: 2.116734\n",
      "Sample 4, Recon 1, Iter 1500, Loss: 2.034767\n",
      "Sample 4, Recon 1, Iter 1600, Loss: 1.958627\n",
      "Sample 4, Recon 1, Iter 1700, Loss: 1.887376\n",
      "Sample 4, Recon 1, Iter 1800, Loss: 1.820177\n",
      "Sample 4, Recon 1, Iter 1900, Loss: 1.756934\n",
      "Sample 4, Recon 1, Iter 2000, Loss: 1.696844\n",
      "Sample 4, Recon 1, Iter 2100, Loss: 1.640081\n",
      "Sample 4, Recon 1, Iter 2200, Loss: 1.585972\n",
      "Sample 4, Recon 1, Iter 2300, Loss: 1.534448\n",
      "Sample 4, Recon 1, Iter 2400, Loss: 1.485240\n",
      "Sample 4, Recon 1, Iter 2500, Loss: 1.438384\n",
      "Sample 4, Recon 1, Iter 2600, Loss: 1.393574\n",
      "Sample 4, Recon 1, Iter 2700, Loss: 1.350773\n",
      "Sample 4, Recon 1, Iter 2800, Loss: 1.309911\n",
      "Sample 4, Recon 1, Iter 2900, Loss: 1.270977\n",
      "Sample 4, Recon 1, Iter 3000, Loss: 1.234161\n",
      "Sample 4, Recon 1, Iter 3100, Loss: 1.199425\n",
      "Sample 4, Recon 1, Iter 3200, Loss: 1.166617\n",
      "Sample 4, Recon 1, Iter 3300, Loss: 1.135392\n",
      "Sample 4, Recon 1, Iter 3400, Loss: 1.106209\n",
      "Sample 4, Recon 1, Iter 3500, Loss: 1.078489\n",
      "Sample 4, Recon 1, Iter 3600, Loss: 1.052387\n",
      "Sample 4, Recon 1, Iter 3700, Loss: 1.027506\n",
      "Sample 4, Recon 1, Iter 3800, Loss: 1.004466\n",
      "Sample 4, Recon 1, Iter 3900, Loss: 0.981791\n",
      "Sample 4, Recon 1, Iter 4000, Loss: 0.960345\n",
      "Sample 4, Recon 1, Iter 4100, Loss: 0.940633\n",
      "Sample 4, Recon 1, Iter 4200, Loss: 0.921735\n",
      "Sample 4, Recon 1, Iter 4300, Loss: 0.903200\n",
      "Sample 4, Recon 1, Iter 4400, Loss: 0.886155\n",
      "Sample 4, Recon 1, Iter 4500, Loss: 0.869040\n",
      "Sample 4, Recon 1, Iter 4600, Loss: 0.852426\n",
      "Sample 4, Recon 1, Iter 4700, Loss: 0.837645\n",
      "Sample 4, Recon 1, Iter 4800, Loss: 0.823355\n",
      "Sample 4, Recon 1, Iter 4900, Loss: 0.809238\n",
      "Sample 4, Recon 1, Iter 5000, Loss: 0.796195\n",
      "Sample 4, Recon 1, Iter 5100, Loss: 0.784927\n",
      "Sample 4, Recon 1, Iter 5200, Loss: 0.772056\n",
      "Sample 4, Recon 1, Iter 5300, Loss: 0.760635\n",
      "Sample 4, Recon 1, Iter 5400, Loss: 0.749491\n",
      "Sample 4, Recon 1, Iter 5500, Loss: 0.738724\n",
      "Sample 4, Recon 1, Iter 5600, Loss: 0.728065\n",
      "Sample 4, Recon 1, Iter 5700, Loss: 0.719465\n",
      "Sample 4, Recon 1, Iter 5800, Loss: 0.709298\n",
      "Sample 4, Recon 1, Iter 5900, Loss: 0.699726\n",
      "Sample 4, Recon 1, Iter 6000, Loss: 0.691774\n",
      "Sample 4, Recon 1, Iter 6100, Loss: 0.683709\n",
      "Sample 4, Recon 1, Iter 6200, Loss: 0.675552\n",
      "Sample 4, Recon 1, Iter 6300, Loss: 0.667884\n",
      "Sample 4, Recon 1, Iter 6400, Loss: 0.660069\n",
      "Sample 4, Recon 1, Iter 6500, Loss: 0.653148\n",
      "Sample 4, Recon 1, Iter 6600, Loss: 0.646197\n",
      "Sample 4, Recon 1, Iter 6700, Loss: 0.639394\n",
      "Sample 4, Recon 1, Iter 6800, Loss: 0.633112\n",
      "Sample 4, Recon 1, Iter 6900, Loss: 0.628400\n",
      "Sample 4, Recon 1, Iter 7000, Loss: 0.622927\n",
      "Sample 4, Recon 1, Iter 7100, Loss: 0.615626\n",
      "Sample 4, Recon 1, Iter 7200, Loss: 0.610253\n",
      "Sample 4, Recon 1, Iter 7300, Loss: 0.604658\n",
      "Sample 4, Recon 1, Iter 7400, Loss: 0.600560\n",
      "Sample 4, Recon 1, Iter 7500, Loss: 0.594517\n",
      "Sample 4, Recon 1, Iter 7600, Loss: 0.589658\n",
      "Sample 4, Recon 1, Iter 7700, Loss: 0.584682\n",
      "Sample 4, Recon 1, Iter 7800, Loss: 0.579915\n",
      "Sample 4, Recon 1, Iter 7900, Loss: 0.575594\n",
      "Sample 4, Recon 1, Iter 8000, Loss: 0.570684\n",
      "Sample 4, Recon 1, Iter 8100, Loss: 0.566400\n",
      "Sample 4, Recon 1, Iter 8200, Loss: 0.561522\n",
      "Sample 4, Recon 1, Iter 8300, Loss: 0.562004\n",
      "Sample 4, Recon 1, Iter 8400, Loss: 0.555186\n",
      "Sample 4, Recon 1, Iter 8500, Loss: 0.548808\n",
      "Sample 4, Recon 1, Iter 8600, Loss: 0.545185\n",
      "Sample 4, Recon 1, Iter 8700, Loss: 0.540791\n",
      "Sample 4, Recon 1, Iter 8800, Loss: 0.541188\n",
      "Sample 4, Recon 1, Iter 8900, Loss: 0.533423\n",
      "Sample 4, Recon 1, Iter 9000, Loss: 0.529610\n",
      "Sample 4, Recon 1, Iter 9100, Loss: 0.525748\n",
      "Sample 4, Recon 1, Iter 9200, Loss: 0.522700\n",
      "Sample 4, Recon 1, Iter 9300, Loss: 0.518865\n",
      "Sample 4, Recon 1, Iter 9400, Loss: 0.515441\n",
      "Sample 4, Recon 1, Iter 9500, Loss: 0.512675\n",
      "Sample 4, Recon 1, Iter 9600, Loss: 0.509890\n",
      "Sample 4, Recon 1, Iter 9700, Loss: 0.506109\n",
      "Sample 4, Recon 1, Iter 9800, Loss: 0.502830\n",
      "Sample 4, Recon 1, Iter 9900, Loss: 0.499725\n",
      "Sample 4, Recon 2, Iter 0, Loss: 14.174582\n",
      "Sample 4, Recon 2, Iter 100, Loss: 4.127648\n",
      "Sample 4, Recon 2, Iter 200, Loss: 3.737940\n",
      "Sample 4, Recon 2, Iter 300, Loss: 3.457467\n",
      "Sample 4, Recon 2, Iter 400, Loss: 3.234961\n",
      "Sample 4, Recon 2, Iter 500, Loss: 3.059455\n",
      "Sample 4, Recon 2, Iter 600, Loss: 2.914294\n",
      "Sample 4, Recon 2, Iter 700, Loss: 2.787117\n",
      "Sample 4, Recon 2, Iter 800, Loss: 2.668890\n",
      "Sample 4, Recon 2, Iter 900, Loss: 2.556273\n",
      "Sample 4, Recon 2, Iter 1000, Loss: 2.447492\n",
      "Sample 4, Recon 2, Iter 1100, Loss: 2.343424\n",
      "Sample 4, Recon 2, Iter 1200, Loss: 2.241603\n",
      "Sample 4, Recon 2, Iter 1300, Loss: 2.142373\n",
      "Sample 4, Recon 2, Iter 1400, Loss: 2.048534\n",
      "Sample 4, Recon 2, Iter 1500, Loss: 1.962689\n",
      "Sample 4, Recon 2, Iter 1600, Loss: 1.885740\n",
      "Sample 4, Recon 2, Iter 1700, Loss: 1.816098\n",
      "Sample 4, Recon 2, Iter 1800, Loss: 1.751659\n",
      "Sample 4, Recon 2, Iter 1900, Loss: 1.691319\n",
      "Sample 4, Recon 2, Iter 2000, Loss: 1.634525\n",
      "Sample 4, Recon 2, Iter 2100, Loss: 1.580741\n",
      "Sample 4, Recon 2, Iter 2200, Loss: 1.529269\n",
      "Sample 4, Recon 2, Iter 2300, Loss: 1.480339\n",
      "Sample 4, Recon 2, Iter 2400, Loss: 1.434386\n",
      "Sample 4, Recon 2, Iter 2500, Loss: 1.389906\n",
      "Sample 4, Recon 2, Iter 2600, Loss: 1.347827\n",
      "Sample 4, Recon 2, Iter 2700, Loss: 1.307042\n",
      "Sample 4, Recon 2, Iter 2800, Loss: 1.268308\n",
      "Sample 4, Recon 2, Iter 2900, Loss: 1.231159\n",
      "Sample 4, Recon 2, Iter 3000, Loss: 1.195624\n",
      "Sample 4, Recon 2, Iter 3100, Loss: 1.161862\n",
      "Sample 4, Recon 2, Iter 3200, Loss: 1.129627\n",
      "Sample 4, Recon 2, Iter 3300, Loss: 1.099371\n",
      "Sample 4, Recon 2, Iter 3400, Loss: 1.071263\n",
      "Sample 4, Recon 2, Iter 3500, Loss: 1.045961\n",
      "Sample 4, Recon 2, Iter 3600, Loss: 1.020009\n",
      "Sample 4, Recon 2, Iter 3700, Loss: 0.997087\n",
      "Sample 4, Recon 2, Iter 3800, Loss: 0.975483\n",
      "Sample 4, Recon 2, Iter 3900, Loss: 0.954900\n",
      "Sample 4, Recon 2, Iter 4000, Loss: 0.935985\n",
      "Sample 4, Recon 2, Iter 4100, Loss: 0.917723\n",
      "Sample 4, Recon 2, Iter 4200, Loss: 0.900928\n",
      "Sample 4, Recon 2, Iter 4300, Loss: 0.884571\n",
      "Sample 4, Recon 2, Iter 4400, Loss: 0.869754\n",
      "Sample 4, Recon 2, Iter 4500, Loss: 0.855443\n",
      "Sample 4, Recon 2, Iter 4600, Loss: 0.841693\n",
      "Sample 4, Recon 2, Iter 4700, Loss: 0.828726\n",
      "Sample 4, Recon 2, Iter 4800, Loss: 0.816200\n",
      "Sample 4, Recon 2, Iter 4900, Loss: 0.804533\n",
      "Sample 4, Recon 2, Iter 5000, Loss: 0.792413\n",
      "Sample 4, Recon 2, Iter 5100, Loss: 0.781827\n",
      "Sample 4, Recon 2, Iter 5200, Loss: 0.771687\n",
      "Sample 4, Recon 2, Iter 5300, Loss: 0.760943\n",
      "Sample 4, Recon 2, Iter 5400, Loss: 0.750830\n",
      "Sample 4, Recon 2, Iter 5500, Loss: 0.741865\n",
      "Sample 4, Recon 2, Iter 5600, Loss: 0.733793\n",
      "Sample 4, Recon 2, Iter 5700, Loss: 0.724217\n",
      "Sample 4, Recon 2, Iter 5800, Loss: 0.715605\n",
      "Sample 4, Recon 2, Iter 5900, Loss: 0.706772\n",
      "Sample 4, Recon 2, Iter 6000, Loss: 0.698682\n",
      "Sample 4, Recon 2, Iter 6100, Loss: 0.691756\n",
      "Sample 4, Recon 2, Iter 6200, Loss: 0.684312\n",
      "Sample 4, Recon 2, Iter 6300, Loss: 0.676584\n",
      "Sample 4, Recon 2, Iter 6400, Loss: 0.671163\n",
      "Sample 4, Recon 2, Iter 6500, Loss: 0.662841\n",
      "Sample 4, Recon 2, Iter 6600, Loss: 0.658472\n",
      "Sample 4, Recon 2, Iter 6700, Loss: 0.650566\n",
      "Sample 4, Recon 2, Iter 6800, Loss: 0.643806\n",
      "Sample 4, Recon 2, Iter 6900, Loss: 0.638439\n",
      "Sample 4, Recon 2, Iter 7000, Loss: 0.632706\n",
      "Sample 4, Recon 2, Iter 7100, Loss: 0.628900\n",
      "Sample 4, Recon 2, Iter 7200, Loss: 0.623464\n",
      "Sample 4, Recon 2, Iter 7300, Loss: 0.616903\n",
      "Sample 4, Recon 2, Iter 7400, Loss: 0.612395\n",
      "Sample 4, Recon 2, Iter 7500, Loss: 0.607687\n",
      "Sample 4, Recon 2, Iter 7600, Loss: 0.602804\n",
      "Sample 4, Recon 2, Iter 7700, Loss: 0.598307\n",
      "Sample 4, Recon 2, Iter 7800, Loss: 0.594281\n",
      "Sample 4, Recon 2, Iter 7900, Loss: 0.590425\n",
      "Sample 4, Recon 2, Iter 8000, Loss: 0.586394\n",
      "Sample 4, Recon 2, Iter 8100, Loss: 0.581046\n",
      "Sample 4, Recon 2, Iter 8200, Loss: 0.578091\n",
      "Sample 4, Recon 2, Iter 8300, Loss: 0.573111\n",
      "Sample 4, Recon 2, Iter 8400, Loss: 0.568963\n",
      "Sample 4, Recon 2, Iter 8500, Loss: 0.566404\n",
      "Sample 4, Recon 2, Iter 8600, Loss: 0.561947\n",
      "Sample 4, Recon 2, Iter 8700, Loss: 0.558473\n",
      "Sample 4, Recon 2, Iter 8800, Loss: 0.554657\n",
      "Sample 4, Recon 2, Iter 8900, Loss: 0.551789\n",
      "Sample 4, Recon 2, Iter 9000, Loss: 0.548183\n",
      "Sample 4, Recon 2, Iter 9100, Loss: 0.544806\n",
      "Sample 4, Recon 2, Iter 9200, Loss: 0.543240\n",
      "Sample 4, Recon 2, Iter 9300, Loss: 0.539829\n",
      "Sample 4, Recon 2, Iter 9400, Loss: 0.540560\n",
      "Sample 4, Recon 2, Iter 9500, Loss: 0.534224\n",
      "Sample 4, Recon 2, Iter 9600, Loss: 0.531180\n",
      "Sample 4, Recon 2, Iter 9700, Loss: 0.528616\n",
      "Sample 4, Recon 2, Iter 9800, Loss: 0.526111\n",
      "Sample 4, Recon 2, Iter 9900, Loss: 0.523885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:  50%|█████     | 5/10 [3:22:16<3:22:05, 2425.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 5, Recon 0, Iter 0, Loss: 15.805341\n",
      "Sample 5, Recon 0, Iter 100, Loss: 4.280476\n",
      "Sample 5, Recon 0, Iter 200, Loss: 3.614552\n",
      "Sample 5, Recon 0, Iter 300, Loss: 3.262625\n",
      "Sample 5, Recon 0, Iter 400, Loss: 3.050522\n",
      "Sample 5, Recon 0, Iter 500, Loss: 2.906803\n",
      "Sample 5, Recon 0, Iter 600, Loss: 2.789263\n",
      "Sample 5, Recon 0, Iter 700, Loss: 2.687781\n",
      "Sample 5, Recon 0, Iter 800, Loss: 2.595409\n",
      "Sample 5, Recon 0, Iter 900, Loss: 2.508285\n",
      "Sample 5, Recon 0, Iter 1000, Loss: 2.424669\n",
      "Sample 5, Recon 0, Iter 1100, Loss: 2.341742\n",
      "Sample 5, Recon 0, Iter 1200, Loss: 2.255262\n",
      "Sample 5, Recon 0, Iter 1300, Loss: 2.162829\n",
      "Sample 5, Recon 0, Iter 1400, Loss: 2.066997\n",
      "Sample 5, Recon 0, Iter 1500, Loss: 1.973005\n",
      "Sample 5, Recon 0, Iter 1600, Loss: 1.888705\n",
      "Sample 5, Recon 0, Iter 1700, Loss: 1.815387\n",
      "Sample 5, Recon 0, Iter 1800, Loss: 1.750129\n",
      "Sample 5, Recon 0, Iter 1900, Loss: 1.690473\n",
      "Sample 5, Recon 0, Iter 2000, Loss: 1.634676\n",
      "Sample 5, Recon 0, Iter 2100, Loss: 1.581744\n",
      "Sample 5, Recon 0, Iter 2200, Loss: 1.531084\n",
      "Sample 5, Recon 0, Iter 2300, Loss: 1.482353\n",
      "Sample 5, Recon 0, Iter 2400, Loss: 1.435425\n",
      "Sample 5, Recon 0, Iter 2500, Loss: 1.390395\n",
      "Sample 5, Recon 0, Iter 2600, Loss: 1.347282\n",
      "Sample 5, Recon 0, Iter 2700, Loss: 1.306170\n",
      "Sample 5, Recon 0, Iter 2800, Loss: 1.266962\n",
      "Sample 5, Recon 0, Iter 2900, Loss: 1.229751\n",
      "Sample 5, Recon 0, Iter 3000, Loss: 1.194476\n",
      "Sample 5, Recon 0, Iter 3100, Loss: 1.160838\n",
      "Sample 5, Recon 0, Iter 3200, Loss: 1.128845\n",
      "Sample 5, Recon 0, Iter 3300, Loss: 1.098306\n",
      "Sample 5, Recon 0, Iter 3400, Loss: 1.069874\n",
      "Sample 5, Recon 0, Iter 3500, Loss: 1.044148\n",
      "Sample 5, Recon 0, Iter 3600, Loss: 1.019035\n",
      "Sample 5, Recon 0, Iter 3700, Loss: 0.995229\n",
      "Sample 5, Recon 0, Iter 3800, Loss: 0.972831\n",
      "Sample 5, Recon 0, Iter 3900, Loss: 0.952270\n",
      "Sample 5, Recon 0, Iter 4000, Loss: 0.932437\n",
      "Sample 5, Recon 0, Iter 4100, Loss: 0.914077\n",
      "Sample 5, Recon 0, Iter 4200, Loss: 0.896497\n",
      "Sample 5, Recon 0, Iter 4300, Loss: 0.879723\n",
      "Sample 5, Recon 0, Iter 4400, Loss: 0.864416\n",
      "Sample 5, Recon 0, Iter 4500, Loss: 0.849016\n",
      "Sample 5, Recon 0, Iter 4600, Loss: 0.835966\n",
      "Sample 5, Recon 0, Iter 4700, Loss: 0.820727\n",
      "Sample 5, Recon 0, Iter 4800, Loss: 0.807227\n",
      "Sample 5, Recon 0, Iter 4900, Loss: 0.794410\n",
      "Sample 5, Recon 0, Iter 5000, Loss: 0.782217\n",
      "Sample 5, Recon 0, Iter 5100, Loss: 0.770473\n",
      "Sample 5, Recon 0, Iter 5200, Loss: 0.759927\n",
      "Sample 5, Recon 0, Iter 5300, Loss: 0.749046\n",
      "Sample 5, Recon 0, Iter 5400, Loss: 0.740025\n",
      "Sample 5, Recon 0, Iter 5500, Loss: 0.729432\n",
      "Sample 5, Recon 0, Iter 5600, Loss: 0.720690\n",
      "Sample 5, Recon 0, Iter 5700, Loss: 0.711693\n",
      "Sample 5, Recon 0, Iter 5800, Loss: 0.703190\n",
      "Sample 5, Recon 0, Iter 5900, Loss: 0.695370\n",
      "Sample 5, Recon 0, Iter 6000, Loss: 0.687370\n",
      "Sample 5, Recon 0, Iter 6100, Loss: 0.679733\n",
      "Sample 5, Recon 0, Iter 6200, Loss: 0.672556\n",
      "Sample 5, Recon 0, Iter 6300, Loss: 0.665647\n",
      "Sample 5, Recon 0, Iter 6400, Loss: 0.659050\n",
      "Sample 5, Recon 0, Iter 6500, Loss: 0.652759\n",
      "Sample 5, Recon 0, Iter 6600, Loss: 0.649736\n",
      "Sample 5, Recon 0, Iter 6700, Loss: 0.640716\n",
      "Sample 5, Recon 0, Iter 6800, Loss: 0.634760\n",
      "Sample 5, Recon 0, Iter 6900, Loss: 0.629067\n",
      "Sample 5, Recon 0, Iter 7000, Loss: 0.623738\n",
      "Sample 5, Recon 0, Iter 7100, Loss: 0.618288\n",
      "Sample 5, Recon 0, Iter 7200, Loss: 0.613264\n",
      "Sample 5, Recon 0, Iter 7300, Loss: 0.608203\n",
      "Sample 5, Recon 0, Iter 7400, Loss: 0.604258\n",
      "Sample 5, Recon 0, Iter 7500, Loss: 0.599623\n",
      "Sample 5, Recon 0, Iter 7600, Loss: 0.594256\n",
      "Sample 5, Recon 0, Iter 7700, Loss: 0.589981\n",
      "Sample 5, Recon 0, Iter 7800, Loss: 0.586115\n",
      "Sample 5, Recon 0, Iter 7900, Loss: 0.581999\n",
      "Sample 5, Recon 0, Iter 8000, Loss: 0.577362\n",
      "Sample 5, Recon 0, Iter 8100, Loss: 0.573013\n",
      "Sample 5, Recon 0, Iter 8200, Loss: 0.569358\n",
      "Sample 5, Recon 0, Iter 8300, Loss: 0.565430\n",
      "Sample 5, Recon 0, Iter 8400, Loss: 0.563789\n",
      "Sample 5, Recon 0, Iter 8500, Loss: 0.559381\n",
      "Sample 5, Recon 0, Iter 8600, Loss: 0.556402\n",
      "Sample 5, Recon 0, Iter 8700, Loss: 0.551687\n",
      "Sample 5, Recon 0, Iter 8800, Loss: 0.548556\n",
      "Sample 5, Recon 0, Iter 8900, Loss: 0.550624\n",
      "Sample 5, Recon 0, Iter 9000, Loss: 0.543301\n",
      "Sample 5, Recon 0, Iter 9100, Loss: 0.540073\n",
      "Sample 5, Recon 0, Iter 9200, Loss: 0.538675\n",
      "Sample 5, Recon 0, Iter 9300, Loss: 0.533614\n",
      "Sample 5, Recon 0, Iter 9400, Loss: 0.530671\n",
      "Sample 5, Recon 0, Iter 9500, Loss: 0.528383\n",
      "Sample 5, Recon 0, Iter 9600, Loss: 0.528756\n",
      "Sample 5, Recon 0, Iter 9700, Loss: 0.523099\n",
      "Sample 5, Recon 0, Iter 9800, Loss: 0.521127\n",
      "Sample 5, Recon 0, Iter 9900, Loss: 0.519604\n",
      "Sample 5, Recon 1, Iter 0, Loss: 20.560648\n",
      "Sample 5, Recon 1, Iter 100, Loss: 4.542825\n",
      "Sample 5, Recon 1, Iter 200, Loss: 3.692556\n",
      "Sample 5, Recon 1, Iter 300, Loss: 3.390776\n",
      "Sample 5, Recon 1, Iter 400, Loss: 3.207152\n",
      "Sample 5, Recon 1, Iter 500, Loss: 3.039633\n",
      "Sample 5, Recon 1, Iter 600, Loss: 2.890631\n",
      "Sample 5, Recon 1, Iter 700, Loss: 2.756531\n",
      "Sample 5, Recon 1, Iter 800, Loss: 2.630378\n",
      "Sample 5, Recon 1, Iter 900, Loss: 2.507663\n",
      "Sample 5, Recon 1, Iter 1000, Loss: 2.388206\n",
      "Sample 5, Recon 1, Iter 1100, Loss: 2.273864\n",
      "Sample 5, Recon 1, Iter 1200, Loss: 2.170042\n",
      "Sample 5, Recon 1, Iter 1300, Loss: 2.080093\n",
      "Sample 5, Recon 1, Iter 1400, Loss: 2.001957\n",
      "Sample 5, Recon 1, Iter 1500, Loss: 1.932122\n",
      "Sample 5, Recon 1, Iter 1600, Loss: 1.868064\n",
      "Sample 5, Recon 1, Iter 1700, Loss: 1.808104\n",
      "Sample 5, Recon 1, Iter 1800, Loss: 1.751260\n",
      "Sample 5, Recon 1, Iter 1900, Loss: 1.697109\n",
      "Sample 5, Recon 1, Iter 2000, Loss: 1.644954\n",
      "Sample 5, Recon 1, Iter 2100, Loss: 1.594715\n",
      "Sample 5, Recon 1, Iter 2200, Loss: 1.546499\n",
      "Sample 5, Recon 1, Iter 2300, Loss: 1.500107\n",
      "Sample 5, Recon 1, Iter 2400, Loss: 1.455556\n",
      "Sample 5, Recon 1, Iter 2500, Loss: 1.412817\n",
      "Sample 5, Recon 1, Iter 2600, Loss: 1.371773\n",
      "Sample 5, Recon 1, Iter 2700, Loss: 1.332556\n",
      "Sample 5, Recon 1, Iter 2800, Loss: 1.295060\n",
      "Sample 5, Recon 1, Iter 2900, Loss: 1.259223\n",
      "Sample 5, Recon 1, Iter 3000, Loss: 1.224878\n",
      "Sample 5, Recon 1, Iter 3100, Loss: 1.191979\n",
      "Sample 5, Recon 1, Iter 3200, Loss: 1.160456\n",
      "Sample 5, Recon 1, Iter 3300, Loss: 1.130397\n",
      "Sample 5, Recon 1, Iter 3400, Loss: 1.101618\n",
      "Sample 5, Recon 1, Iter 3500, Loss: 1.074111\n",
      "Sample 5, Recon 1, Iter 3600, Loss: 1.047920\n",
      "Sample 5, Recon 1, Iter 3700, Loss: 1.023040\n",
      "Sample 5, Recon 1, Iter 3800, Loss: 0.998342\n",
      "Sample 5, Recon 1, Iter 3900, Loss: 0.975356\n",
      "Sample 5, Recon 1, Iter 4000, Loss: 0.952998\n",
      "Sample 5, Recon 1, Iter 4100, Loss: 0.931969\n",
      "Sample 5, Recon 1, Iter 4200, Loss: 0.911557\n",
      "Sample 5, Recon 1, Iter 4300, Loss: 0.892136\n",
      "Sample 5, Recon 1, Iter 4400, Loss: 0.873701\n",
      "Sample 5, Recon 1, Iter 4500, Loss: 0.856150\n",
      "Sample 5, Recon 1, Iter 4600, Loss: 0.839679\n",
      "Sample 5, Recon 1, Iter 4700, Loss: 0.823946\n",
      "Sample 5, Recon 1, Iter 4800, Loss: 0.809397\n",
      "Sample 5, Recon 1, Iter 4900, Loss: 0.795365\n",
      "Sample 5, Recon 1, Iter 5000, Loss: 0.781898\n",
      "Sample 5, Recon 1, Iter 5100, Loss: 0.769153\n",
      "Sample 5, Recon 1, Iter 5200, Loss: 0.757320\n",
      "Sample 5, Recon 1, Iter 5300, Loss: 0.746357\n",
      "Sample 5, Recon 1, Iter 5400, Loss: 0.735418\n",
      "Sample 5, Recon 1, Iter 5500, Loss: 0.725460\n",
      "Sample 5, Recon 1, Iter 5600, Loss: 0.716130\n",
      "Sample 5, Recon 1, Iter 5700, Loss: 0.706500\n",
      "Sample 5, Recon 1, Iter 5800, Loss: 0.698258\n",
      "Sample 5, Recon 1, Iter 5900, Loss: 0.689638\n",
      "Sample 5, Recon 1, Iter 6000, Loss: 0.680934\n",
      "Sample 5, Recon 1, Iter 6100, Loss: 0.672862\n",
      "Sample 5, Recon 1, Iter 6200, Loss: 0.665524\n",
      "Sample 5, Recon 1, Iter 6300, Loss: 0.658289\n",
      "Sample 5, Recon 1, Iter 6400, Loss: 0.651113\n",
      "Sample 5, Recon 1, Iter 6500, Loss: 0.644408\n",
      "Sample 5, Recon 1, Iter 6600, Loss: 0.638791\n",
      "Sample 5, Recon 1, Iter 6700, Loss: 0.632023\n",
      "Sample 5, Recon 1, Iter 6800, Loss: 0.626492\n",
      "Sample 5, Recon 1, Iter 6900, Loss: 0.621193\n",
      "Sample 5, Recon 1, Iter 7000, Loss: 0.615470\n",
      "Sample 5, Recon 1, Iter 7100, Loss: 0.610705\n",
      "Sample 5, Recon 1, Iter 7200, Loss: 0.605616\n",
      "Sample 5, Recon 1, Iter 7300, Loss: 0.602904\n",
      "Sample 5, Recon 1, Iter 7400, Loss: 0.596180\n",
      "Sample 5, Recon 1, Iter 7500, Loss: 0.591443\n",
      "Sample 5, Recon 1, Iter 7600, Loss: 0.588336\n",
      "Sample 5, Recon 1, Iter 7700, Loss: 0.583160\n",
      "Sample 5, Recon 1, Iter 7800, Loss: 0.579564\n",
      "Sample 5, Recon 1, Iter 7900, Loss: 0.575188\n",
      "Sample 5, Recon 1, Iter 8000, Loss: 0.571263\n",
      "Sample 5, Recon 1, Iter 8100, Loss: 0.567952\n",
      "Sample 5, Recon 1, Iter 8200, Loss: 0.564699\n",
      "Sample 5, Recon 1, Iter 8300, Loss: 0.564008\n",
      "Sample 5, Recon 1, Iter 8400, Loss: 0.557239\n",
      "Sample 5, Recon 1, Iter 8500, Loss: 0.555368\n",
      "Sample 5, Recon 1, Iter 8600, Loss: 0.550991\n",
      "Sample 5, Recon 1, Iter 8700, Loss: 0.548000\n",
      "Sample 5, Recon 1, Iter 8800, Loss: 0.545505\n",
      "Sample 5, Recon 1, Iter 8900, Loss: 0.542011\n",
      "Sample 5, Recon 1, Iter 9000, Loss: 0.539007\n",
      "Sample 5, Recon 1, Iter 9100, Loss: 0.536331\n",
      "Sample 5, Recon 1, Iter 9200, Loss: 0.535151\n",
      "Sample 5, Recon 1, Iter 9300, Loss: 0.530817\n",
      "Sample 5, Recon 1, Iter 9400, Loss: 0.528438\n",
      "Sample 5, Recon 1, Iter 9500, Loss: 0.525663\n",
      "Sample 5, Recon 1, Iter 9600, Loss: 0.523864\n",
      "Sample 5, Recon 1, Iter 9700, Loss: 0.521611\n",
      "Sample 5, Recon 1, Iter 9800, Loss: 0.519287\n",
      "Sample 5, Recon 1, Iter 9900, Loss: 0.517032\n",
      "Sample 5, Recon 2, Iter 0, Loss: 13.126416\n",
      "Sample 5, Recon 2, Iter 100, Loss: 5.170096\n",
      "Sample 5, Recon 2, Iter 200, Loss: 4.683710\n",
      "Sample 5, Recon 2, Iter 300, Loss: 4.352983\n",
      "Sample 5, Recon 2, Iter 400, Loss: 3.657440\n",
      "Sample 5, Recon 2, Iter 500, Loss: 3.381981\n",
      "Sample 5, Recon 2, Iter 600, Loss: 3.259238\n",
      "Sample 5, Recon 2, Iter 700, Loss: 3.155912\n",
      "Sample 5, Recon 2, Iter 800, Loss: 3.066958\n",
      "Sample 5, Recon 2, Iter 900, Loss: 2.975701\n",
      "Sample 5, Recon 2, Iter 1000, Loss: 2.892377\n",
      "Sample 5, Recon 2, Iter 1100, Loss: 2.804166\n",
      "Sample 5, Recon 2, Iter 1200, Loss: 2.705466\n",
      "Sample 5, Recon 2, Iter 1300, Loss: 2.608327\n",
      "Sample 5, Recon 2, Iter 1400, Loss: 2.507729\n",
      "Sample 5, Recon 2, Iter 1500, Loss: 2.424798\n",
      "Sample 5, Recon 2, Iter 1600, Loss: 2.355411\n",
      "Sample 5, Recon 2, Iter 1700, Loss: 2.293786\n",
      "Sample 5, Recon 2, Iter 1800, Loss: 2.237254\n",
      "Sample 5, Recon 2, Iter 1900, Loss: 2.149043\n",
      "Sample 5, Recon 2, Iter 2000, Loss: 2.075743\n",
      "Sample 5, Recon 2, Iter 2100, Loss: 2.019905\n",
      "Sample 5, Recon 2, Iter 2200, Loss: 1.972479\n",
      "Sample 5, Recon 2, Iter 2300, Loss: 1.928698\n",
      "Sample 5, Recon 2, Iter 2400, Loss: 1.889949\n",
      "Sample 5, Recon 2, Iter 2500, Loss: 1.852648\n",
      "Sample 5, Recon 2, Iter 2600, Loss: 1.818089\n",
      "Sample 5, Recon 2, Iter 2700, Loss: 1.783874\n",
      "Sample 5, Recon 2, Iter 2800, Loss: 1.751815\n",
      "Sample 5, Recon 2, Iter 2900, Loss: 1.720315\n",
      "Sample 5, Recon 2, Iter 3000, Loss: 1.691319\n",
      "Sample 5, Recon 2, Iter 3100, Loss: 1.664847\n",
      "Sample 5, Recon 2, Iter 3200, Loss: 1.637958\n",
      "Sample 5, Recon 2, Iter 3300, Loss: 1.610435\n",
      "Sample 5, Recon 2, Iter 3400, Loss: 1.585503\n",
      "Sample 5, Recon 2, Iter 3500, Loss: 1.561191\n",
      "Sample 5, Recon 2, Iter 3600, Loss: 1.538520\n",
      "Sample 5, Recon 2, Iter 3700, Loss: 1.518565\n",
      "Sample 5, Recon 2, Iter 3800, Loss: 1.495331\n",
      "Sample 5, Recon 2, Iter 3900, Loss: 1.473954\n",
      "Sample 5, Recon 2, Iter 4000, Loss: 1.455015\n",
      "Sample 5, Recon 2, Iter 4100, Loss: 1.440159\n",
      "Sample 5, Recon 2, Iter 4200, Loss: 1.415769\n",
      "Sample 5, Recon 2, Iter 4300, Loss: 1.398901\n",
      "Sample 5, Recon 2, Iter 4400, Loss: 1.379859\n",
      "Sample 5, Recon 2, Iter 4500, Loss: 1.361470\n",
      "Sample 5, Recon 2, Iter 4600, Loss: 1.343341\n",
      "Sample 5, Recon 2, Iter 4700, Loss: 1.326547\n",
      "Sample 5, Recon 2, Iter 4800, Loss: 1.307789\n",
      "Sample 5, Recon 2, Iter 4900, Loss: 1.287078\n",
      "Sample 5, Recon 2, Iter 5000, Loss: 1.291944\n",
      "Sample 5, Recon 2, Iter 5100, Loss: 1.242821\n",
      "Sample 5, Recon 2, Iter 5200, Loss: 1.220515\n",
      "Sample 5, Recon 2, Iter 5300, Loss: 1.203998\n",
      "Sample 5, Recon 2, Iter 5400, Loss: 1.189062\n",
      "Sample 5, Recon 2, Iter 5500, Loss: 1.174677\n",
      "Sample 5, Recon 2, Iter 5600, Loss: 1.160743\n",
      "Sample 5, Recon 2, Iter 5700, Loss: 1.147018\n",
      "Sample 5, Recon 2, Iter 5800, Loss: 1.133522\n",
      "Sample 5, Recon 2, Iter 5900, Loss: 1.120211\n",
      "Sample 5, Recon 2, Iter 6000, Loss: 1.106816\n",
      "Sample 5, Recon 2, Iter 6100, Loss: 1.093549\n",
      "Sample 5, Recon 2, Iter 6200, Loss: 1.080513\n",
      "Sample 5, Recon 2, Iter 6300, Loss: 1.067511\n",
      "Sample 5, Recon 2, Iter 6400, Loss: 1.054582\n",
      "Sample 5, Recon 2, Iter 6500, Loss: 1.041387\n",
      "Sample 5, Recon 2, Iter 6600, Loss: 1.028696\n",
      "Sample 5, Recon 2, Iter 6700, Loss: 1.015570\n",
      "Sample 5, Recon 2, Iter 6800, Loss: 1.003116\n",
      "Sample 5, Recon 2, Iter 6900, Loss: 0.990422\n",
      "Sample 5, Recon 2, Iter 7000, Loss: 0.977781\n",
      "Sample 5, Recon 2, Iter 7100, Loss: 0.965989\n",
      "Sample 5, Recon 2, Iter 7200, Loss: 0.953482\n",
      "Sample 5, Recon 2, Iter 7300, Loss: 0.941781\n",
      "Sample 5, Recon 2, Iter 7400, Loss: 0.929711\n",
      "Sample 5, Recon 2, Iter 7500, Loss: 0.918447\n",
      "Sample 5, Recon 2, Iter 7600, Loss: 0.907224\n",
      "Sample 5, Recon 2, Iter 7700, Loss: 0.896115\n",
      "Sample 5, Recon 2, Iter 7800, Loss: 0.885967\n",
      "Sample 5, Recon 2, Iter 7900, Loss: 0.874686\n",
      "Sample 5, Recon 2, Iter 8000, Loss: 0.865198\n",
      "Sample 5, Recon 2, Iter 8100, Loss: 0.855681\n",
      "Sample 5, Recon 2, Iter 8200, Loss: 0.846471\n",
      "Sample 5, Recon 2, Iter 8300, Loss: 0.836086\n",
      "Sample 5, Recon 2, Iter 8400, Loss: 0.827169\n",
      "Sample 5, Recon 2, Iter 8500, Loss: 0.818946\n",
      "Sample 5, Recon 2, Iter 8600, Loss: 0.809706\n",
      "Sample 5, Recon 2, Iter 8700, Loss: 0.801293\n",
      "Sample 5, Recon 2, Iter 8800, Loss: 0.793419\n",
      "Sample 5, Recon 2, Iter 8900, Loss: 0.785667\n",
      "Sample 5, Recon 2, Iter 9000, Loss: 0.778307\n",
      "Sample 5, Recon 2, Iter 9100, Loss: 0.770984\n",
      "Sample 5, Recon 2, Iter 9200, Loss: 0.764968\n",
      "Sample 5, Recon 2, Iter 9300, Loss: 0.757532\n",
      "Sample 5, Recon 2, Iter 9400, Loss: 0.750631\n",
      "Sample 5, Recon 2, Iter 9500, Loss: 0.744427\n",
      "Sample 5, Recon 2, Iter 9600, Loss: 0.738006\n",
      "Sample 5, Recon 2, Iter 9700, Loss: 0.731900\n",
      "Sample 5, Recon 2, Iter 9800, Loss: 0.725808\n",
      "Sample 5, Recon 2, Iter 9900, Loss: 0.720290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:  60%|██████    | 6/10 [4:03:31<2:42:48, 2442.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 6, Recon 0, Iter 0, Loss: 10.266039\n",
      "Sample 6, Recon 0, Iter 100, Loss: 5.051946\n",
      "Sample 6, Recon 0, Iter 200, Loss: 3.963484\n",
      "Sample 6, Recon 0, Iter 300, Loss: 3.457649\n",
      "Sample 6, Recon 0, Iter 400, Loss: 3.185921\n",
      "Sample 6, Recon 0, Iter 500, Loss: 2.964640\n",
      "Sample 6, Recon 0, Iter 600, Loss: 2.763370\n",
      "Sample 6, Recon 0, Iter 700, Loss: 2.578847\n",
      "Sample 6, Recon 0, Iter 800, Loss: 2.330081\n",
      "Sample 6, Recon 0, Iter 900, Loss: 2.190760\n",
      "Sample 6, Recon 0, Iter 1000, Loss: 2.088221\n",
      "Sample 6, Recon 0, Iter 1100, Loss: 2.000694\n",
      "Sample 6, Recon 0, Iter 1200, Loss: 1.922232\n",
      "Sample 6, Recon 0, Iter 1300, Loss: 1.852653\n",
      "Sample 6, Recon 0, Iter 1400, Loss: 1.789134\n",
      "Sample 6, Recon 0, Iter 1500, Loss: 1.727700\n",
      "Sample 6, Recon 0, Iter 1600, Loss: 1.670130\n",
      "Sample 6, Recon 0, Iter 1700, Loss: 1.615627\n",
      "Sample 6, Recon 0, Iter 1800, Loss: 1.563237\n",
      "Sample 6, Recon 0, Iter 1900, Loss: 1.512981\n",
      "Sample 6, Recon 0, Iter 2000, Loss: 1.465101\n",
      "Sample 6, Recon 0, Iter 2100, Loss: 1.419598\n",
      "Sample 6, Recon 0, Iter 2200, Loss: 1.376541\n",
      "Sample 6, Recon 0, Iter 2300, Loss: 1.336187\n",
      "Sample 6, Recon 0, Iter 2400, Loss: 1.297271\n",
      "Sample 6, Recon 0, Iter 2500, Loss: 1.261558\n",
      "Sample 6, Recon 0, Iter 2600, Loss: 1.228339\n",
      "Sample 6, Recon 0, Iter 2700, Loss: 1.197008\n",
      "Sample 6, Recon 0, Iter 2800, Loss: 1.167520\n",
      "Sample 6, Recon 0, Iter 2900, Loss: 1.141367\n",
      "Sample 6, Recon 0, Iter 3000, Loss: 1.114665\n",
      "Sample 6, Recon 0, Iter 3100, Loss: 1.091502\n",
      "Sample 6, Recon 0, Iter 3200, Loss: 1.069575\n",
      "Sample 6, Recon 0, Iter 3300, Loss: 1.049405\n",
      "Sample 6, Recon 0, Iter 3400, Loss: 1.030831\n",
      "Sample 6, Recon 0, Iter 3500, Loss: 1.013297\n",
      "Sample 6, Recon 0, Iter 3600, Loss: 0.996104\n",
      "Sample 6, Recon 0, Iter 3700, Loss: 0.982670\n",
      "Sample 6, Recon 0, Iter 3800, Loss: 0.967873\n",
      "Sample 6, Recon 0, Iter 3900, Loss: 0.953649\n",
      "Sample 6, Recon 0, Iter 4000, Loss: 0.940557\n",
      "Sample 6, Recon 0, Iter 4100, Loss: 0.928051\n",
      "Sample 6, Recon 0, Iter 4200, Loss: 0.915533\n",
      "Sample 6, Recon 0, Iter 4300, Loss: 0.904633\n",
      "Sample 6, Recon 0, Iter 4400, Loss: 0.893563\n",
      "Sample 6, Recon 0, Iter 4500, Loss: 0.883773\n",
      "Sample 6, Recon 0, Iter 4600, Loss: 0.873674\n",
      "Sample 6, Recon 0, Iter 4700, Loss: 0.864611\n",
      "Sample 6, Recon 0, Iter 4800, Loss: 0.855120\n",
      "Sample 6, Recon 0, Iter 4900, Loss: 0.846376\n",
      "Sample 6, Recon 0, Iter 5000, Loss: 0.838302\n",
      "Sample 6, Recon 0, Iter 5100, Loss: 0.830667\n",
      "Sample 6, Recon 0, Iter 5200, Loss: 0.831950\n",
      "Sample 6, Recon 0, Iter 5300, Loss: 0.816409\n",
      "Sample 6, Recon 0, Iter 5400, Loss: 0.808854\n",
      "Sample 6, Recon 0, Iter 5500, Loss: 0.803334\n",
      "Sample 6, Recon 0, Iter 5600, Loss: 0.795106\n",
      "Sample 6, Recon 0, Iter 5700, Loss: 0.789484\n",
      "Sample 6, Recon 0, Iter 5800, Loss: 0.782519\n",
      "Sample 6, Recon 0, Iter 5900, Loss: 0.775954\n",
      "Sample 6, Recon 0, Iter 6000, Loss: 0.770285\n",
      "Sample 6, Recon 0, Iter 6100, Loss: 0.764519\n",
      "Sample 6, Recon 0, Iter 6200, Loss: 0.757995\n",
      "Sample 6, Recon 0, Iter 6300, Loss: 0.752134\n",
      "Sample 6, Recon 0, Iter 6400, Loss: 0.747121\n",
      "Sample 6, Recon 0, Iter 6500, Loss: 0.741074\n",
      "Sample 6, Recon 0, Iter 6600, Loss: 0.741204\n",
      "Sample 6, Recon 0, Iter 6700, Loss: 0.731688\n",
      "Sample 6, Recon 0, Iter 6800, Loss: 0.726347\n",
      "Sample 6, Recon 0, Iter 6900, Loss: 0.721158\n",
      "Sample 6, Recon 0, Iter 7000, Loss: 0.716702\n",
      "Sample 6, Recon 0, Iter 7100, Loss: 0.714994\n",
      "Sample 6, Recon 0, Iter 7200, Loss: 0.708097\n",
      "Sample 6, Recon 0, Iter 7300, Loss: 0.703719\n",
      "Sample 6, Recon 0, Iter 7400, Loss: 0.699391\n",
      "Sample 6, Recon 0, Iter 7500, Loss: 0.695224\n",
      "Sample 6, Recon 0, Iter 7600, Loss: 0.691447\n",
      "Sample 6, Recon 0, Iter 7700, Loss: 0.691016\n",
      "Sample 6, Recon 0, Iter 7800, Loss: 0.682889\n",
      "Sample 6, Recon 0, Iter 7900, Loss: 0.678873\n",
      "Sample 6, Recon 0, Iter 8000, Loss: 0.676686\n",
      "Sample 6, Recon 0, Iter 8100, Loss: 0.671266\n",
      "Sample 6, Recon 0, Iter 8200, Loss: 0.669178\n",
      "Sample 6, Recon 0, Iter 8300, Loss: 0.663816\n",
      "Sample 6, Recon 0, Iter 8400, Loss: 0.665887\n",
      "Sample 6, Recon 0, Iter 8500, Loss: 0.657077\n",
      "Sample 6, Recon 0, Iter 8600, Loss: 0.653607\n",
      "Sample 6, Recon 0, Iter 8700, Loss: 0.652064\n",
      "Sample 6, Recon 0, Iter 8800, Loss: 0.647549\n",
      "Sample 6, Recon 0, Iter 8900, Loss: 0.644229\n",
      "Sample 6, Recon 0, Iter 9000, Loss: 0.640905\n",
      "Sample 6, Recon 0, Iter 9100, Loss: 0.637806\n",
      "Sample 6, Recon 0, Iter 9200, Loss: 0.634393\n",
      "Sample 6, Recon 0, Iter 9300, Loss: 0.633269\n",
      "Sample 6, Recon 0, Iter 9400, Loss: 0.628154\n",
      "Sample 6, Recon 0, Iter 9500, Loss: 0.625250\n",
      "Sample 6, Recon 0, Iter 9600, Loss: 0.622754\n",
      "Sample 6, Recon 0, Iter 9700, Loss: 0.619605\n",
      "Sample 6, Recon 0, Iter 9800, Loss: 0.617570\n",
      "Sample 6, Recon 0, Iter 9900, Loss: 0.616140\n",
      "Sample 6, Recon 1, Iter 0, Loss: 16.474966\n",
      "Sample 6, Recon 1, Iter 100, Loss: 5.608964\n",
      "Sample 6, Recon 1, Iter 200, Loss: 4.060585\n",
      "Sample 6, Recon 1, Iter 300, Loss: 3.528997\n",
      "Sample 6, Recon 1, Iter 400, Loss: 3.219773\n",
      "Sample 6, Recon 1, Iter 500, Loss: 3.063477\n",
      "Sample 6, Recon 1, Iter 600, Loss: 2.939773\n",
      "Sample 6, Recon 1, Iter 700, Loss: 2.825869\n",
      "Sample 6, Recon 1, Iter 800, Loss: 2.713080\n",
      "Sample 6, Recon 1, Iter 900, Loss: 2.582276\n",
      "Sample 6, Recon 1, Iter 1000, Loss: 2.446139\n",
      "Sample 6, Recon 1, Iter 1100, Loss: 2.311452\n",
      "Sample 6, Recon 1, Iter 1200, Loss: 2.191564\n",
      "Sample 6, Recon 1, Iter 1300, Loss: 2.091789\n",
      "Sample 6, Recon 1, Iter 1400, Loss: 2.005752\n",
      "Sample 6, Recon 1, Iter 1500, Loss: 1.928096\n",
      "Sample 6, Recon 1, Iter 1600, Loss: 1.856737\n",
      "Sample 6, Recon 1, Iter 1700, Loss: 1.789860\n",
      "Sample 6, Recon 1, Iter 1800, Loss: 1.726822\n",
      "Sample 6, Recon 1, Iter 1900, Loss: 1.667532\n",
      "Sample 6, Recon 1, Iter 2000, Loss: 1.612004\n",
      "Sample 6, Recon 1, Iter 2100, Loss: 1.560384\n",
      "Sample 6, Recon 1, Iter 2200, Loss: 1.511707\n",
      "Sample 6, Recon 1, Iter 2300, Loss: 1.466968\n",
      "Sample 6, Recon 1, Iter 2400, Loss: 1.425458\n",
      "Sample 6, Recon 1, Iter 2500, Loss: 1.386867\n",
      "Sample 6, Recon 1, Iter 2600, Loss: 1.351848\n",
      "Sample 6, Recon 1, Iter 2700, Loss: 1.319649\n",
      "Sample 6, Recon 1, Iter 2800, Loss: 1.290344\n",
      "Sample 6, Recon 1, Iter 2900, Loss: 1.263435\n",
      "Sample 6, Recon 1, Iter 3000, Loss: 1.237172\n",
      "Sample 6, Recon 1, Iter 3100, Loss: 1.213672\n",
      "Sample 6, Recon 1, Iter 3200, Loss: 1.190899\n",
      "Sample 6, Recon 1, Iter 3300, Loss: 1.170155\n",
      "Sample 6, Recon 1, Iter 3400, Loss: 1.150115\n",
      "Sample 6, Recon 1, Iter 3500, Loss: 1.131472\n",
      "Sample 6, Recon 1, Iter 3600, Loss: 1.114837\n",
      "Sample 6, Recon 1, Iter 3700, Loss: 1.098614\n",
      "Sample 6, Recon 1, Iter 3800, Loss: 1.084174\n",
      "Sample 6, Recon 1, Iter 3900, Loss: 1.070799\n",
      "Sample 6, Recon 1, Iter 4000, Loss: 1.057364\n",
      "Sample 6, Recon 1, Iter 4100, Loss: 1.044973\n",
      "Sample 6, Recon 1, Iter 4200, Loss: 1.032554\n",
      "Sample 6, Recon 1, Iter 4300, Loss: 1.020402\n",
      "Sample 6, Recon 1, Iter 4400, Loss: 1.009025\n",
      "Sample 6, Recon 1, Iter 4500, Loss: 0.998154\n",
      "Sample 6, Recon 1, Iter 4600, Loss: 0.988028\n",
      "Sample 6, Recon 1, Iter 4700, Loss: 0.979187\n",
      "Sample 6, Recon 1, Iter 4800, Loss: 0.970138\n",
      "Sample 6, Recon 1, Iter 4900, Loss: 0.959644\n",
      "Sample 6, Recon 1, Iter 5000, Loss: 0.948774\n",
      "Sample 6, Recon 1, Iter 5100, Loss: 0.940939\n",
      "Sample 6, Recon 1, Iter 5200, Loss: 0.931333\n",
      "Sample 6, Recon 1, Iter 5300, Loss: 0.922028\n",
      "Sample 6, Recon 1, Iter 5400, Loss: 0.914859\n",
      "Sample 6, Recon 1, Iter 5500, Loss: 0.907034\n",
      "Sample 6, Recon 1, Iter 5600, Loss: 0.897935\n",
      "Sample 6, Recon 1, Iter 5700, Loss: 0.892382\n",
      "Sample 6, Recon 1, Iter 5800, Loss: 0.884448\n",
      "Sample 6, Recon 1, Iter 5900, Loss: 0.878060\n",
      "Sample 6, Recon 1, Iter 6000, Loss: 0.871349\n",
      "Sample 6, Recon 1, Iter 6100, Loss: 0.865170\n",
      "Sample 6, Recon 1, Iter 6200, Loss: 0.859830\n",
      "Sample 6, Recon 1, Iter 6300, Loss: 0.853300\n",
      "Sample 6, Recon 1, Iter 6400, Loss: 0.847356\n",
      "Sample 6, Recon 1, Iter 6500, Loss: 0.839769\n",
      "Sample 6, Recon 1, Iter 6600, Loss: 0.835119\n",
      "Sample 6, Recon 1, Iter 6700, Loss: 0.827269\n",
      "Sample 6, Recon 1, Iter 6800, Loss: 0.821934\n",
      "Sample 6, Recon 1, Iter 6900, Loss: 0.815101\n",
      "Sample 6, Recon 1, Iter 7000, Loss: 0.809991\n",
      "Sample 6, Recon 1, Iter 7100, Loss: 0.804636\n",
      "Sample 6, Recon 1, Iter 7200, Loss: 0.801784\n",
      "Sample 6, Recon 1, Iter 7300, Loss: 0.795960\n",
      "Sample 6, Recon 1, Iter 7400, Loss: 0.789471\n",
      "Sample 6, Recon 1, Iter 7500, Loss: 0.784999\n",
      "Sample 6, Recon 1, Iter 7600, Loss: 0.781994\n",
      "Sample 6, Recon 1, Iter 7700, Loss: 0.775583\n",
      "Sample 6, Recon 1, Iter 7800, Loss: 0.771890\n",
      "Sample 6, Recon 1, Iter 7900, Loss: 0.768021\n",
      "Sample 6, Recon 1, Iter 8000, Loss: 0.764264\n",
      "Sample 6, Recon 1, Iter 8100, Loss: 0.760355\n",
      "Sample 6, Recon 1, Iter 8200, Loss: 0.755435\n",
      "Sample 6, Recon 1, Iter 8300, Loss: 0.752200\n",
      "Sample 6, Recon 1, Iter 8400, Loss: 0.749316\n",
      "Sample 6, Recon 1, Iter 8500, Loss: 0.743873\n",
      "Sample 6, Recon 1, Iter 8600, Loss: 0.739769\n",
      "Sample 6, Recon 1, Iter 8700, Loss: 0.737770\n",
      "Sample 6, Recon 1, Iter 8800, Loss: 0.732903\n",
      "Sample 6, Recon 1, Iter 8900, Loss: 0.729036\n",
      "Sample 6, Recon 1, Iter 9000, Loss: 0.727096\n",
      "Sample 6, Recon 1, Iter 9100, Loss: 0.722817\n",
      "Sample 6, Recon 1, Iter 9200, Loss: 0.719436\n",
      "Sample 6, Recon 1, Iter 9300, Loss: 0.718164\n",
      "Sample 6, Recon 1, Iter 9400, Loss: 0.714876\n",
      "Sample 6, Recon 1, Iter 9500, Loss: 0.710917\n",
      "Sample 6, Recon 1, Iter 9600, Loss: 0.709444\n",
      "Sample 6, Recon 1, Iter 9700, Loss: 0.705565\n",
      "Sample 6, Recon 1, Iter 9800, Loss: 0.706605\n",
      "Sample 6, Recon 1, Iter 9900, Loss: 0.698995\n",
      "Sample 6, Recon 2, Iter 0, Loss: 24.336695\n",
      "Sample 6, Recon 2, Iter 100, Loss: 6.227009\n",
      "Sample 6, Recon 2, Iter 200, Loss: 4.385176\n",
      "Sample 6, Recon 2, Iter 300, Loss: 3.720065\n",
      "Sample 6, Recon 2, Iter 400, Loss: 3.423874\n",
      "Sample 6, Recon 2, Iter 500, Loss: 3.237011\n",
      "Sample 6, Recon 2, Iter 600, Loss: 3.084314\n",
      "Sample 6, Recon 2, Iter 700, Loss: 2.935623\n",
      "Sample 6, Recon 2, Iter 800, Loss: 2.809504\n",
      "Sample 6, Recon 2, Iter 900, Loss: 2.691421\n",
      "Sample 6, Recon 2, Iter 1000, Loss: 2.576602\n",
      "Sample 6, Recon 2, Iter 1100, Loss: 2.465928\n",
      "Sample 6, Recon 2, Iter 1200, Loss: 2.361655\n",
      "Sample 6, Recon 2, Iter 1300, Loss: 2.264163\n",
      "Sample 6, Recon 2, Iter 1400, Loss: 2.174232\n",
      "Sample 6, Recon 2, Iter 1500, Loss: 2.092390\n",
      "Sample 6, Recon 2, Iter 1600, Loss: 2.017585\n",
      "Sample 6, Recon 2, Iter 1700, Loss: 1.948123\n",
      "Sample 6, Recon 2, Iter 1800, Loss: 1.883468\n",
      "Sample 6, Recon 2, Iter 1900, Loss: 1.822546\n",
      "Sample 6, Recon 2, Iter 2000, Loss: 1.764815\n",
      "Sample 6, Recon 2, Iter 2100, Loss: 1.709798\n",
      "Sample 6, Recon 2, Iter 2200, Loss: 1.657508\n",
      "Sample 6, Recon 2, Iter 2300, Loss: 1.607766\n",
      "Sample 6, Recon 2, Iter 2400, Loss: 1.559973\n",
      "Sample 6, Recon 2, Iter 2500, Loss: 1.514519\n",
      "Sample 6, Recon 2, Iter 2600, Loss: 1.471095\n",
      "Sample 6, Recon 2, Iter 2700, Loss: 1.429798\n",
      "Sample 6, Recon 2, Iter 2800, Loss: 1.391044\n",
      "Sample 6, Recon 2, Iter 2900, Loss: 1.354322\n",
      "Sample 6, Recon 2, Iter 3000, Loss: 1.318984\n",
      "Sample 6, Recon 2, Iter 3100, Loss: 1.285609\n",
      "Sample 6, Recon 2, Iter 3200, Loss: 1.253852\n",
      "Sample 6, Recon 2, Iter 3300, Loss: 1.223510\n",
      "Sample 6, Recon 2, Iter 3400, Loss: 1.194455\n",
      "Sample 6, Recon 2, Iter 3500, Loss: 1.167835\n",
      "Sample 6, Recon 2, Iter 3600, Loss: 1.141068\n",
      "Sample 6, Recon 2, Iter 3700, Loss: 1.116803\n",
      "Sample 6, Recon 2, Iter 3800, Loss: 1.093226\n",
      "Sample 6, Recon 2, Iter 3900, Loss: 1.070695\n",
      "Sample 6, Recon 2, Iter 4000, Loss: 1.049438\n",
      "Sample 6, Recon 2, Iter 4100, Loss: 1.030141\n",
      "Sample 6, Recon 2, Iter 4200, Loss: 1.012385\n",
      "Sample 6, Recon 2, Iter 4300, Loss: 0.994099\n",
      "Sample 6, Recon 2, Iter 4400, Loss: 0.977415\n",
      "Sample 6, Recon 2, Iter 4500, Loss: 0.961679\n",
      "Sample 6, Recon 2, Iter 4600, Loss: 0.946984\n",
      "Sample 6, Recon 2, Iter 4700, Loss: 0.932302\n",
      "Sample 6, Recon 2, Iter 4800, Loss: 0.918385\n",
      "Sample 6, Recon 2, Iter 4900, Loss: 0.905099\n",
      "Sample 6, Recon 2, Iter 5000, Loss: 0.892315\n",
      "Sample 6, Recon 2, Iter 5100, Loss: 0.880516\n",
      "Sample 6, Recon 2, Iter 5200, Loss: 0.870702\n",
      "Sample 6, Recon 2, Iter 5300, Loss: 0.858271\n",
      "Sample 6, Recon 2, Iter 5400, Loss: 0.847769\n",
      "Sample 6, Recon 2, Iter 5500, Loss: 0.837394\n",
      "Sample 6, Recon 2, Iter 5600, Loss: 0.828855\n",
      "Sample 6, Recon 2, Iter 5700, Loss: 0.819455\n",
      "Sample 6, Recon 2, Iter 5800, Loss: 0.809266\n",
      "Sample 6, Recon 2, Iter 5900, Loss: 0.801482\n",
      "Sample 6, Recon 2, Iter 6000, Loss: 0.792995\n",
      "Sample 6, Recon 2, Iter 6100, Loss: 0.785182\n",
      "Sample 6, Recon 2, Iter 6200, Loss: 0.776611\n",
      "Sample 6, Recon 2, Iter 6300, Loss: 0.768942\n",
      "Sample 6, Recon 2, Iter 6400, Loss: 0.761954\n",
      "Sample 6, Recon 2, Iter 6500, Loss: 0.754104\n",
      "Sample 6, Recon 2, Iter 6600, Loss: 0.746996\n",
      "Sample 6, Recon 2, Iter 6700, Loss: 0.741526\n",
      "Sample 6, Recon 2, Iter 6800, Loss: 0.733104\n",
      "Sample 6, Recon 2, Iter 6900, Loss: 0.729537\n",
      "Sample 6, Recon 2, Iter 7000, Loss: 0.720153\n",
      "Sample 6, Recon 2, Iter 7100, Loss: 0.713857\n",
      "Sample 6, Recon 2, Iter 7200, Loss: 0.708385\n",
      "Sample 6, Recon 2, Iter 7300, Loss: 0.701403\n",
      "Sample 6, Recon 2, Iter 7400, Loss: 0.695244\n",
      "Sample 6, Recon 2, Iter 7500, Loss: 0.689212\n",
      "Sample 6, Recon 2, Iter 7600, Loss: 0.683145\n",
      "Sample 6, Recon 2, Iter 7700, Loss: 0.679286\n",
      "Sample 6, Recon 2, Iter 7800, Loss: 0.671895\n",
      "Sample 6, Recon 2, Iter 7900, Loss: 0.666540\n",
      "Sample 6, Recon 2, Iter 8000, Loss: 0.660705\n",
      "Sample 6, Recon 2, Iter 8100, Loss: 0.655467\n",
      "Sample 6, Recon 2, Iter 8200, Loss: 0.651119\n",
      "Sample 6, Recon 2, Iter 8300, Loss: 0.645588\n",
      "Sample 6, Recon 2, Iter 8400, Loss: 0.640405\n",
      "Sample 6, Recon 2, Iter 8500, Loss: 0.637483\n",
      "Sample 6, Recon 2, Iter 8600, Loss: 0.631260\n",
      "Sample 6, Recon 2, Iter 8700, Loss: 0.626500\n",
      "Sample 6, Recon 2, Iter 8800, Loss: 0.621683\n",
      "Sample 6, Recon 2, Iter 8900, Loss: 0.617572\n",
      "Sample 6, Recon 2, Iter 9000, Loss: 0.612700\n",
      "Sample 6, Recon 2, Iter 9100, Loss: 0.608797\n",
      "Sample 6, Recon 2, Iter 9200, Loss: 0.604483\n",
      "Sample 6, Recon 2, Iter 9300, Loss: 0.600121\n",
      "Sample 6, Recon 2, Iter 9400, Loss: 0.596220\n",
      "Sample 6, Recon 2, Iter 9500, Loss: 0.592777\n",
      "Sample 6, Recon 2, Iter 9600, Loss: 0.591358\n",
      "Sample 6, Recon 2, Iter 9700, Loss: 0.584996\n",
      "Sample 6, Recon 2, Iter 9800, Loss: 0.581280\n",
      "Sample 6, Recon 2, Iter 9900, Loss: 0.578108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:  70%|███████   | 7/10 [4:44:23<2:02:16, 2445.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 7, Recon 0, Iter 0, Loss: 22.527262\n",
      "Sample 7, Recon 0, Iter 100, Loss: 6.003623\n",
      "Sample 7, Recon 0, Iter 200, Loss: 4.255674\n",
      "Sample 7, Recon 0, Iter 300, Loss: 3.521397\n",
      "Sample 7, Recon 0, Iter 400, Loss: 3.137603\n",
      "Sample 7, Recon 0, Iter 500, Loss: 2.909885\n",
      "Sample 7, Recon 0, Iter 600, Loss: 2.745500\n",
      "Sample 7, Recon 0, Iter 700, Loss: 2.611931\n",
      "Sample 7, Recon 0, Iter 800, Loss: 2.494111\n",
      "Sample 7, Recon 0, Iter 900, Loss: 2.387814\n",
      "Sample 7, Recon 0, Iter 1000, Loss: 2.291152\n",
      "Sample 7, Recon 0, Iter 1100, Loss: 2.201910\n",
      "Sample 7, Recon 0, Iter 1200, Loss: 2.119109\n",
      "Sample 7, Recon 0, Iter 1300, Loss: 2.042385\n",
      "Sample 7, Recon 0, Iter 1400, Loss: 1.971725\n",
      "Sample 7, Recon 0, Iter 1500, Loss: 1.906331\n",
      "Sample 7, Recon 0, Iter 1600, Loss: 1.846004\n",
      "Sample 7, Recon 0, Iter 1700, Loss: 1.789231\n",
      "Sample 7, Recon 0, Iter 1800, Loss: 1.735435\n",
      "Sample 7, Recon 0, Iter 1900, Loss: 1.684183\n",
      "Sample 7, Recon 0, Iter 2000, Loss: 1.635075\n",
      "Sample 7, Recon 0, Iter 2100, Loss: 1.588173\n",
      "Sample 7, Recon 0, Iter 2200, Loss: 1.542927\n",
      "Sample 7, Recon 0, Iter 2300, Loss: 1.499036\n",
      "Sample 7, Recon 0, Iter 2400, Loss: 1.456601\n",
      "Sample 7, Recon 0, Iter 2500, Loss: 1.415425\n",
      "Sample 7, Recon 0, Iter 2600, Loss: 1.375911\n",
      "Sample 7, Recon 0, Iter 2700, Loss: 1.337500\n",
      "Sample 7, Recon 0, Iter 2800, Loss: 1.300765\n",
      "Sample 7, Recon 0, Iter 2900, Loss: 1.265565\n",
      "Sample 7, Recon 0, Iter 3000, Loss: 1.231483\n",
      "Sample 7, Recon 0, Iter 3100, Loss: 1.198810\n",
      "Sample 7, Recon 0, Iter 3200, Loss: 1.167266\n",
      "Sample 7, Recon 0, Iter 3300, Loss: 1.136512\n",
      "Sample 7, Recon 0, Iter 3400, Loss: 1.106735\n",
      "Sample 7, Recon 0, Iter 3500, Loss: 1.078520\n",
      "Sample 7, Recon 0, Iter 3600, Loss: 1.050766\n",
      "Sample 7, Recon 0, Iter 3700, Loss: 1.024385\n",
      "Sample 7, Recon 0, Iter 3800, Loss: 0.999149\n",
      "Sample 7, Recon 0, Iter 3900, Loss: 0.974760\n",
      "Sample 7, Recon 0, Iter 4000, Loss: 0.953700\n",
      "Sample 7, Recon 0, Iter 4100, Loss: 0.931192\n",
      "Sample 7, Recon 0, Iter 4200, Loss: 0.913224\n",
      "Sample 7, Recon 0, Iter 4300, Loss: 0.892751\n",
      "Sample 7, Recon 0, Iter 4400, Loss: 0.873867\n",
      "Sample 7, Recon 0, Iter 4500, Loss: 0.857212\n",
      "Sample 7, Recon 0, Iter 4600, Loss: 0.840215\n",
      "Sample 7, Recon 0, Iter 4700, Loss: 0.824527\n",
      "Sample 7, Recon 0, Iter 4800, Loss: 0.819527\n",
      "Sample 7, Recon 0, Iter 4900, Loss: 0.797201\n",
      "Sample 7, Recon 0, Iter 5000, Loss: 0.783194\n",
      "Sample 7, Recon 0, Iter 5100, Loss: 0.769283\n",
      "Sample 7, Recon 0, Iter 5200, Loss: 0.755677\n",
      "Sample 7, Recon 0, Iter 5300, Loss: 0.742456\n",
      "Sample 7, Recon 0, Iter 5400, Loss: 0.729338\n",
      "Sample 7, Recon 0, Iter 5500, Loss: 0.716561\n",
      "Sample 7, Recon 0, Iter 5600, Loss: 0.704346\n",
      "Sample 7, Recon 0, Iter 5700, Loss: 0.691983\n",
      "Sample 7, Recon 0, Iter 5800, Loss: 0.679612\n",
      "Sample 7, Recon 0, Iter 5900, Loss: 0.667992\n",
      "Sample 7, Recon 0, Iter 6000, Loss: 0.656789\n",
      "Sample 7, Recon 0, Iter 6100, Loss: 0.646792\n",
      "Sample 7, Recon 0, Iter 6200, Loss: 0.635154\n",
      "Sample 7, Recon 0, Iter 6300, Loss: 0.624712\n",
      "Sample 7, Recon 0, Iter 6400, Loss: 0.614539\n",
      "Sample 7, Recon 0, Iter 6500, Loss: 0.604830\n",
      "Sample 7, Recon 0, Iter 6600, Loss: 0.596475\n",
      "Sample 7, Recon 0, Iter 6700, Loss: 0.586410\n",
      "Sample 7, Recon 0, Iter 6800, Loss: 0.577252\n",
      "Sample 7, Recon 0, Iter 6900, Loss: 0.568667\n",
      "Sample 7, Recon 0, Iter 7000, Loss: 0.560210\n",
      "Sample 7, Recon 0, Iter 7100, Loss: 0.551857\n",
      "Sample 7, Recon 0, Iter 7200, Loss: 0.544003\n",
      "Sample 7, Recon 0, Iter 7300, Loss: 0.536601\n",
      "Sample 7, Recon 0, Iter 7400, Loss: 0.529379\n",
      "Sample 7, Recon 0, Iter 7500, Loss: 0.523222\n",
      "Sample 7, Recon 0, Iter 7600, Loss: 0.515690\n",
      "Sample 7, Recon 0, Iter 7700, Loss: 0.509018\n",
      "Sample 7, Recon 0, Iter 7800, Loss: 0.503015\n",
      "Sample 7, Recon 0, Iter 7900, Loss: 0.496658\n",
      "Sample 7, Recon 0, Iter 8000, Loss: 0.491058\n",
      "Sample 7, Recon 0, Iter 8100, Loss: 0.484926\n",
      "Sample 7, Recon 0, Iter 8200, Loss: 0.478755\n",
      "Sample 7, Recon 0, Iter 8300, Loss: 0.473233\n",
      "Sample 7, Recon 0, Iter 8400, Loss: 0.468284\n",
      "Sample 7, Recon 0, Iter 8500, Loss: 0.463614\n",
      "Sample 7, Recon 0, Iter 8600, Loss: 0.457684\n",
      "Sample 7, Recon 0, Iter 8700, Loss: 0.452701\n",
      "Sample 7, Recon 0, Iter 8800, Loss: 0.447972\n",
      "Sample 7, Recon 0, Iter 8900, Loss: 0.443425\n",
      "Sample 7, Recon 0, Iter 9000, Loss: 0.438959\n",
      "Sample 7, Recon 0, Iter 9100, Loss: 0.435102\n",
      "Sample 7, Recon 0, Iter 9200, Loss: 0.431014\n",
      "Sample 7, Recon 0, Iter 9300, Loss: 0.426585\n",
      "Sample 7, Recon 0, Iter 9400, Loss: 0.422575\n",
      "Sample 7, Recon 0, Iter 9500, Loss: 0.418787\n",
      "Sample 7, Recon 0, Iter 9600, Loss: 0.415154\n",
      "Sample 7, Recon 0, Iter 9700, Loss: 0.411706\n",
      "Sample 7, Recon 0, Iter 9800, Loss: 0.408184\n",
      "Sample 7, Recon 0, Iter 9900, Loss: 0.404532\n",
      "Sample 7, Recon 1, Iter 0, Loss: 19.554367\n",
      "Sample 7, Recon 1, Iter 100, Loss: 5.676753\n",
      "Sample 7, Recon 1, Iter 200, Loss: 4.164993\n",
      "Sample 7, Recon 1, Iter 300, Loss: 3.354310\n",
      "Sample 7, Recon 1, Iter 400, Loss: 3.026551\n",
      "Sample 7, Recon 1, Iter 500, Loss: 2.865574\n",
      "Sample 7, Recon 1, Iter 600, Loss: 2.742679\n",
      "Sample 7, Recon 1, Iter 700, Loss: 2.632823\n",
      "Sample 7, Recon 1, Iter 800, Loss: 2.533700\n",
      "Sample 7, Recon 1, Iter 900, Loss: 2.442312\n",
      "Sample 7, Recon 1, Iter 1000, Loss: 2.355283\n",
      "Sample 7, Recon 1, Iter 1100, Loss: 2.268440\n",
      "Sample 7, Recon 1, Iter 1200, Loss: 2.179421\n",
      "Sample 7, Recon 1, Iter 1300, Loss: 2.091550\n",
      "Sample 7, Recon 1, Iter 1400, Loss: 2.008199\n",
      "Sample 7, Recon 1, Iter 1500, Loss: 1.931344\n",
      "Sample 7, Recon 1, Iter 1600, Loss: 1.861927\n",
      "Sample 7, Recon 1, Iter 1700, Loss: 1.799004\n",
      "Sample 7, Recon 1, Iter 1800, Loss: 1.741058\n",
      "Sample 7, Recon 1, Iter 1900, Loss: 1.686858\n",
      "Sample 7, Recon 1, Iter 2000, Loss: 1.635508\n",
      "Sample 7, Recon 1, Iter 2100, Loss: 1.586590\n",
      "Sample 7, Recon 1, Iter 2200, Loss: 1.539863\n",
      "Sample 7, Recon 1, Iter 2300, Loss: 1.494956\n",
      "Sample 7, Recon 1, Iter 2400, Loss: 1.451869\n",
      "Sample 7, Recon 1, Iter 2500, Loss: 1.410241\n",
      "Sample 7, Recon 1, Iter 2600, Loss: 1.370226\n",
      "Sample 7, Recon 1, Iter 2700, Loss: 1.331617\n",
      "Sample 7, Recon 1, Iter 2800, Loss: 1.294331\n",
      "Sample 7, Recon 1, Iter 2900, Loss: 1.258325\n",
      "Sample 7, Recon 1, Iter 3000, Loss: 1.223514\n",
      "Sample 7, Recon 1, Iter 3100, Loss: 1.189962\n",
      "Sample 7, Recon 1, Iter 3200, Loss: 1.157715\n",
      "Sample 7, Recon 1, Iter 3300, Loss: 1.126649\n",
      "Sample 7, Recon 1, Iter 3400, Loss: 1.096458\n",
      "Sample 7, Recon 1, Iter 3500, Loss: 1.067390\n",
      "Sample 7, Recon 1, Iter 3600, Loss: 1.039598\n",
      "Sample 7, Recon 1, Iter 3700, Loss: 1.012657\n",
      "Sample 7, Recon 1, Iter 3800, Loss: 0.987135\n",
      "Sample 7, Recon 1, Iter 3900, Loss: 0.962502\n",
      "Sample 7, Recon 1, Iter 4000, Loss: 0.938979\n",
      "Sample 7, Recon 1, Iter 4100, Loss: 0.916984\n",
      "Sample 7, Recon 1, Iter 4200, Loss: 0.896129\n",
      "Sample 7, Recon 1, Iter 4300, Loss: 0.876522\n",
      "Sample 7, Recon 1, Iter 4400, Loss: 0.857701\n",
      "Sample 7, Recon 1, Iter 4500, Loss: 0.840058\n",
      "Sample 7, Recon 1, Iter 4600, Loss: 0.823509\n",
      "Sample 7, Recon 1, Iter 4700, Loss: 0.807511\n",
      "Sample 7, Recon 1, Iter 4800, Loss: 0.792468\n",
      "Sample 7, Recon 1, Iter 4900, Loss: 0.778109\n",
      "Sample 7, Recon 1, Iter 5000, Loss: 0.764369\n",
      "Sample 7, Recon 1, Iter 5100, Loss: 0.750751\n",
      "Sample 7, Recon 1, Iter 5200, Loss: 0.738304\n",
      "Sample 7, Recon 1, Iter 5300, Loss: 0.726585\n",
      "Sample 7, Recon 1, Iter 5400, Loss: 0.715474\n",
      "Sample 7, Recon 1, Iter 5500, Loss: 0.704993\n",
      "Sample 7, Recon 1, Iter 5600, Loss: 0.695238\n",
      "Sample 7, Recon 1, Iter 5700, Loss: 0.685554\n",
      "Sample 7, Recon 1, Iter 5800, Loss: 0.676929\n",
      "Sample 7, Recon 1, Iter 5900, Loss: 0.667263\n",
      "Sample 7, Recon 1, Iter 6000, Loss: 0.659114\n",
      "Sample 7, Recon 1, Iter 6100, Loss: 0.650306\n",
      "Sample 7, Recon 1, Iter 6200, Loss: 0.642909\n",
      "Sample 7, Recon 1, Iter 6300, Loss: 0.634400\n",
      "Sample 7, Recon 1, Iter 6400, Loss: 0.627073\n",
      "Sample 7, Recon 1, Iter 6500, Loss: 0.620025\n",
      "Sample 7, Recon 1, Iter 6600, Loss: 0.613521\n",
      "Sample 7, Recon 1, Iter 6700, Loss: 0.607488\n",
      "Sample 7, Recon 1, Iter 6800, Loss: 0.601521\n",
      "Sample 7, Recon 1, Iter 6900, Loss: 0.595707\n",
      "Sample 7, Recon 1, Iter 7000, Loss: 0.590574\n",
      "Sample 7, Recon 1, Iter 7100, Loss: 0.584553\n",
      "Sample 7, Recon 1, Iter 7200, Loss: 0.580002\n",
      "Sample 7, Recon 1, Iter 7300, Loss: 0.574272\n",
      "Sample 7, Recon 1, Iter 7400, Loss: 0.569013\n",
      "Sample 7, Recon 1, Iter 7500, Loss: 0.564776\n",
      "Sample 7, Recon 1, Iter 7600, Loss: 0.560745\n",
      "Sample 7, Recon 1, Iter 7700, Loss: 0.555698\n",
      "Sample 7, Recon 1, Iter 7800, Loss: 0.552665\n",
      "Sample 7, Recon 1, Iter 7900, Loss: 0.546874\n",
      "Sample 7, Recon 1, Iter 8000, Loss: 0.542912\n",
      "Sample 7, Recon 1, Iter 8100, Loss: 0.539056\n",
      "Sample 7, Recon 1, Iter 8200, Loss: 0.535200\n",
      "Sample 7, Recon 1, Iter 8300, Loss: 0.531647\n",
      "Sample 7, Recon 1, Iter 8400, Loss: 0.528320\n",
      "Sample 7, Recon 1, Iter 8500, Loss: 0.525912\n",
      "Sample 7, Recon 1, Iter 8600, Loss: 0.521971\n",
      "Sample 7, Recon 1, Iter 8700, Loss: 0.517790\n",
      "Sample 7, Recon 1, Iter 8800, Loss: 0.514742\n",
      "Sample 7, Recon 1, Iter 8900, Loss: 0.511598\n",
      "Sample 7, Recon 1, Iter 9000, Loss: 0.508502\n",
      "Sample 7, Recon 1, Iter 9100, Loss: 0.506581\n",
      "Sample 7, Recon 1, Iter 9200, Loss: 0.502906\n",
      "Sample 7, Recon 1, Iter 9300, Loss: 0.500132\n",
      "Sample 7, Recon 1, Iter 9400, Loss: 0.497828\n",
      "Sample 7, Recon 1, Iter 9500, Loss: 0.494389\n",
      "Sample 7, Recon 1, Iter 9600, Loss: 0.492143\n",
      "Sample 7, Recon 1, Iter 9700, Loss: 0.491650\n",
      "Sample 7, Recon 1, Iter 9800, Loss: 0.486910\n",
      "Sample 7, Recon 1, Iter 9900, Loss: 0.485098\n",
      "Sample 7, Recon 2, Iter 0, Loss: 16.742052\n",
      "Sample 7, Recon 2, Iter 100, Loss: 5.112777\n",
      "Sample 7, Recon 2, Iter 200, Loss: 4.074427\n",
      "Sample 7, Recon 2, Iter 300, Loss: 3.515106\n",
      "Sample 7, Recon 2, Iter 400, Loss: 3.137155\n",
      "Sample 7, Recon 2, Iter 500, Loss: 2.900404\n",
      "Sample 7, Recon 2, Iter 600, Loss: 2.751007\n",
      "Sample 7, Recon 2, Iter 700, Loss: 2.633245\n",
      "Sample 7, Recon 2, Iter 800, Loss: 2.523845\n",
      "Sample 7, Recon 2, Iter 900, Loss: 2.417214\n",
      "Sample 7, Recon 2, Iter 1000, Loss: 2.312980\n",
      "Sample 7, Recon 2, Iter 1100, Loss: 2.209638\n",
      "Sample 7, Recon 2, Iter 1200, Loss: 2.106157\n",
      "Sample 7, Recon 2, Iter 1300, Loss: 1.999732\n",
      "Sample 7, Recon 2, Iter 1400, Loss: 1.886672\n",
      "Sample 7, Recon 2, Iter 1500, Loss: 1.803372\n",
      "Sample 7, Recon 2, Iter 1600, Loss: 1.733759\n",
      "Sample 7, Recon 2, Iter 1700, Loss: 1.670403\n",
      "Sample 7, Recon 2, Iter 1800, Loss: 1.611192\n",
      "Sample 7, Recon 2, Iter 1900, Loss: 1.555447\n",
      "Sample 7, Recon 2, Iter 2000, Loss: 1.502586\n",
      "Sample 7, Recon 2, Iter 2100, Loss: 1.452242\n",
      "Sample 7, Recon 2, Iter 2200, Loss: 1.404297\n",
      "Sample 7, Recon 2, Iter 2300, Loss: 1.358255\n",
      "Sample 7, Recon 2, Iter 2400, Loss: 1.314614\n",
      "Sample 7, Recon 2, Iter 2500, Loss: 1.272820\n",
      "Sample 7, Recon 2, Iter 2600, Loss: 1.233590\n",
      "Sample 7, Recon 2, Iter 2700, Loss: 1.196966\n",
      "Sample 7, Recon 2, Iter 2800, Loss: 1.160863\n",
      "Sample 7, Recon 2, Iter 2900, Loss: 1.127913\n",
      "Sample 7, Recon 2, Iter 3000, Loss: 1.096232\n",
      "Sample 7, Recon 2, Iter 3100, Loss: 1.065673\n",
      "Sample 7, Recon 2, Iter 3200, Loss: 1.036688\n",
      "Sample 7, Recon 2, Iter 3300, Loss: 1.009587\n",
      "Sample 7, Recon 2, Iter 3400, Loss: 0.983761\n",
      "Sample 7, Recon 2, Iter 3500, Loss: 0.958894\n",
      "Sample 7, Recon 2, Iter 3600, Loss: 0.935539\n",
      "Sample 7, Recon 2, Iter 3700, Loss: 0.913099\n",
      "Sample 7, Recon 2, Iter 3800, Loss: 0.891690\n",
      "Sample 7, Recon 2, Iter 3900, Loss: 0.871050\n",
      "Sample 7, Recon 2, Iter 4000, Loss: 0.851501\n",
      "Sample 7, Recon 2, Iter 4100, Loss: 0.832954\n",
      "Sample 7, Recon 2, Iter 4200, Loss: 0.815440\n",
      "Sample 7, Recon 2, Iter 4300, Loss: 0.798675\n",
      "Sample 7, Recon 2, Iter 4400, Loss: 0.783655\n",
      "Sample 7, Recon 2, Iter 4500, Loss: 0.767689\n",
      "Sample 7, Recon 2, Iter 4600, Loss: 0.753205\n",
      "Sample 7, Recon 2, Iter 4700, Loss: 0.739784\n",
      "Sample 7, Recon 2, Iter 4800, Loss: 0.727655\n",
      "Sample 7, Recon 2, Iter 4900, Loss: 0.715037\n",
      "Sample 7, Recon 2, Iter 5000, Loss: 0.703403\n",
      "Sample 7, Recon 2, Iter 5100, Loss: 0.693805\n",
      "Sample 7, Recon 2, Iter 5200, Loss: 0.684910\n",
      "Sample 7, Recon 2, Iter 5300, Loss: 0.673340\n",
      "Sample 7, Recon 2, Iter 5400, Loss: 0.664479\n",
      "Sample 7, Recon 2, Iter 5500, Loss: 0.655552\n",
      "Sample 7, Recon 2, Iter 5600, Loss: 0.648534\n",
      "Sample 7, Recon 2, Iter 5700, Loss: 0.639234\n",
      "Sample 7, Recon 2, Iter 5800, Loss: 0.631668\n",
      "Sample 7, Recon 2, Iter 5900, Loss: 0.623760\n",
      "Sample 7, Recon 2, Iter 6000, Loss: 0.616733\n",
      "Sample 7, Recon 2, Iter 6100, Loss: 0.609833\n",
      "Sample 7, Recon 2, Iter 6200, Loss: 0.604518\n",
      "Sample 7, Recon 2, Iter 6300, Loss: 0.596484\n",
      "Sample 7, Recon 2, Iter 6400, Loss: 0.590733\n",
      "Sample 7, Recon 2, Iter 6500, Loss: 0.584606\n",
      "Sample 7, Recon 2, Iter 6600, Loss: 0.578908\n",
      "Sample 7, Recon 2, Iter 6700, Loss: 0.573980\n",
      "Sample 7, Recon 2, Iter 6800, Loss: 0.568944\n",
      "Sample 7, Recon 2, Iter 6900, Loss: 0.563107\n",
      "Sample 7, Recon 2, Iter 7000, Loss: 0.558165\n",
      "Sample 7, Recon 2, Iter 7100, Loss: 0.553243\n",
      "Sample 7, Recon 2, Iter 7200, Loss: 0.549327\n",
      "Sample 7, Recon 2, Iter 7300, Loss: 0.544731\n",
      "Sample 7, Recon 2, Iter 7400, Loss: 0.539891\n",
      "Sample 7, Recon 2, Iter 7500, Loss: 0.535601\n",
      "Sample 7, Recon 2, Iter 7600, Loss: 0.531514\n",
      "Sample 7, Recon 2, Iter 7700, Loss: 0.528613\n",
      "Sample 7, Recon 2, Iter 7800, Loss: 0.524007\n",
      "Sample 7, Recon 2, Iter 7900, Loss: 0.519946\n",
      "Sample 7, Recon 2, Iter 8000, Loss: 0.517948\n",
      "Sample 7, Recon 2, Iter 8100, Loss: 0.512718\n",
      "Sample 7, Recon 2, Iter 8200, Loss: 0.511441\n",
      "Sample 7, Recon 2, Iter 8300, Loss: 0.506070\n",
      "Sample 7, Recon 2, Iter 8400, Loss: 0.503804\n",
      "Sample 7, Recon 2, Iter 8500, Loss: 0.500855\n",
      "Sample 7, Recon 2, Iter 8600, Loss: 0.496949\n",
      "Sample 7, Recon 2, Iter 8700, Loss: 0.494974\n",
      "Sample 7, Recon 2, Iter 8800, Loss: 0.491633\n",
      "Sample 7, Recon 2, Iter 8900, Loss: 0.489182\n",
      "Sample 7, Recon 2, Iter 9000, Loss: 0.485241\n",
      "Sample 7, Recon 2, Iter 9100, Loss: 0.482526\n",
      "Sample 7, Recon 2, Iter 9200, Loss: 0.480018\n",
      "Sample 7, Recon 2, Iter 9300, Loss: 0.477807\n",
      "Sample 7, Recon 2, Iter 9400, Loss: 0.474848\n",
      "Sample 7, Recon 2, Iter 9500, Loss: 0.472164\n",
      "Sample 7, Recon 2, Iter 9600, Loss: 0.469627\n",
      "Sample 7, Recon 2, Iter 9700, Loss: 0.467686\n",
      "Sample 7, Recon 2, Iter 9800, Loss: 0.464966\n",
      "Sample 7, Recon 2, Iter 9900, Loss: 0.462412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:  80%|████████  | 8/10 [5:25:16<1:21:35, 2447.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 8, Recon 0, Iter 0, Loss: 15.227189\n",
      "Sample 8, Recon 0, Iter 100, Loss: 4.409702\n",
      "Sample 8, Recon 0, Iter 200, Loss: 3.831912\n",
      "Sample 8, Recon 0, Iter 300, Loss: 3.493089\n",
      "Sample 8, Recon 0, Iter 400, Loss: 3.194493\n",
      "Sample 8, Recon 0, Iter 500, Loss: 3.004460\n",
      "Sample 8, Recon 0, Iter 600, Loss: 2.870986\n",
      "Sample 8, Recon 0, Iter 700, Loss: 2.754467\n",
      "Sample 8, Recon 0, Iter 800, Loss: 2.637343\n",
      "Sample 8, Recon 0, Iter 900, Loss: 2.508688\n",
      "Sample 8, Recon 0, Iter 1000, Loss: 2.367514\n",
      "Sample 8, Recon 0, Iter 1100, Loss: 2.236184\n",
      "Sample 8, Recon 0, Iter 1200, Loss: 2.126427\n",
      "Sample 8, Recon 0, Iter 1300, Loss: 2.035750\n",
      "Sample 8, Recon 0, Iter 1400, Loss: 1.956011\n",
      "Sample 8, Recon 0, Iter 1500, Loss: 1.883764\n",
      "Sample 8, Recon 0, Iter 1600, Loss: 1.816899\n",
      "Sample 8, Recon 0, Iter 1700, Loss: 1.754778\n",
      "Sample 8, Recon 0, Iter 1800, Loss: 1.696709\n",
      "Sample 8, Recon 0, Iter 1900, Loss: 1.642534\n",
      "Sample 8, Recon 0, Iter 2000, Loss: 1.592100\n",
      "Sample 8, Recon 0, Iter 2100, Loss: 1.544813\n",
      "Sample 8, Recon 0, Iter 2200, Loss: 1.499993\n",
      "Sample 8, Recon 0, Iter 2300, Loss: 1.456855\n",
      "Sample 8, Recon 0, Iter 2400, Loss: 1.415708\n",
      "Sample 8, Recon 0, Iter 2500, Loss: 1.376170\n",
      "Sample 8, Recon 0, Iter 2600, Loss: 1.338536\n",
      "Sample 8, Recon 0, Iter 2700, Loss: 1.303048\n",
      "Sample 8, Recon 0, Iter 2800, Loss: 1.269333\n",
      "Sample 8, Recon 0, Iter 2900, Loss: 1.238175\n",
      "Sample 8, Recon 0, Iter 3000, Loss: 1.207603\n",
      "Sample 8, Recon 0, Iter 3100, Loss: 1.179805\n",
      "Sample 8, Recon 0, Iter 3200, Loss: 1.153259\n",
      "Sample 8, Recon 0, Iter 3300, Loss: 1.128239\n",
      "Sample 8, Recon 0, Iter 3400, Loss: 1.104947\n",
      "Sample 8, Recon 0, Iter 3500, Loss: 1.083345\n",
      "Sample 8, Recon 0, Iter 3600, Loss: 1.062405\n",
      "Sample 8, Recon 0, Iter 3700, Loss: 1.042623\n",
      "Sample 8, Recon 0, Iter 3800, Loss: 1.023835\n",
      "Sample 8, Recon 0, Iter 3900, Loss: 1.006591\n",
      "Sample 8, Recon 0, Iter 4000, Loss: 0.989462\n",
      "Sample 8, Recon 0, Iter 4100, Loss: 0.974358\n",
      "Sample 8, Recon 0, Iter 4200, Loss: 0.957958\n",
      "Sample 8, Recon 0, Iter 4300, Loss: 0.943294\n",
      "Sample 8, Recon 0, Iter 4400, Loss: 0.929017\n",
      "Sample 8, Recon 0, Iter 4500, Loss: 0.915877\n",
      "Sample 8, Recon 0, Iter 4600, Loss: 0.902844\n",
      "Sample 8, Recon 0, Iter 4700, Loss: 0.891122\n",
      "Sample 8, Recon 0, Iter 4800, Loss: 0.879129\n",
      "Sample 8, Recon 0, Iter 4900, Loss: 0.868276\n",
      "Sample 8, Recon 0, Iter 5000, Loss: 0.857830\n",
      "Sample 8, Recon 0, Iter 5100, Loss: 0.847386\n",
      "Sample 8, Recon 0, Iter 5200, Loss: 0.837608\n",
      "Sample 8, Recon 0, Iter 5300, Loss: 0.827897\n",
      "Sample 8, Recon 0, Iter 5400, Loss: 0.819566\n",
      "Sample 8, Recon 0, Iter 5500, Loss: 0.810284\n",
      "Sample 8, Recon 0, Iter 5600, Loss: 0.801907\n",
      "Sample 8, Recon 0, Iter 5700, Loss: 0.793736\n",
      "Sample 8, Recon 0, Iter 5800, Loss: 0.785551\n",
      "Sample 8, Recon 0, Iter 5900, Loss: 0.778562\n",
      "Sample 8, Recon 0, Iter 6000, Loss: 0.770533\n",
      "Sample 8, Recon 0, Iter 6100, Loss: 0.763113\n",
      "Sample 8, Recon 0, Iter 6200, Loss: 0.756655\n",
      "Sample 8, Recon 0, Iter 6300, Loss: 0.750799\n",
      "Sample 8, Recon 0, Iter 6400, Loss: 0.743016\n",
      "Sample 8, Recon 0, Iter 6500, Loss: 0.737134\n",
      "Sample 8, Recon 0, Iter 6600, Loss: 0.731325\n",
      "Sample 8, Recon 0, Iter 6700, Loss: 0.725201\n",
      "Sample 8, Recon 0, Iter 6800, Loss: 0.719531\n",
      "Sample 8, Recon 0, Iter 6900, Loss: 0.713521\n",
      "Sample 8, Recon 0, Iter 7000, Loss: 0.708150\n",
      "Sample 8, Recon 0, Iter 7100, Loss: 0.702731\n",
      "Sample 8, Recon 0, Iter 7200, Loss: 0.697605\n",
      "Sample 8, Recon 0, Iter 7300, Loss: 0.692344\n",
      "Sample 8, Recon 0, Iter 7400, Loss: 0.687565\n",
      "Sample 8, Recon 0, Iter 7500, Loss: 0.682702\n",
      "Sample 8, Recon 0, Iter 7600, Loss: 0.679271\n",
      "Sample 8, Recon 0, Iter 7700, Loss: 0.673670\n",
      "Sample 8, Recon 0, Iter 7800, Loss: 0.669028\n",
      "Sample 8, Recon 0, Iter 7900, Loss: 0.665004\n",
      "Sample 8, Recon 0, Iter 8000, Loss: 0.660802\n",
      "Sample 8, Recon 0, Iter 8100, Loss: 0.656549\n",
      "Sample 8, Recon 0, Iter 8200, Loss: 0.654599\n",
      "Sample 8, Recon 0, Iter 8300, Loss: 0.649652\n",
      "Sample 8, Recon 0, Iter 8400, Loss: 0.645425\n",
      "Sample 8, Recon 0, Iter 8500, Loss: 0.641357\n",
      "Sample 8, Recon 0, Iter 8600, Loss: 0.638892\n",
      "Sample 8, Recon 0, Iter 8700, Loss: 0.635560\n",
      "Sample 8, Recon 0, Iter 8800, Loss: 0.631024\n",
      "Sample 8, Recon 0, Iter 8900, Loss: 0.627084\n",
      "Sample 8, Recon 0, Iter 9000, Loss: 0.623669\n",
      "Sample 8, Recon 0, Iter 9100, Loss: 0.622509\n",
      "Sample 8, Recon 0, Iter 9200, Loss: 0.616919\n",
      "Sample 8, Recon 0, Iter 9300, Loss: 0.614670\n",
      "Sample 8, Recon 0, Iter 9400, Loss: 0.610765\n",
      "Sample 8, Recon 0, Iter 9500, Loss: 0.609039\n",
      "Sample 8, Recon 0, Iter 9600, Loss: 0.605186\n",
      "Sample 8, Recon 0, Iter 9700, Loss: 0.601616\n",
      "Sample 8, Recon 0, Iter 9800, Loss: 0.599148\n",
      "Sample 8, Recon 0, Iter 9900, Loss: 0.597477\n",
      "Sample 8, Recon 1, Iter 0, Loss: 24.870939\n",
      "Sample 8, Recon 1, Iter 100, Loss: 5.151982\n",
      "Sample 8, Recon 1, Iter 200, Loss: 4.410282\n",
      "Sample 8, Recon 1, Iter 300, Loss: 4.017370\n",
      "Sample 8, Recon 1, Iter 400, Loss: 3.697088\n",
      "Sample 8, Recon 1, Iter 500, Loss: 3.431345\n",
      "Sample 8, Recon 1, Iter 600, Loss: 3.215919\n",
      "Sample 8, Recon 1, Iter 700, Loss: 3.068792\n",
      "Sample 8, Recon 1, Iter 800, Loss: 2.951685\n",
      "Sample 8, Recon 1, Iter 900, Loss: 2.850077\n",
      "Sample 8, Recon 1, Iter 1000, Loss: 2.755736\n",
      "Sample 8, Recon 1, Iter 1100, Loss: 2.661853\n",
      "Sample 8, Recon 1, Iter 1200, Loss: 2.561774\n",
      "Sample 8, Recon 1, Iter 1300, Loss: 2.451747\n",
      "Sample 8, Recon 1, Iter 1400, Loss: 2.336071\n",
      "Sample 8, Recon 1, Iter 1500, Loss: 2.229252\n",
      "Sample 8, Recon 1, Iter 1600, Loss: 2.134505\n",
      "Sample 8, Recon 1, Iter 1700, Loss: 2.048597\n",
      "Sample 8, Recon 1, Iter 1800, Loss: 1.969089\n",
      "Sample 8, Recon 1, Iter 1900, Loss: 1.894603\n",
      "Sample 8, Recon 1, Iter 2000, Loss: 1.824677\n",
      "Sample 8, Recon 1, Iter 2100, Loss: 1.758272\n",
      "Sample 8, Recon 1, Iter 2200, Loss: 1.695650\n",
      "Sample 8, Recon 1, Iter 2300, Loss: 1.636995\n",
      "Sample 8, Recon 1, Iter 2400, Loss: 1.582295\n",
      "Sample 8, Recon 1, Iter 2500, Loss: 1.530915\n",
      "Sample 8, Recon 1, Iter 2600, Loss: 1.482937\n",
      "Sample 8, Recon 1, Iter 2700, Loss: 1.438131\n",
      "Sample 8, Recon 1, Iter 2800, Loss: 1.395859\n",
      "Sample 8, Recon 1, Iter 2900, Loss: 1.355424\n",
      "Sample 8, Recon 1, Iter 3000, Loss: 1.317232\n",
      "Sample 8, Recon 1, Iter 3100, Loss: 1.281559\n",
      "Sample 8, Recon 1, Iter 3200, Loss: 1.247241\n",
      "Sample 8, Recon 1, Iter 3300, Loss: 1.215107\n",
      "Sample 8, Recon 1, Iter 3400, Loss: 1.186113\n",
      "Sample 8, Recon 1, Iter 3500, Loss: 1.156408\n",
      "Sample 8, Recon 1, Iter 3600, Loss: 1.129466\n",
      "Sample 8, Recon 1, Iter 3700, Loss: 1.105015\n",
      "Sample 8, Recon 1, Iter 3800, Loss: 1.080640\n",
      "Sample 8, Recon 1, Iter 3900, Loss: 1.058331\n",
      "Sample 8, Recon 1, Iter 4000, Loss: 1.036673\n",
      "Sample 8, Recon 1, Iter 4100, Loss: 1.016847\n",
      "Sample 8, Recon 1, Iter 4200, Loss: 0.997999\n",
      "Sample 8, Recon 1, Iter 4300, Loss: 0.980304\n",
      "Sample 8, Recon 1, Iter 4400, Loss: 0.964981\n",
      "Sample 8, Recon 1, Iter 4500, Loss: 0.948824\n",
      "Sample 8, Recon 1, Iter 4600, Loss: 0.931806\n",
      "Sample 8, Recon 1, Iter 4700, Loss: 0.917401\n",
      "Sample 8, Recon 1, Iter 4800, Loss: 0.903318\n",
      "Sample 8, Recon 1, Iter 4900, Loss: 0.889872\n",
      "Sample 8, Recon 1, Iter 5000, Loss: 0.876462\n",
      "Sample 8, Recon 1, Iter 5100, Loss: 0.863715\n",
      "Sample 8, Recon 1, Iter 5200, Loss: 0.851357\n",
      "Sample 8, Recon 1, Iter 5300, Loss: 0.839430\n",
      "Sample 8, Recon 1, Iter 5400, Loss: 0.828222\n",
      "Sample 8, Recon 1, Iter 5500, Loss: 0.818211\n",
      "Sample 8, Recon 1, Iter 5600, Loss: 0.806639\n",
      "Sample 8, Recon 1, Iter 5700, Loss: 0.797172\n",
      "Sample 8, Recon 1, Iter 5800, Loss: 0.789406\n",
      "Sample 8, Recon 1, Iter 5900, Loss: 0.778170\n",
      "Sample 8, Recon 1, Iter 6000, Loss: 0.770054\n",
      "Sample 8, Recon 1, Iter 6100, Loss: 0.760128\n",
      "Sample 8, Recon 1, Iter 6200, Loss: 0.751485\n",
      "Sample 8, Recon 1, Iter 6300, Loss: 0.743166\n",
      "Sample 8, Recon 1, Iter 6400, Loss: 0.735302\n",
      "Sample 8, Recon 1, Iter 6500, Loss: 0.727135\n",
      "Sample 8, Recon 1, Iter 6600, Loss: 0.719739\n",
      "Sample 8, Recon 1, Iter 6700, Loss: 0.712618\n",
      "Sample 8, Recon 1, Iter 6800, Loss: 0.705628\n",
      "Sample 8, Recon 1, Iter 6900, Loss: 0.698708\n",
      "Sample 8, Recon 1, Iter 7000, Loss: 0.691798\n",
      "Sample 8, Recon 1, Iter 7100, Loss: 0.684816\n",
      "Sample 8, Recon 1, Iter 7200, Loss: 0.680124\n",
      "Sample 8, Recon 1, Iter 7300, Loss: 0.672182\n",
      "Sample 8, Recon 1, Iter 7400, Loss: 0.666504\n",
      "Sample 8, Recon 1, Iter 7500, Loss: 0.660159\n",
      "Sample 8, Recon 1, Iter 7600, Loss: 0.654288\n",
      "Sample 8, Recon 1, Iter 7700, Loss: 0.649225\n",
      "Sample 8, Recon 1, Iter 7800, Loss: 0.643621\n",
      "Sample 8, Recon 1, Iter 7900, Loss: 0.638378\n",
      "Sample 8, Recon 1, Iter 8000, Loss: 0.633439\n",
      "Sample 8, Recon 1, Iter 8100, Loss: 0.628187\n",
      "Sample 8, Recon 1, Iter 8200, Loss: 0.623308\n",
      "Sample 8, Recon 1, Iter 8300, Loss: 0.618644\n",
      "Sample 8, Recon 1, Iter 8400, Loss: 0.615296\n",
      "Sample 8, Recon 1, Iter 8500, Loss: 0.609967\n",
      "Sample 8, Recon 1, Iter 8600, Loss: 0.605263\n",
      "Sample 8, Recon 1, Iter 8700, Loss: 0.602398\n",
      "Sample 8, Recon 1, Iter 8800, Loss: 0.596825\n",
      "Sample 8, Recon 1, Iter 8900, Loss: 0.592695\n",
      "Sample 8, Recon 1, Iter 9000, Loss: 0.588731\n",
      "Sample 8, Recon 1, Iter 9100, Loss: 0.584798\n",
      "Sample 8, Recon 1, Iter 9200, Loss: 0.581636\n",
      "Sample 8, Recon 1, Iter 9300, Loss: 0.578052\n",
      "Sample 8, Recon 1, Iter 9400, Loss: 0.575372\n",
      "Sample 8, Recon 1, Iter 9500, Loss: 0.570697\n",
      "Sample 8, Recon 1, Iter 9600, Loss: 0.567483\n",
      "Sample 8, Recon 1, Iter 9700, Loss: 0.564654\n",
      "Sample 8, Recon 1, Iter 9800, Loss: 0.560854\n",
      "Sample 8, Recon 1, Iter 9900, Loss: 0.558482\n",
      "Sample 8, Recon 2, Iter 0, Loss: 17.120861\n",
      "Sample 8, Recon 2, Iter 100, Loss: 4.575165\n",
      "Sample 8, Recon 2, Iter 200, Loss: 3.774271\n",
      "Sample 8, Recon 2, Iter 300, Loss: 3.418830\n",
      "Sample 8, Recon 2, Iter 400, Loss: 3.218712\n",
      "Sample 8, Recon 2, Iter 500, Loss: 3.070475\n",
      "Sample 8, Recon 2, Iter 600, Loss: 2.943493\n",
      "Sample 8, Recon 2, Iter 700, Loss: 2.822715\n",
      "Sample 8, Recon 2, Iter 800, Loss: 2.698924\n",
      "Sample 8, Recon 2, Iter 900, Loss: 2.565987\n",
      "Sample 8, Recon 2, Iter 1000, Loss: 2.420064\n",
      "Sample 8, Recon 2, Iter 1100, Loss: 2.284489\n",
      "Sample 8, Recon 2, Iter 1200, Loss: 2.166842\n",
      "Sample 8, Recon 2, Iter 1300, Loss: 2.064548\n",
      "Sample 8, Recon 2, Iter 1400, Loss: 1.972811\n",
      "Sample 8, Recon 2, Iter 1500, Loss: 1.889078\n",
      "Sample 8, Recon 2, Iter 1600, Loss: 1.812387\n",
      "Sample 8, Recon 2, Iter 1700, Loss: 1.742185\n",
      "Sample 8, Recon 2, Iter 1800, Loss: 1.677329\n",
      "Sample 8, Recon 2, Iter 1900, Loss: 1.616892\n",
      "Sample 8, Recon 2, Iter 2000, Loss: 1.560323\n",
      "Sample 8, Recon 2, Iter 2100, Loss: 1.507281\n",
      "Sample 8, Recon 2, Iter 2200, Loss: 1.458055\n",
      "Sample 8, Recon 2, Iter 2300, Loss: 1.412295\n",
      "Sample 8, Recon 2, Iter 2400, Loss: 1.368650\n",
      "Sample 8, Recon 2, Iter 2500, Loss: 1.326852\n",
      "Sample 8, Recon 2, Iter 2600, Loss: 1.287667\n",
      "Sample 8, Recon 2, Iter 2700, Loss: 1.251078\n",
      "Sample 8, Recon 2, Iter 2800, Loss: 1.216571\n",
      "Sample 8, Recon 2, Iter 2900, Loss: 1.183587\n",
      "Sample 8, Recon 2, Iter 3000, Loss: 1.152482\n",
      "Sample 8, Recon 2, Iter 3100, Loss: 1.123133\n",
      "Sample 8, Recon 2, Iter 3200, Loss: 1.095484\n",
      "Sample 8, Recon 2, Iter 3300, Loss: 1.068667\n",
      "Sample 8, Recon 2, Iter 3400, Loss: 1.043495\n",
      "Sample 8, Recon 2, Iter 3500, Loss: 1.019566\n",
      "Sample 8, Recon 2, Iter 3600, Loss: 0.997226\n",
      "Sample 8, Recon 2, Iter 3700, Loss: 0.976130\n",
      "Sample 8, Recon 2, Iter 3800, Loss: 0.955181\n",
      "Sample 8, Recon 2, Iter 3900, Loss: 0.935717\n",
      "Sample 8, Recon 2, Iter 4000, Loss: 0.917779\n",
      "Sample 8, Recon 2, Iter 4100, Loss: 0.899568\n",
      "Sample 8, Recon 2, Iter 4200, Loss: 0.882127\n",
      "Sample 8, Recon 2, Iter 4300, Loss: 0.866602\n",
      "Sample 8, Recon 2, Iter 4400, Loss: 0.850244\n",
      "Sample 8, Recon 2, Iter 4500, Loss: 0.835474\n",
      "Sample 8, Recon 2, Iter 4600, Loss: 0.821774\n",
      "Sample 8, Recon 2, Iter 4700, Loss: 0.807947\n",
      "Sample 8, Recon 2, Iter 4800, Loss: 0.794966\n",
      "Sample 8, Recon 2, Iter 4900, Loss: 0.782770\n",
      "Sample 8, Recon 2, Iter 5000, Loss: 0.771884\n",
      "Sample 8, Recon 2, Iter 5100, Loss: 0.760342\n",
      "Sample 8, Recon 2, Iter 5200, Loss: 0.749776\n",
      "Sample 8, Recon 2, Iter 5300, Loss: 0.739376\n",
      "Sample 8, Recon 2, Iter 5400, Loss: 0.729780\n",
      "Sample 8, Recon 2, Iter 5500, Loss: 0.720718\n",
      "Sample 8, Recon 2, Iter 5600, Loss: 0.711273\n",
      "Sample 8, Recon 2, Iter 5700, Loss: 0.704510\n",
      "Sample 8, Recon 2, Iter 5800, Loss: 0.695788\n",
      "Sample 8, Recon 2, Iter 5900, Loss: 0.689574\n",
      "Sample 8, Recon 2, Iter 6000, Loss: 0.682471\n",
      "Sample 8, Recon 2, Iter 6100, Loss: 0.675922\n",
      "Sample 8, Recon 2, Iter 6200, Loss: 0.669527\n",
      "Sample 8, Recon 2, Iter 6300, Loss: 0.664157\n",
      "Sample 8, Recon 2, Iter 6400, Loss: 0.657071\n",
      "Sample 8, Recon 2, Iter 6500, Loss: 0.651400\n",
      "Sample 8, Recon 2, Iter 6600, Loss: 0.645815\n",
      "Sample 8, Recon 2, Iter 6700, Loss: 0.640517\n",
      "Sample 8, Recon 2, Iter 6800, Loss: 0.635300\n",
      "Sample 8, Recon 2, Iter 6900, Loss: 0.637493\n",
      "Sample 8, Recon 2, Iter 7000, Loss: 0.628464\n",
      "Sample 8, Recon 2, Iter 7100, Loss: 0.622578\n",
      "Sample 8, Recon 2, Iter 7200, Loss: 0.618302\n",
      "Sample 8, Recon 2, Iter 7300, Loss: 0.612319\n",
      "Sample 8, Recon 2, Iter 7400, Loss: 0.608645\n",
      "Sample 8, Recon 2, Iter 7500, Loss: 0.602758\n",
      "Sample 8, Recon 2, Iter 7600, Loss: 0.597880\n",
      "Sample 8, Recon 2, Iter 7700, Loss: 0.593092\n",
      "Sample 8, Recon 2, Iter 7800, Loss: 0.589328\n",
      "Sample 8, Recon 2, Iter 7900, Loss: 0.584225\n",
      "Sample 8, Recon 2, Iter 8000, Loss: 0.579870\n",
      "Sample 8, Recon 2, Iter 8100, Loss: 0.575442\n",
      "Sample 8, Recon 2, Iter 8200, Loss: 0.570550\n",
      "Sample 8, Recon 2, Iter 8300, Loss: 0.566214\n",
      "Sample 8, Recon 2, Iter 8400, Loss: 0.561822\n",
      "Sample 8, Recon 2, Iter 8500, Loss: 0.557894\n",
      "Sample 8, Recon 2, Iter 8600, Loss: 0.553471\n",
      "Sample 8, Recon 2, Iter 8700, Loss: 0.549617\n",
      "Sample 8, Recon 2, Iter 8800, Loss: 0.545954\n",
      "Sample 8, Recon 2, Iter 8900, Loss: 0.541748\n",
      "Sample 8, Recon 2, Iter 9000, Loss: 0.537968\n",
      "Sample 8, Recon 2, Iter 9100, Loss: 0.534360\n",
      "Sample 8, Recon 2, Iter 9200, Loss: 0.530676\n",
      "Sample 8, Recon 2, Iter 9300, Loss: 0.527273\n",
      "Sample 8, Recon 2, Iter 9400, Loss: 0.523691\n",
      "Sample 8, Recon 2, Iter 9500, Loss: 0.520478\n",
      "Sample 8, Recon 2, Iter 9600, Loss: 0.517075\n",
      "Sample 8, Recon 2, Iter 9700, Loss: 0.513586\n",
      "Sample 8, Recon 2, Iter 9800, Loss: 0.510874\n",
      "Sample 8, Recon 2, Iter 9900, Loss: 0.508323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:  90%|█████████ | 9/10 [6:06:07<40:48, 2448.88s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9, Recon 0, Iter 0, Loss: 20.640848\n",
      "Sample 9, Recon 0, Iter 100, Loss: 5.178938\n",
      "Sample 9, Recon 0, Iter 200, Loss: 4.053443\n",
      "Sample 9, Recon 0, Iter 300, Loss: 3.619526\n",
      "Sample 9, Recon 0, Iter 400, Loss: 3.291709\n",
      "Sample 9, Recon 0, Iter 500, Loss: 3.033714\n",
      "Sample 9, Recon 0, Iter 600, Loss: 2.857104\n",
      "Sample 9, Recon 0, Iter 700, Loss: 2.701177\n",
      "Sample 9, Recon 0, Iter 800, Loss: 2.563535\n",
      "Sample 9, Recon 0, Iter 900, Loss: 2.440037\n",
      "Sample 9, Recon 0, Iter 1000, Loss: 2.327774\n",
      "Sample 9, Recon 0, Iter 1100, Loss: 2.226061\n",
      "Sample 9, Recon 0, Iter 1200, Loss: 2.134440\n",
      "Sample 9, Recon 0, Iter 1300, Loss: 2.051748\n",
      "Sample 9, Recon 0, Iter 1400, Loss: 1.976199\n",
      "Sample 9, Recon 0, Iter 1500, Loss: 1.906599\n",
      "Sample 9, Recon 0, Iter 1600, Loss: 1.841429\n",
      "Sample 9, Recon 0, Iter 1700, Loss: 1.779921\n",
      "Sample 9, Recon 0, Iter 1800, Loss: 1.722325\n",
      "Sample 9, Recon 0, Iter 1900, Loss: 1.668058\n",
      "Sample 9, Recon 0, Iter 2000, Loss: 1.616875\n",
      "Sample 9, Recon 0, Iter 2100, Loss: 1.568705\n",
      "Sample 9, Recon 0, Iter 2200, Loss: 1.523333\n",
      "Sample 9, Recon 0, Iter 2300, Loss: 1.480362\n",
      "Sample 9, Recon 0, Iter 2400, Loss: 1.439438\n",
      "Sample 9, Recon 0, Iter 2500, Loss: 1.400814\n",
      "Sample 9, Recon 0, Iter 2600, Loss: 1.363606\n",
      "Sample 9, Recon 0, Iter 2700, Loss: 1.328748\n",
      "Sample 9, Recon 0, Iter 2800, Loss: 1.295670\n",
      "Sample 9, Recon 0, Iter 2900, Loss: 1.264155\n",
      "Sample 9, Recon 0, Iter 3000, Loss: 1.234303\n",
      "Sample 9, Recon 0, Iter 3100, Loss: 1.205690\n",
      "Sample 9, Recon 0, Iter 3200, Loss: 1.178475\n",
      "Sample 9, Recon 0, Iter 3300, Loss: 1.151906\n",
      "Sample 9, Recon 0, Iter 3400, Loss: 1.126999\n",
      "Sample 9, Recon 0, Iter 3500, Loss: 1.102742\n",
      "Sample 9, Recon 0, Iter 3600, Loss: 1.079679\n",
      "Sample 9, Recon 0, Iter 3700, Loss: 1.056963\n",
      "Sample 9, Recon 0, Iter 3800, Loss: 1.035128\n",
      "Sample 9, Recon 0, Iter 3900, Loss: 1.014075\n",
      "Sample 9, Recon 0, Iter 4000, Loss: 0.993718\n",
      "Sample 9, Recon 0, Iter 4100, Loss: 0.973790\n",
      "Sample 9, Recon 0, Iter 4200, Loss: 0.954468\n",
      "Sample 9, Recon 0, Iter 4300, Loss: 0.936051\n",
      "Sample 9, Recon 0, Iter 4400, Loss: 0.918351\n",
      "Sample 9, Recon 0, Iter 4500, Loss: 0.900914\n",
      "Sample 9, Recon 0, Iter 4600, Loss: 0.884520\n",
      "Sample 9, Recon 0, Iter 4700, Loss: 0.869085\n",
      "Sample 9, Recon 0, Iter 4800, Loss: 0.854481\n",
      "Sample 9, Recon 0, Iter 4900, Loss: 0.839937\n",
      "Sample 9, Recon 0, Iter 5000, Loss: 0.826882\n",
      "Sample 9, Recon 0, Iter 5100, Loss: 0.814638\n",
      "Sample 9, Recon 0, Iter 5200, Loss: 0.801182\n",
      "Sample 9, Recon 0, Iter 5300, Loss: 0.789878\n",
      "Sample 9, Recon 0, Iter 5400, Loss: 0.778590\n",
      "Sample 9, Recon 0, Iter 5500, Loss: 0.768781\n",
      "Sample 9, Recon 0, Iter 5600, Loss: 0.757723\n",
      "Sample 9, Recon 0, Iter 5700, Loss: 0.747735\n",
      "Sample 9, Recon 0, Iter 5800, Loss: 0.738434\n",
      "Sample 9, Recon 0, Iter 5900, Loss: 0.729454\n",
      "Sample 9, Recon 0, Iter 6000, Loss: 0.721052\n",
      "Sample 9, Recon 0, Iter 6100, Loss: 0.712811\n",
      "Sample 9, Recon 0, Iter 6200, Loss: 0.704986\n",
      "Sample 9, Recon 0, Iter 6300, Loss: 0.697223\n",
      "Sample 9, Recon 0, Iter 6400, Loss: 0.689740\n",
      "Sample 9, Recon 0, Iter 6500, Loss: 0.683666\n",
      "Sample 9, Recon 0, Iter 6600, Loss: 0.676222\n",
      "Sample 9, Recon 0, Iter 6700, Loss: 0.669544\n",
      "Sample 9, Recon 0, Iter 6800, Loss: 0.663990\n",
      "Sample 9, Recon 0, Iter 6900, Loss: 0.657103\n",
      "Sample 9, Recon 0, Iter 7000, Loss: 0.652275\n",
      "Sample 9, Recon 0, Iter 7100, Loss: 0.646045\n",
      "Sample 9, Recon 0, Iter 7200, Loss: 0.640111\n",
      "Sample 9, Recon 0, Iter 7300, Loss: 0.635129\n",
      "Sample 9, Recon 0, Iter 7400, Loss: 0.629915\n",
      "Sample 9, Recon 0, Iter 7500, Loss: 0.624683\n",
      "Sample 9, Recon 0, Iter 7600, Loss: 0.621423\n",
      "Sample 9, Recon 0, Iter 7700, Loss: 0.615157\n",
      "Sample 9, Recon 0, Iter 7800, Loss: 0.611040\n",
      "Sample 9, Recon 0, Iter 7900, Loss: 0.606099\n",
      "Sample 9, Recon 0, Iter 8000, Loss: 0.603600\n",
      "Sample 9, Recon 0, Iter 8100, Loss: 0.597760\n",
      "Sample 9, Recon 0, Iter 8200, Loss: 0.596809\n",
      "Sample 9, Recon 0, Iter 8300, Loss: 0.590134\n",
      "Sample 9, Recon 0, Iter 8400, Loss: 0.588051\n",
      "Sample 9, Recon 0, Iter 8500, Loss: 0.582646\n",
      "Sample 9, Recon 0, Iter 8600, Loss: 0.578833\n",
      "Sample 9, Recon 0, Iter 8700, Loss: 0.575635\n",
      "Sample 9, Recon 0, Iter 8800, Loss: 0.572121\n",
      "Sample 9, Recon 0, Iter 8900, Loss: 0.569241\n",
      "Sample 9, Recon 0, Iter 9000, Loss: 0.566969\n",
      "Sample 9, Recon 0, Iter 9100, Loss: 0.562381\n",
      "Sample 9, Recon 0, Iter 9200, Loss: 0.559799\n",
      "Sample 9, Recon 0, Iter 9300, Loss: 0.556579\n",
      "Sample 9, Recon 0, Iter 9400, Loss: 0.553164\n",
      "Sample 9, Recon 0, Iter 9500, Loss: 0.550161\n",
      "Sample 9, Recon 0, Iter 9600, Loss: 0.547324\n",
      "Sample 9, Recon 0, Iter 9700, Loss: 0.544587\n",
      "Sample 9, Recon 0, Iter 9800, Loss: 0.542476\n",
      "Sample 9, Recon 0, Iter 9900, Loss: 0.539461\n",
      "Sample 9, Recon 1, Iter 0, Loss: 15.923925\n",
      "Sample 9, Recon 1, Iter 100, Loss: 4.330895\n",
      "Sample 9, Recon 1, Iter 200, Loss: 3.577162\n",
      "Sample 9, Recon 1, Iter 300, Loss: 3.269478\n",
      "Sample 9, Recon 1, Iter 400, Loss: 3.057653\n",
      "Sample 9, Recon 1, Iter 500, Loss: 2.902304\n",
      "Sample 9, Recon 1, Iter 600, Loss: 2.770159\n",
      "Sample 9, Recon 1, Iter 700, Loss: 2.641355\n",
      "Sample 9, Recon 1, Iter 800, Loss: 2.508908\n",
      "Sample 9, Recon 1, Iter 900, Loss: 2.375814\n",
      "Sample 9, Recon 1, Iter 1000, Loss: 2.251948\n",
      "Sample 9, Recon 1, Iter 1100, Loss: 2.143088\n",
      "Sample 9, Recon 1, Iter 1200, Loss: 2.046896\n",
      "Sample 9, Recon 1, Iter 1300, Loss: 1.960995\n",
      "Sample 9, Recon 1, Iter 1400, Loss: 1.883414\n",
      "Sample 9, Recon 1, Iter 1500, Loss: 1.811789\n",
      "Sample 9, Recon 1, Iter 1600, Loss: 1.744835\n",
      "Sample 9, Recon 1, Iter 1700, Loss: 1.681787\n",
      "Sample 9, Recon 1, Iter 1800, Loss: 1.622070\n",
      "Sample 9, Recon 1, Iter 1900, Loss: 1.566278\n",
      "Sample 9, Recon 1, Iter 2000, Loss: 1.513683\n",
      "Sample 9, Recon 1, Iter 2100, Loss: 1.464276\n",
      "Sample 9, Recon 1, Iter 2200, Loss: 1.418139\n",
      "Sample 9, Recon 1, Iter 2300, Loss: 1.375501\n",
      "Sample 9, Recon 1, Iter 2400, Loss: 1.335878\n",
      "Sample 9, Recon 1, Iter 2500, Loss: 1.298757\n",
      "Sample 9, Recon 1, Iter 2600, Loss: 1.264718\n",
      "Sample 9, Recon 1, Iter 2700, Loss: 1.233066\n",
      "Sample 9, Recon 1, Iter 2800, Loss: 1.203080\n",
      "Sample 9, Recon 1, Iter 2900, Loss: 1.174725\n",
      "Sample 9, Recon 1, Iter 3000, Loss: 1.148328\n",
      "Sample 9, Recon 1, Iter 3100, Loss: 1.123368\n",
      "Sample 9, Recon 1, Iter 3200, Loss: 1.100406\n",
      "Sample 9, Recon 1, Iter 3300, Loss: 1.077631\n",
      "Sample 9, Recon 1, Iter 3400, Loss: 1.056881\n",
      "Sample 9, Recon 1, Iter 3500, Loss: 1.036703\n",
      "Sample 9, Recon 1, Iter 3600, Loss: 1.017910\n",
      "Sample 9, Recon 1, Iter 3700, Loss: 0.999446\n",
      "Sample 9, Recon 1, Iter 3800, Loss: 0.983967\n",
      "Sample 9, Recon 1, Iter 3900, Loss: 0.965402\n",
      "Sample 9, Recon 1, Iter 4000, Loss: 0.948862\n",
      "Sample 9, Recon 1, Iter 4100, Loss: 0.933459\n",
      "Sample 9, Recon 1, Iter 4200, Loss: 0.918552\n",
      "Sample 9, Recon 1, Iter 4300, Loss: 0.906293\n",
      "Sample 9, Recon 1, Iter 4400, Loss: 0.891263\n",
      "Sample 9, Recon 1, Iter 4500, Loss: 0.878164\n",
      "Sample 9, Recon 1, Iter 4600, Loss: 0.865834\n",
      "Sample 9, Recon 1, Iter 4700, Loss: 0.853841\n",
      "Sample 9, Recon 1, Iter 4800, Loss: 0.842420\n",
      "Sample 9, Recon 1, Iter 4900, Loss: 0.830812\n",
      "Sample 9, Recon 1, Iter 5000, Loss: 0.820325\n",
      "Sample 9, Recon 1, Iter 5100, Loss: 0.809493\n",
      "Sample 9, Recon 1, Iter 5200, Loss: 0.799006\n",
      "Sample 9, Recon 1, Iter 5300, Loss: 0.789450\n",
      "Sample 9, Recon 1, Iter 5400, Loss: 0.780231\n",
      "Sample 9, Recon 1, Iter 5500, Loss: 0.770791\n",
      "Sample 9, Recon 1, Iter 5600, Loss: 0.761976\n",
      "Sample 9, Recon 1, Iter 5700, Loss: 0.754666\n",
      "Sample 9, Recon 1, Iter 5800, Loss: 0.745349\n",
      "Sample 9, Recon 1, Iter 5900, Loss: 0.737376\n",
      "Sample 9, Recon 1, Iter 6000, Loss: 0.729601\n",
      "Sample 9, Recon 1, Iter 6100, Loss: 0.722252\n",
      "Sample 9, Recon 1, Iter 6200, Loss: 0.715235\n",
      "Sample 9, Recon 1, Iter 6300, Loss: 0.708466\n",
      "Sample 9, Recon 1, Iter 6400, Loss: 0.702017\n",
      "Sample 9, Recon 1, Iter 6500, Loss: 0.695352\n",
      "Sample 9, Recon 1, Iter 6600, Loss: 0.689535\n",
      "Sample 9, Recon 1, Iter 6700, Loss: 0.685873\n",
      "Sample 9, Recon 1, Iter 6800, Loss: 0.677763\n",
      "Sample 9, Recon 1, Iter 6900, Loss: 0.674679\n",
      "Sample 9, Recon 1, Iter 7000, Loss: 0.667079\n",
      "Sample 9, Recon 1, Iter 7100, Loss: 0.662281\n",
      "Sample 9, Recon 1, Iter 7200, Loss: 0.656543\n",
      "Sample 9, Recon 1, Iter 7300, Loss: 0.651454\n",
      "Sample 9, Recon 1, Iter 7400, Loss: 0.647506\n",
      "Sample 9, Recon 1, Iter 7500, Loss: 0.641791\n",
      "Sample 9, Recon 1, Iter 7600, Loss: 0.637491\n",
      "Sample 9, Recon 1, Iter 7700, Loss: 0.633420\n",
      "Sample 9, Recon 1, Iter 7800, Loss: 0.627558\n",
      "Sample 9, Recon 1, Iter 7900, Loss: 0.623322\n",
      "Sample 9, Recon 1, Iter 8000, Loss: 0.619285\n",
      "Sample 9, Recon 1, Iter 8100, Loss: 0.615471\n",
      "Sample 9, Recon 1, Iter 8200, Loss: 0.611189\n",
      "Sample 9, Recon 1, Iter 8300, Loss: 0.608203\n",
      "Sample 9, Recon 1, Iter 8400, Loss: 0.603343\n",
      "Sample 9, Recon 1, Iter 8500, Loss: 0.599604\n",
      "Sample 9, Recon 1, Iter 8600, Loss: 0.596119\n",
      "Sample 9, Recon 1, Iter 8700, Loss: 0.592564\n",
      "Sample 9, Recon 1, Iter 8800, Loss: 0.589163\n",
      "Sample 9, Recon 1, Iter 8900, Loss: 0.586134\n",
      "Sample 9, Recon 1, Iter 9000, Loss: 0.582817\n",
      "Sample 9, Recon 1, Iter 9100, Loss: 0.580393\n",
      "Sample 9, Recon 1, Iter 9200, Loss: 0.577582\n",
      "Sample 9, Recon 1, Iter 9300, Loss: 0.578409\n",
      "Sample 9, Recon 1, Iter 9400, Loss: 0.570260\n",
      "Sample 9, Recon 1, Iter 9500, Loss: 0.567180\n",
      "Sample 9, Recon 1, Iter 9600, Loss: 0.564598\n",
      "Sample 9, Recon 1, Iter 9700, Loss: 0.561985\n",
      "Sample 9, Recon 1, Iter 9800, Loss: 0.559201\n",
      "Sample 9, Recon 1, Iter 9900, Loss: 0.557462\n",
      "Sample 9, Recon 2, Iter 0, Loss: 18.736439\n",
      "Sample 9, Recon 2, Iter 100, Loss: 4.775609\n",
      "Sample 9, Recon 2, Iter 200, Loss: 3.681075\n",
      "Sample 9, Recon 2, Iter 300, Loss: 3.326539\n",
      "Sample 9, Recon 2, Iter 400, Loss: 3.095315\n",
      "Sample 9, Recon 2, Iter 500, Loss: 2.914892\n",
      "Sample 9, Recon 2, Iter 600, Loss: 2.774353\n",
      "Sample 9, Recon 2, Iter 700, Loss: 2.659294\n",
      "Sample 9, Recon 2, Iter 800, Loss: 2.553788\n",
      "Sample 9, Recon 2, Iter 900, Loss: 2.449932\n",
      "Sample 9, Recon 2, Iter 1000, Loss: 2.347019\n",
      "Sample 9, Recon 2, Iter 1100, Loss: 2.246180\n",
      "Sample 9, Recon 2, Iter 1200, Loss: 2.148679\n",
      "Sample 9, Recon 2, Iter 1300, Loss: 2.057119\n",
      "Sample 9, Recon 2, Iter 1400, Loss: 1.973013\n",
      "Sample 9, Recon 2, Iter 1500, Loss: 1.896838\n",
      "Sample 9, Recon 2, Iter 1600, Loss: 1.827500\n",
      "Sample 9, Recon 2, Iter 1700, Loss: 1.763229\n",
      "Sample 9, Recon 2, Iter 1800, Loss: 1.703148\n",
      "Sample 9, Recon 2, Iter 1900, Loss: 1.646203\n",
      "Sample 9, Recon 2, Iter 2000, Loss: 1.592405\n",
      "Sample 9, Recon 2, Iter 2100, Loss: 1.542110\n",
      "Sample 9, Recon 2, Iter 2200, Loss: 1.494504\n",
      "Sample 9, Recon 2, Iter 2300, Loss: 1.451434\n",
      "Sample 9, Recon 2, Iter 2400, Loss: 1.409359\n",
      "Sample 9, Recon 2, Iter 2500, Loss: 1.370536\n",
      "Sample 9, Recon 2, Iter 2600, Loss: 1.333257\n",
      "Sample 9, Recon 2, Iter 2700, Loss: 1.298150\n",
      "Sample 9, Recon 2, Iter 2800, Loss: 1.265067\n",
      "Sample 9, Recon 2, Iter 2900, Loss: 1.234220\n",
      "Sample 9, Recon 2, Iter 3000, Loss: 1.204064\n",
      "Sample 9, Recon 2, Iter 3100, Loss: 1.174664\n",
      "Sample 9, Recon 2, Iter 3200, Loss: 1.146026\n",
      "Sample 9, Recon 2, Iter 3300, Loss: 1.118080\n",
      "Sample 9, Recon 2, Iter 3400, Loss: 1.091031\n",
      "Sample 9, Recon 2, Iter 3500, Loss: 1.065302\n",
      "Sample 9, Recon 2, Iter 3600, Loss: 1.039629\n",
      "Sample 9, Recon 2, Iter 3700, Loss: 1.015104\n",
      "Sample 9, Recon 2, Iter 3800, Loss: 0.991470\n",
      "Sample 9, Recon 2, Iter 3900, Loss: 0.968847\n",
      "Sample 9, Recon 2, Iter 4000, Loss: 0.946599\n",
      "Sample 9, Recon 2, Iter 4100, Loss: 0.925430\n",
      "Sample 9, Recon 2, Iter 4200, Loss: 0.905067\n",
      "Sample 9, Recon 2, Iter 4300, Loss: 0.885541\n",
      "Sample 9, Recon 2, Iter 4400, Loss: 0.867287\n",
      "Sample 9, Recon 2, Iter 4500, Loss: 0.849651\n",
      "Sample 9, Recon 2, Iter 4600, Loss: 0.832748\n",
      "Sample 9, Recon 2, Iter 4700, Loss: 0.816879\n",
      "Sample 9, Recon 2, Iter 4800, Loss: 0.800960\n",
      "Sample 9, Recon 2, Iter 4900, Loss: 0.786310\n",
      "Sample 9, Recon 2, Iter 5000, Loss: 0.773379\n",
      "Sample 9, Recon 2, Iter 5100, Loss: 0.759130\n",
      "Sample 9, Recon 2, Iter 5200, Loss: 0.746226\n",
      "Sample 9, Recon 2, Iter 5300, Loss: 0.734222\n",
      "Sample 9, Recon 2, Iter 5400, Loss: 0.722082\n",
      "Sample 9, Recon 2, Iter 5500, Loss: 0.710323\n",
      "Sample 9, Recon 2, Iter 5600, Loss: 0.699273\n",
      "Sample 9, Recon 2, Iter 5700, Loss: 0.688703\n",
      "Sample 9, Recon 2, Iter 5800, Loss: 0.679915\n",
      "Sample 9, Recon 2, Iter 5900, Loss: 0.669991\n",
      "Sample 9, Recon 2, Iter 6000, Loss: 0.659452\n",
      "Sample 9, Recon 2, Iter 6100, Loss: 0.651205\n",
      "Sample 9, Recon 2, Iter 6200, Loss: 0.641683\n",
      "Sample 9, Recon 2, Iter 6300, Loss: 0.633433\n",
      "Sample 9, Recon 2, Iter 6400, Loss: 0.625633\n",
      "Sample 9, Recon 2, Iter 6500, Loss: 0.617986\n",
      "Sample 9, Recon 2, Iter 6600, Loss: 0.610767\n",
      "Sample 9, Recon 2, Iter 6700, Loss: 0.603951\n",
      "Sample 9, Recon 2, Iter 6800, Loss: 0.597383\n",
      "Sample 9, Recon 2, Iter 6900, Loss: 0.591044\n",
      "Sample 9, Recon 2, Iter 7000, Loss: 0.584607\n",
      "Sample 9, Recon 2, Iter 7100, Loss: 0.578822\n",
      "Sample 9, Recon 2, Iter 7200, Loss: 0.573029\n",
      "Sample 9, Recon 2, Iter 7300, Loss: 0.567981\n",
      "Sample 9, Recon 2, Iter 7400, Loss: 0.562163\n",
      "Sample 9, Recon 2, Iter 7500, Loss: 0.557815\n",
      "Sample 9, Recon 2, Iter 7600, Loss: 0.551883\n",
      "Sample 9, Recon 2, Iter 7700, Loss: 0.548081\n",
      "Sample 9, Recon 2, Iter 7800, Loss: 0.542354\n",
      "Sample 9, Recon 2, Iter 7900, Loss: 0.538767\n",
      "Sample 9, Recon 2, Iter 8000, Loss: 0.533933\n",
      "Sample 9, Recon 2, Iter 8100, Loss: 0.530153\n",
      "Sample 9, Recon 2, Iter 8200, Loss: 0.524858\n",
      "Sample 9, Recon 2, Iter 8300, Loss: 0.520999\n",
      "Sample 9, Recon 2, Iter 8400, Loss: 0.517235\n",
      "Sample 9, Recon 2, Iter 8500, Loss: 0.513268\n",
      "Sample 9, Recon 2, Iter 8600, Loss: 0.509810\n",
      "Sample 9, Recon 2, Iter 8700, Loss: 0.506071\n",
      "Sample 9, Recon 2, Iter 8800, Loss: 0.504121\n",
      "Sample 9, Recon 2, Iter 8900, Loss: 0.499615\n",
      "Sample 9, Recon 2, Iter 9000, Loss: 0.496862\n",
      "Sample 9, Recon 2, Iter 9100, Loss: 0.492660\n",
      "Sample 9, Recon 2, Iter 9200, Loss: 0.489946\n",
      "Sample 9, Recon 2, Iter 9300, Loss: 0.487308\n",
      "Sample 9, Recon 2, Iter 9400, Loss: 0.483549\n",
      "Sample 9, Recon 2, Iter 9500, Loss: 0.481889\n",
      "Sample 9, Recon 2, Iter 9600, Loss: 0.478058\n",
      "Sample 9, Recon 2, Iter 9700, Loss: 0.475233\n",
      "Sample 9, Recon 2, Iter 9800, Loss: 0.472825\n",
      "Sample 9, Recon 2, Iter 9900, Loss: 0.470598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|██████████| 10/10 [6:46:40<00:00, 2440.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generate results\n",
    "print(\"Generating activation differences for Llama-2 layers...\")\n",
    "results = generate_activation_differences_llama(model, X_data, n_samples=25, n_reconstructions=25)\n",
    "#406m 40.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70ca85c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved. Shape: (30, 13)\n",
      "\n",
      "First few rows:\n",
      "   sample_idx  reconstruction_idx  layer_0_min_abs_diff  \\\n",
      "0           0                   0              0.000000   \n",
      "1           0                   1              0.000008   \n",
      "2           0                   2              0.000015   \n",
      "3           1                   0              0.000019   \n",
      "4           1                   1              0.000061   \n",
      "\n",
      "   layer_0_mean_abs_diff  layer_0_max_abs_diff  layer_1_min_abs_diff  \\\n",
      "0               0.803223              6.522461              0.000000   \n",
      "1               0.812638              5.780273              0.000000   \n",
      "2               0.808089              6.396973              0.000061   \n",
      "3               0.808671              6.916016              0.000015   \n",
      "4               0.801016              4.633301              0.000000   \n",
      "\n",
      "   layer_1_mean_abs_diff  layer_1_max_abs_diff  layer_2_min_abs_diff  \\\n",
      "0               0.830116            752.273438              0.000092   \n",
      "1               0.837497            755.449219              0.000000   \n",
      "2               0.833267            753.881836              0.000000   \n",
      "3               0.833820            754.520264              0.000000   \n",
      "4               0.826034            753.515625              0.000000   \n",
      "\n",
      "   layer_2_mean_abs_diff  layer_2_max_abs_diff  all_layers_max_diff  \\\n",
      "0               0.826986            755.824219           755.824219   \n",
      "1               0.833590            759.451172           759.451172   \n",
      "2               0.830068            758.076904           758.076904   \n",
      "3               0.830613            758.265625           758.265625   \n",
      "4               0.822384            757.499023           757.499023   \n",
      "\n",
      "   all_layers_min_of_max  \n",
      "0               6.522461  \n",
      "1               5.780273  \n",
      "2               6.396973  \n",
      "3               6.916016  \n",
      "4               4.633301  \n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "results.to_csv('llama2_activation_diff_results.csv', index=False)\n",
    "print(f\"Results saved. Shape: {results.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(results.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
