{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6126ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e998355",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8a03637",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84bad784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "# X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, \n",
    "#                          n_redundant=5, n_classes=3, random_state=42)\n",
    "\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33249338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e744295",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train).to(device)\n",
    "X_test = torch.FloatTensor(X_test).to(device)\n",
    "y_train = torch.LongTensor(y_train).to(device)\n",
    "y_test = torch.LongTensor(y_test).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "310e2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
    "        super(ANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.fc3 = nn.Linear(hidden_sizes[1], num_classes)\n",
    "        self.relu = nn.ReLU()    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "955eb912",
   "metadata": {},
   "outputs": [],
   "source": [
    "verified_model = ANN(input_size=4, hidden_sizes=[64, 32], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1efb8404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(verified_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7c64374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30], Loss: 0.8291\n",
      "Epoch [10/30], Loss: 0.5939\n",
      "Epoch [15/30], Loss: 0.4449\n",
      "Epoch [20/30], Loss: 0.3514\n",
      "Epoch [25/30], Loss: 0.2984\n",
      "Epoch [30/30], Loss: 0.2333\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    verified_model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = verified_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {train_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b31e8964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.67%\n",
      "Train Accuracy: 95.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "verified_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = verified_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "verified_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = verified_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Train Accuracy: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29ed7dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x16d1620cbf0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations = {}\n",
    "def get_activation(name, storage_dict):\n",
    "    def hook(model, input, output):\n",
    "        storage_dict[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "verified_model.fc1.register_forward_hook(get_activation('fc1', activations))\n",
    "verified_model.fc2.register_forward_hook(get_activation('fc2', activations))\n",
    "verified_model.fc3.register_forward_hook(get_activation('fc3', activations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7b1453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_data = X_train[0]\n",
    "\n",
    "# Get verified activations from original model\n",
    "verified_model.eval()\n",
    "with torch.no_grad():\n",
    "    _ = verified_model(calibration_data)\n",
    "    target_activations = {k: v.clone() for k, v in activations.items()}\n",
    "\n",
    "real_activations = target_activations.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39a58e0",
   "metadata": {},
   "source": [
    "# ADVERSARY SETUP INVERSE TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86740f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def crack_input(target_output, model, learning_rate=0.001, iterations=10000, method='pseudo_inverse', return_all_activations=True):\n",
    "    model.eval()\n",
    "    \n",
    "    # Extract model parameters\n",
    "    W1 = model.fc1.weight.data\n",
    "    b1 = model.fc1.bias.data\n",
    "    W2 = model.fc2.weight.data\n",
    "    b2 = model.fc2.bias.data\n",
    "    W3 = model.fc3.weight.data\n",
    "    b3 = model.fc3.bias.data\n",
    "    \n",
    "    # Dictionary to store predicted activations\n",
    "    predicted_activations = {}\n",
    "    \n",
    "    if method == 'pseudo_inverse':\n",
    "        # Layer 3 inverse: output -> h2\n",
    "        W3_inv = torch.pinverse(W3)\n",
    "        h2 = W3_inv @ (target_output.squeeze() - b3)\n",
    "        h2 = torch.clamp(h2, min=0)  # ReLU constraint\n",
    "        predicted_activations['fc2'] = h2.unsqueeze(0)  # Store fc2 output\n",
    "        \n",
    "        # Layer 2 inverse: h2 -> h1\n",
    "        W2_inv = torch.pinverse(W2)\n",
    "        h1 = W2_inv @ (h2 - b2)\n",
    "        h1 = torch.clamp(h1, min=0)  # ReLU constraint\n",
    "        predicted_activations['fc1'] = h1.unsqueeze(0)  # Store fc1 output\n",
    "        \n",
    "        # Layer 1 inverse: h1 -> input\n",
    "        W1_inv = torch.pinverse(W1)\n",
    "        x_reconstructed = W1_inv @ (h1 - b1)\n",
    "        \n",
    "    elif method == 'svd':\n",
    "        # Layer 3 inverse using SVD\n",
    "        U3, S3, V3 = torch.svd(W3)\n",
    "        S3_inv = torch.where(S3 > 1e-6, 1.0/S3, torch.zeros_like(S3))\n",
    "        W3_inv = V3 @ torch.diag(S3_inv) @ U3.t()\n",
    "        h2 = W3_inv @ (target_output.squeeze() - b3)\n",
    "        h2 = torch.clamp(h2, min=0)\n",
    "        predicted_activations['fc2'] = h2.unsqueeze(0)\n",
    "        \n",
    "        # Layer 2 inverse using SVD\n",
    "        U2, S2, V2 = torch.svd(W2)\n",
    "        S2_inv = torch.where(S2 > 1e-6, 1.0/S2, torch.zeros_like(S2))\n",
    "        W2_inv = V2 @ torch.diag(S2_inv) @ U2.t()\n",
    "        h1 = W2_inv @ (h2 - b2)\n",
    "        h1 = torch.clamp(h1, min=0)\n",
    "        predicted_activations['fc1'] = h1.unsqueeze(0)\n",
    "        \n",
    "        # Layer 1 inverse using SVD\n",
    "        U1, S1, V1 = torch.svd(W1)\n",
    "        S1_inv = torch.where(S1 > 1e-6, 1.0/S1, torch.zeros_like(S1))\n",
    "        W1_inv = V1 @ torch.diag(S1_inv) @ U1.t()\n",
    "        x_reconstructed = W1_inv @ (h1 - b1)\n",
    "        \n",
    "    elif method == 'regularized':\n",
    "        # Regularized inverse (Ridge regression style)\n",
    "        lambda_reg = 1e-4\n",
    "        \n",
    "        # Layer 3 inverse\n",
    "        W3_reg_inv = torch.inverse(W3.t() @ W3 + lambda_reg * torch.eye(W3.shape[1], device=device)) @ W3.t()\n",
    "        h2 = W3_reg_inv @ (target_output.squeeze() - b3)\n",
    "        h2 = torch.clamp(h2, min=0)\n",
    "        predicted_activations['fc2'] = h2.unsqueeze(0)\n",
    "        \n",
    "        # Layer 2 inverse\n",
    "        W2_reg_inv = torch.inverse(W2.t() @ W2 + lambda_reg * torch.eye(W2.shape[1], device=device)) @ W2.t()\n",
    "        h1 = W2_reg_inv @ (h2 - b2)\n",
    "        h1 = torch.clamp(h1, min=0)\n",
    "        predicted_activations['fc1'] = h1.unsqueeze(0)\n",
    "        \n",
    "        # Layer 1 inverse\n",
    "        W1_reg_inv = torch.inverse(W1.t() @ W1 + lambda_reg * torch.eye(W1.shape[1], device=device)) @ W1.t()\n",
    "        x_reconstructed = W1_reg_inv @ (h1 - b1)\n",
    "    \n",
    "    # Store fc3 output (which is the target output)\n",
    "    predicted_activations['fc3'] = target_output\n",
    "    \n",
    "    x_reconstructed = x_reconstructed.unsqueeze(0)\n",
    "    \n",
    "    if return_all_activations:\n",
    "        return x_reconstructed, predicted_activations\n",
    "    else:\n",
    "        return x_reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d5d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "pred_inputs = crack_input(real_activations['fc3'], verified_model, learning_rate=0.001, iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c0411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting reconstruction experiments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing inputs:   0%|          | 0/50 [00:00<?, ?it/s]C:\\Users\\hskay\\AppData\\Local\\Temp\\ipykernel_20100\\2850931624.py:6: UserWarning: Using a target size (torch.Size([1, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  metrics['input_mse'] = F.mse_loss(original_input, reconstructed_input).item()\n",
      "C:\\Users\\hskay\\AppData\\Local\\Temp\\ipykernel_20100\\2850931624.py:7: UserWarning: Using a target size (torch.Size([1, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  metrics['input_mae'] = F.l1_loss(original_input, reconstructed_input).item()\n",
      "Processing inputs: 100%|██████████| 50/50 [03:38<00:00,  4.36s/it]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "ROUND = 2\n",
    "N_INPUTS = 120\n",
    "results = pd.DataFrame(columns=[\n",
    "    'input_id', 'round_id', \n",
    "    'fc1_min_abs_diff', 'fc1_max_abs_diff', 'fc1_mean_abs_diff',\n",
    "    'fc2_min_abs_diff', 'fc2_max_abs_diff', 'fc2_mean_abs_diff',\n",
    "    'fc3_min_abs_diff', 'fc3_max_abs_diff', 'fc3_mean_abs_diff',\n",
    "    'all_layers_max_diff', 'all_layers_min_of_max',\n",
    "    'real_input', 'pred_input', 'inverse_method'\n",
    "])\n",
    "\n",
    "for i in range(min(N_INPUTS, len(X_train))):\n",
    "    print(f\"Input {i+1}\")\n",
    "    for j in range(ROUND):\n",
    "        print(f\"Round {j+1}\")\n",
    "        \n",
    "        # Try different inverse methods\n",
    "        inverse_method = np.random.choice(['pseudo_inverse', 'svd', 'regularized'])\n",
    "\n",
    "        # Registering hooks to capture activations\n",
    "        activations = {}\n",
    "        hooks = []\n",
    "        hooks.append(verified_model.fc1.register_forward_hook(get_activation('fc1', activations)))\n",
    "        hooks.append(verified_model.fc2.register_forward_hook(get_activation('fc2', activations)))\n",
    "        hooks.append(verified_model.fc3.register_forward_hook(get_activation('fc3', activations)))\n",
    "\n",
    "        calibration_data = X_train[i]\n",
    "\n",
    "        # Get verified activations from original model\n",
    "        verified_model.eval()\n",
    "        with torch.no_grad():\n",
    "            _ = verified_model(calibration_data)\n",
    "            target_activations = {k: v.clone() for k, v in activations.items()}\n",
    "\n",
    "        real_activations = target_activations.copy()\n",
    "        \n",
    "        # Remove hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "\n",
    "        # INVERSE TRANSFORM INPUT GENERATION WITH PREDICTED ACTIVATIONS\n",
    "        pred_inputs, pred_activations = crack_input(\n",
    "            real_activations['fc3'], \n",
    "            verified_model, \n",
    "            method=inverse_method,\n",
    "            return_all_activations=True\n",
    "        )\n",
    "\n",
    "        round_results = {'input_id': i+1, 'round_id': j+1, 'inverse_method': inverse_method}\n",
    "        \n",
    "        # Compare real and predicted activations (from inverse transform)\n",
    "        all_layer_max_diffs = []\n",
    "        for layer in real_activations.keys():\n",
    "            # Calculate differences between real and inverse-predicted activations\n",
    "            abs_diff = torch.abs(real_activations[layer] - pred_activations[layer])\n",
    "            mean_abs_diff = abs_diff.mean().item()\n",
    "            max_abs_diff = abs_diff.max().item()\n",
    "            min_abs_diff = abs_diff.min().item()\n",
    "\n",
    "            # Store in results dictionary\n",
    "            round_results[f'{layer}_min_abs_diff'] = min_abs_diff\n",
    "            round_results[f'{layer}_max_abs_diff'] = max_abs_diff\n",
    "            round_results[f'{layer}_mean_abs_diff'] = mean_abs_diff\n",
    "            \n",
    "            all_layer_max_diffs.append(max_abs_diff)\n",
    "        \n",
    "        # Store the maximum difference across ALL layers\n",
    "        round_results['all_layers_max_diff'] = max(all_layer_max_diffs)\n",
    "        round_results['all_layers_min_of_max'] = min(all_layer_max_diffs)\n",
    "\n",
    "        # Append results to DataFrame\n",
    "        round_results['real_input'] = X_train[i].cpu().flatten().numpy().tolist()\n",
    "        round_results['pred_input'] = pred_inputs.cpu().flatten().numpy().tolist()\n",
    "        results = pd.concat([results, pd.DataFrame([round_results])], ignore_index=True)\n",
    "        \n",
    "results.to_csv('activation_diff_results_inverse_transform.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
