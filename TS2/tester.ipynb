{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from scipy.special import kl_div\n",
    "import os\n",
    "from model_structure import get_preprocessing_transforms,get_resnet_model,BCNN, train_model, evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_NAME = \"pretrained\" # \"my\" or \"pretrained\"\n",
    "MODEL_1_DIR = 'best_model_1.pth'\n",
    "MODEL_2_DIR = 'best_model_2.pth'\n",
    "K_ROUND = 50  # Number of rounds to monitor\n",
    "SAVE_FREQUENCY = 250  # Save every 5 steps, adjust as needed\n",
    "THE_CLASS = 1 # 0 cat or squirrel, 1 dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transforms\n",
    "train_transform, val_transform = get_preprocessing_transforms(INPUT_SIZE)\n",
    "\n",
    "test_data_1_dir = 'data4model_1/test/'\n",
    "test_data_2_dir = 'data4model_2/test/'\n",
    "train_data_1_dir = 'data4model_1/train/'\n",
    "train_data_2_dir = 'data4model_2/train/'\n",
    "\n",
    "# Load data set\n",
    "dataset_test_1 = datasets.ImageFolder(test_data_1_dir,transform=val_transform)\n",
    "dataset_train_1 = datasets.ImageFolder(train_data_1_dir,transform=val_transform)\n",
    "dataset_test_2 = datasets.ImageFolder(test_data_2_dir,transform=val_transform)\n",
    "dataset_train_2 = datasets.ImageFolder(train_data_2_dir,transform=val_transform)\n",
    "additional_set = datasets.ImageFolder('data4model_1/for_extra_test/',transform=val_transform)\n",
    "\n",
    "additional_loader = DataLoader(additional_set, shuffle=False, batch_size=BATCH_SIZE)\n",
    "test_loader_1 = DataLoader(dataset_test_1, shuffle=False, batch_size=BATCH_SIZE)\n",
    "train_loader_1 = DataLoader(dataset_train_1, shuffle=False, batch_size=BATCH_SIZE)\n",
    "test_loader_2 = DataLoader(dataset_test_2, shuffle=False, batch_size=BATCH_SIZE)\n",
    "train_loader_2 = DataLoader(dataset_train_2, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "dataSets =  {\"Model_1:Train\": train_loader_1,\n",
    "        \"Model_1:Test\": test_loader_1,\n",
    "        \"Model_2:Train\": train_loader_2,\n",
    "        \"Model_2:Test\": test_loader_2,\n",
    "        \"Model_1:additional_set\": additional_loader\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hskay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hskay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\hskay\\AppData\\Local\\Temp\\ipykernel_10224\\1659288269.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights = torch.load(model_1_dir, map_location=torch.device(DEVICE))\n",
      "C:\\Users\\hskay\\AppData\\Local\\Temp\\ipykernel_10224\\1659288269.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights = torch.load(model_2_dir, map_location=torch.device(DEVICE))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "# model_1 = BCNN().to(DEVICE)\n",
    "model_1 = get_resnet_model(2).to(DEVICE)\n",
    "weights = torch.load(MODEL_1_DIR, map_location=torch.device(DEVICE))\n",
    "model_1.load_state_dict(weights)\n",
    "model_1.eval()\n",
    "\n",
    "#model_2 = BCNN().to(DEVICE)\n",
    "model_2 = get_resnet_model(2).to(DEVICE)\n",
    "weights = torch.load(MODEL_2_DIR, map_location=torch.device(DEVICE))\n",
    "model_2.load_state_dict(weights)\n",
    "model_2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_1:Train\n",
      "Accuracy:  0.9533333333333334\n",
      "F1 Score:  0.9526707234617985\n",
      "confusion Matrix: \n",
      " [[1451   49]\n",
      " [  91 1409]]\n",
      "\n",
      "\n",
      "Model_1:Test\n",
      "Accuracy:  0.95\n",
      "F1 Score:  0.9489795918367347\n",
      "confusion Matrix: \n",
      " [[97  3]\n",
      " [ 7 93]]\n",
      "\n",
      "\n",
      "Model_2:Train\n",
      "Accuracy:  0.985\n",
      "F1 Score:  0.9848841115216661\n",
      "confusion Matrix: \n",
      " [[1489   11]\n",
      " [  34 1466]]\n",
      "\n",
      "\n",
      "Model_2:Test\n",
      "Accuracy:  0.965\n",
      "F1 Score:  0.9644670050761421\n",
      "confusion Matrix: \n",
      " [[98  2]\n",
      " [ 5 95]]\n",
      "\n",
      "\n",
      "Model_1:additional_set\n",
      "Accuracy:  0.9071830106183635\n",
      "F1 Score:  0.9045845640169513\n",
      "confusion Matrix: \n",
      " [[3740  260]\n",
      " [ 483 3522]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in dataSets:\n",
    "    print(key)\n",
    "    if \"Model_1\" in key: # Verified Model\n",
    "        model = model_1\n",
    "    else:\n",
    "        model = model_2\n",
    "    \n",
    "    results = evaluate_model(model.to(DEVICE), dataSets[key], DEVICE)\n",
    "    print(\"Accuracy: \", results[0])\n",
    "    print(\"F1 Score: \", results[1])\n",
    "    print(\"confusion Matrix: \\n\",results[3])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST STARTS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_values = {\"Model_1\": {}, \n",
    "                     \"Model_2\": {}}\n",
    "\n",
    "# Register hooks to capture activation values for each layer\n",
    "# This function will register hooks to capture the input, output, weights, \n",
    "# and biases of each layer in the model and store them in the `activation_values` dictionary.\n",
    "# Pytorch backend will call this function when the model is run in the forward pass.\n",
    "def register_hooks(model, model_name):\n",
    "    activation_values[model_name] = {}\n",
    "    hooks = []\n",
    "    \n",
    "    def get_hook(name):\n",
    "        def hook(module, input, output):\n",
    "            # Store input, output, weights and biases for each layer with keeping their shape \n",
    "            activation_values[model_name][name] = {\n",
    "                'input': input[0].detach(),\n",
    "                'output': output.detach(),\n",
    "                'weight': module.weight.detach() if hasattr(module, 'weight') else None,\n",
    "                'bias': module.bias.detach() if hasattr(module, 'bias') and module.bias is not None else None,\n",
    "            }\n",
    "            if isinstance(module, nn.Conv2d):   # For Conv2d layers, also store kernel size, padding, stride, and dilation because \n",
    "                                                # This information to calculate contributions of each neuron in the layer required.\n",
    "                activation_values[model_name][name]['conv_params'] = {\n",
    "                    'kernel_size': module.kernel_size[0] if isinstance(module.kernel_size, tuple) else module.kernel_size,\n",
    "                    'padding': module.padding[0] if isinstance(module.padding, tuple) else module.padding,\n",
    "                    'stride': module.stride[0] if isinstance(module.stride, tuple) else module.stride,\n",
    "                    'dilation': module.dilation[0] if isinstance(module.dilation, tuple) else module.dilation\n",
    "                }\n",
    "        return hook\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)): # Just register hooks for Conv2d and Linear layers not for all layers.\n",
    "            hooks.append(module.register_forward_hook(get_hook(name)))\n",
    "    \n",
    "    return hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_neuron_indices(activations, n_neurons=1,randomness=False):\n",
    "    indices = {}\n",
    "    \n",
    "    for layer_name, layer_data in activations.items():\n",
    "        # skipping if layer without weight such as ReLU, Dropout, etc.\n",
    "        if not isinstance(layer_data, dict) or 'output' not in layer_data:\n",
    "            continue\n",
    "            \n",
    "        # activation output and flatten it\n",
    "        activation = layer_data['output']\n",
    "        flattened = activation.view(activation.shape[0], -1)\n",
    "        flattened_np = flattened.cpu().numpy().squeeze()\n",
    "        # positive (activated) indices\n",
    "        positive_indices = np.where(flattened_np > 0)[0]\n",
    "        \n",
    "        # skipping if no activated neurons\n",
    "        if len(positive_indices) == 0:\n",
    "            continue\n",
    "\n",
    "        if randomness:    \n",
    "            # random neurons from activated ones\n",
    "            selected_indices = np.random.choice(\n",
    "                positive_indices,\n",
    "                size=min(n_neurons, len(positive_indices)),\n",
    "                replace=False\n",
    "            )\n",
    "        else:\n",
    "            positive_values = flattened_np[positive_indices]\n",
    "            min_value_idx = positive_indices[np.argmin(positive_values)]\n",
    "            \n",
    "            # Select the minimum value index\n",
    "            selected_indices = np.array([min_value_idx])\n",
    "        \n",
    "        # storing selected indices\n",
    "        indices[layer_name] = {\n",
    "            'neuron_idx': selected_indices,\n",
    "            'all_activated_indices': positive_indices,\n",
    "            'original_shape': activation.shape\n",
    "        }\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_activations(model1_activations, model2_activations, selected_indices):\n",
    "    comparison_results = {}\n",
    "    \n",
    "    for layer_name, indices_info in selected_indices.items():\n",
    "        if layer_name not in model1_activations or layer_name not in model2_activations:\n",
    "            continue\n",
    "        \n",
    "        data1 = model1_activations[layer_name]\n",
    "        data2 = model2_activations[layer_name]\n",
    "        \n",
    "        if not isinstance(data1, dict) or not isinstance(data2, dict):\n",
    "            continue\n",
    "        \n",
    "        selected_indices = indices_info['neuron_idx']\n",
    "        original_shape = indices_info.get('original_shape', data1['output'].shape)\n",
    "        \n",
    "        # get inputs, weights, biases, and stored outputs\n",
    "        input1 = data1['input']\n",
    "        weights1 = data1['weight']\n",
    "        bias1 = data1.get('bias')\n",
    "        weights2 = data2['weight']\n",
    "        bias2 = data2.get('bias')\n",
    "        stored_output1 = data1['output']\n",
    "    \n",
    "        layer_results = {\n",
    "            'neuron_comparisons': [],\n",
    "            'model1_verification_errors': [],\n",
    "            'cross_model_differences': [],\n",
    "            'model1_activation_value':[],\n",
    "            'model2_activation_value':[]\n",
    "        }\n",
    "        \n",
    "        for idx in selected_indices:\n",
    "            if len(weights1.shape) == 4:\n",
    "                # dimensions: batch_size, channels, height, width\n",
    "                batch_size, channels, height, width = original_shape\n",
    "                \n",
    "                # converting flat idx to corresponding coordinates\n",
    "                channel = idx // (height * width)\n",
    "                pos_in_channel = idx % (height * width)\n",
    "                h_idx = pos_in_channel // width\n",
    "                w_idx = pos_in_channel % width\n",
    "                \n",
    "                # get stored activation for this position\n",
    "                stored_activation = stored_output1[0, channel, h_idx, w_idx].item()\n",
    "                \n",
    "                # get convolution parameters\n",
    "                kernel_size = weights1.shape[2]\n",
    "                padding = kernel_size // 2  # symmetric padding\n",
    "                \n",
    "                # get input patch that corresponds to this position\n",
    "                conv_params = data1.get('conv_params', {\n",
    "                    'kernel_size': 3, \n",
    "                    'padding': 1,\n",
    "                    'stride': 1,\n",
    "                    'dilation': 1\n",
    "                })\n",
    "                \n",
    "                kernel_size = conv_params['kernel_size']\n",
    "                padding = conv_params['padding'] \n",
    "                stride = conv_params['stride']\n",
    "                dilation = conv_params['dilation']\n",
    "                \n",
    "                # Calculate input patch position\n",
    "                # With padding=1, we need to adjust position getting correct patch\n",
    "                in_h_start = h_idx * stride - padding\n",
    "                in_w_start = w_idx * stride - padding\n",
    "                in_h_end = in_h_start + kernel_size\n",
    "                in_w_end = in_w_start + kernel_size\n",
    "                \n",
    "                # Extract input patch accounting for padding\n",
    "                input_patch = torch.zeros(input1.shape[1], kernel_size, kernel_size, device=input1.device)\n",
    "                \n",
    "                for i in range(kernel_size):\n",
    "                    for j in range(kernel_size):\n",
    "                        h_pos = in_h_start + i * dilation\n",
    "                        w_pos = in_w_start + j * dilation\n",
    "                        \n",
    "                        # If position is within bounds of input\n",
    "                        if 0 <= h_pos < input1.shape[2] and 0 <= w_pos < input1.shape[3]:\n",
    "                            input_patch[:, i, j] = input1[0, :, h_pos, w_pos]\n",
    "\n",
    "                # Compute expected activation with model1 weights\n",
    "                if channel < weights1.shape[0]:\n",
    "                    kernel1 = weights1[channel]\n",
    "                    # Classic convolution calculation\n",
    "                    calculated_activation1 = (input_patch * kernel1).sum()\n",
    "                    \n",
    "                    if bias1 is not None:\n",
    "                        calculated_activation1 += bias1[channel]\n",
    "\n",
    "                    calculated_activation1 = max(0, calculated_activation1.item())  # ReLU\n",
    "                else:\n",
    "                    calculated_activation1 = 0\n",
    "                \n",
    "                # Compute activation using model2 weights with model1 input\n",
    "                if channel < weights2.shape[0]:\n",
    "                    kernel2 = weights2[channel]\n",
    "                    calculated_activation2 = (input_patch * kernel2).sum()\n",
    "                    if bias2 is not None:\n",
    "                        calculated_activation2 += bias2[channel]\n",
    "                    calculated_activation2 = max(0, calculated_activation2.item())  # ReLU\n",
    "                else:\n",
    "                    calculated_activation2 = 0\n",
    "                \n",
    "                neuron_data = {\n",
    "                    'type': 'conv',\n",
    "                    'position': {'channel': int(channel), 'height': int(h_idx), 'width': int(w_idx)},\n",
    "                    'input_patch': {\n",
    "                        'data': input_patch.detach().cpu().numpy(),\n",
    "                        'coords': {'h_start': in_h_start, 'h_end': in_h_end, \n",
    "                                  'w_start': in_w_start, 'w_end': in_w_end}\n",
    "                    }\n",
    "                }\n",
    "            else:\n",
    "                \n",
    "                flattened_output = stored_output1.view(stored_output1.shape[0], -1)\n",
    "                \n",
    "                # check if it is correct when i remove this\n",
    "                if idx >= flattened_output.shape[1] or idx >= weights1.shape[0]:\n",
    "                    continue\n",
    "                \n",
    "                # get stored activation\n",
    "                stored_activation = flattened_output[0, idx].item()\n",
    "                \n",
    "                # unrolling input for dot product\n",
    "                flattened_input = input1.flatten(start_dim=1)\n",
    "                \n",
    "                # check if it is correct when i remove this\n",
    "                if flattened_input.shape[1] != weights1.shape[1]:\n",
    "                    continue\n",
    "                \n",
    "                # calculate model1 activation\n",
    "                calculated_activation1 = torch.matmul(flattened_input, weights1[idx])\n",
    "                if bias1 is not None and idx < bias1.shape[0]:\n",
    "                    calculated_activation1 += bias1[idx]\n",
    "                calculated_activation1 = max(0, calculated_activation1.item())  # ReLU\n",
    "                \n",
    "                # Calculate using model2 weights with model1 input\n",
    "                calculated_activation2 = torch.matmul(flattened_input, weights2[idx])\n",
    "                if bias2 is not None and idx < bias2.shape[0]:\n",
    "                    calculated_activation2 += bias2[idx]\n",
    "                calculated_activation2 = max(0, calculated_activation2.item())  # ReLU\n",
    "                \n",
    "                neuron_data = {\n",
    "                    'type': 'linear',\n",
    "                    'position': {'neuron': int(idx)},\n",
    "                    'input_values': flattened_input.detach().cpu().numpy()\n",
    "                }\n",
    "            \n",
    "            # Calculate verification error (how accurate our calculation is)\n",
    "            verification_error = abs(stored_activation - calculated_activation1)\n",
    "            \n",
    "            # Calculate cross-model difference\n",
    "            cross_model_difference = abs(calculated_activation1 - calculated_activation2)\n",
    "            \n",
    "            # Store comparison for this neuron\n",
    "            neuron_comparison = {\n",
    "                'neuron_idx': int(idx),\n",
    "                'stored_activation': stored_activation,\n",
    "                'calculated_activation1': calculated_activation1,\n",
    "                'calculated_activation2': calculated_activation2,\n",
    "                'verification_error': verification_error,\n",
    "                'cross_model_difference': cross_model_difference,\n",
    "                'neuron_data': neuron_data\n",
    "            }\n",
    "            \n",
    "            layer_results['neuron_comparisons'].append(neuron_comparison)\n",
    "            layer_results['model1_verification_errors'].append(verification_error)\n",
    "            layer_results['cross_model_differences'].append(cross_model_difference)\n",
    "            layer_results['model1_activation_value'].append(stored_activation)\n",
    "            layer_results['model2_activation_value'].append(calculated_activation2)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        if layer_results['neuron_comparisons']:\n",
    "            layer_results['mean_verification_error'] = sum(layer_results['model1_verification_errors']) / len(layer_results['model1_verification_errors'])\n",
    "            layer_results['mean_cross_model_difference'] = sum(layer_results['cross_model_differences']) / len(layer_results['cross_model_differences'])\n",
    "        else:\n",
    "            layer_results['mean_verification_error'] = float('nan')\n",
    "            layer_results['mean_cross_model_difference'] = float('nan')\n",
    "        \n",
    "        comparison_results[layer_name] = layer_results\n",
    "    \n",
    "    return comparison_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [(img, idx) for idx, (img, label) in enumerate(dataSets[\"Model_1:additional_set\"].dataset) \n",
    "            if label == THE_CLASS  and model_1(img.unsqueeze(0).to(DEVICE))\n",
    "                                          .argmax().item() == THE_CLASS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_values = {\"Model_1\":{},\n",
    "                    \"Model_2\":{}}\n",
    "CLASSES = { \n",
    "    0: 'cat',\n",
    "    1: 'dog'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_stats_columns = [\n",
    "    'image_id', 'round', 'layer_name', \n",
    "    'mean_verification_error', 'mean_cross_model_difference', 'model1_activation_value', 'model2_activation_value'\n",
    "]\n",
    "\n",
    "output_file = f'{MODEL_NAME}_model_random_paths_for_{CLASSES[THE_CLASS]}_and_{K_ROUND}_rounds.csv'\n",
    "\n",
    "try:\n",
    "    existing_df = pd.read_csv(output_file)\n",
    "    last_image = existing_df['image_id'].max()\n",
    "    last_round = existing_df[existing_df['image_id'] == last_image]['round'].max()\n",
    "    print(f\"Resuming from image {last_image}, round {last_round}\")\n",
    "    file_exists = True\n",
    "except FileNotFoundError:\n",
    "    # Create a new file with headers\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(','.join(layer_stats_columns) + '\\n')\n",
    "    last_image = -1\n",
    "    last_round = -1\n",
    "    file_exists = True\n",
    "    print(\"Starting new run\")\n",
    "\n",
    "\n",
    "results_buffer = []\n",
    "for image, pic_index in images:\n",
    "    # Skip already processed images\n",
    "    if pic_index < last_image:\n",
    "        continue\n",
    "        \n",
    "    # register hooks\n",
    "    hooks1 = register_hooks(model_1, \"Model_1\")\n",
    "    hooks2 = register_hooks(model_2, \"Model_2\")\n",
    "    \n",
    "    # clearing\n",
    "    activation_values[\"Model_1\"] = {}\n",
    "    activation_values[\"Model_2\"] = {}\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        pred1 = model_1(image.unsqueeze(0).to(DEVICE))\n",
    "        pred2 = model_2(image.unsqueeze(0).to(DEVICE))\n",
    "\n",
    "    for i in range(K_ROUND):\n",
    "        # Skip already processed rounds for the last image\n",
    "        if pic_index == last_image and i <= last_round:\n",
    "            continue\n",
    "\n",
    "        # Select random neurons and compare\n",
    "        selected_indices = select_random_neuron_indices(\n",
    "            activation_values[\"Model_1\"], \n",
    "            n_neurons=1,\n",
    "            randomness=True\n",
    "        )\n",
    "        result = compare_model_activations(\n",
    "            activation_values[\"Model_1\"], \n",
    "            activation_values[\"Model_2\"], \n",
    "            selected_indices\n",
    "        )\n",
    "        \n",
    "        # Process results for each layer\n",
    "        for layer_name, layer_data in result.items():\n",
    "            results_buffer.append({\n",
    "                'image_id': pic_index,\n",
    "                'round': i,\n",
    "                'layer_name': layer_name,\n",
    "                'mean_verification_error': layer_data.get('mean_verification_error', np.nan),\n",
    "                'mean_cross_model_difference': layer_data.get('mean_cross_model_difference', np.nan),\n",
    "                'model1_activation_value': layer_data.get('model1_activation_value', np.nan),\n",
    "                'model2_activation_value': layer_data.get('model2_activation_value', np.nan)\n",
    "            })\n",
    "        \n",
    "        print('\\r ',f\"pic_index: {pic_index:02d}\", f\"round: {i:02d}\",end=\"\\n\")\n",
    "            \n",
    "        if len(results_buffer) >= SAVE_FREQUENCY:\n",
    "            temp_df = pd.DataFrame(results_buffer)\n",
    "            temp_df.to_csv(output_file, mode='a', header=False, index=False)\n",
    "            print(f\"Appended batch at image {pic_index}, round {i}\")\n",
    "            # Clear buffer\n",
    "            results_buffer = []\n",
    "    # Remove hooks\n",
    "    for hook in hooks1 + hooks2:\n",
    "        hook.remove()\n",
    "\n",
    "# Save any remaining results in the buffer\n",
    "if results_buffer:\n",
    "    temp_df = pd.DataFrame(results_buffer)\n",
    "    temp_df.to_csv(output_file, mode='a', header=False, index=False)\n",
    "print(\"Processing complete. Final results saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
